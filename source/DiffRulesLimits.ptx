<?xml version="1.0" encoding="utf-8"?>
<chapter xmlns:xi="http://www.w3.org/2001/XInclude"  xml:id="CHAPTERdiff-rules-with-limits">
 <title>The Differentiation Rules via Limits</title>

<section xml:id="SECTIONlimit-theorems-laws">
<title>The Limit Rules (Theorems)</title>

<subsection xml:id="SUBSECTIONlimit-comp-cont">
<title>The Limit of a Composition and Continuity at a Point</title>

<blockquote>
<p>
In the old days when people invented a new
  function they had something useful in mind. Now, they invent
  them deliberately just to invalidate our ancestors' reasoning,
  and that is all they are ever going to get out of
  them.
</p>
<attribution>
<url href="https://mathshistory.st-andrews.ac.uk/Biographies/Poincare" visual="https://mathshistory.st-andrews.ac.uk/Biographies/Poincare">Henri
    Poincarè</url> (1854-1912)
</attribution>
</blockquote>

<p>

In this section we will state several theorems about limits which we
will need in the sections following.  The limit concept is very subtle
and our understanding of it is still quite intuitive.  We are not yet
quite prepared to prove these theorems so we will leave these theorems
unproven for now. Our immediate goal is simply to understand what they
say and learn how to use them.  In the next section we will begin
using these theorems to show how the limit in
<xref ref="DEFINITIONDerivative"></xref> allows us to recapture all of the
major results we used in Part I of this text.
</p>
<p>

In <xref ref="CHAPTERformal-limits">Chapter</xref> we will finally discard our
intuitive definition of a limit
(<xref ref="DEFINITIONLimitAtInfinity-Intuitive"></xref>) and formally define
both a limit at infinity (<xref ref="DEFINITIONlimit-at-neginfinity">Theorem</xref>) and
a limit at a point (<xref ref="DEFINITIONlimits-at-real">Theorem</xref>). Then we will
return to the theorems in this section and (finally) prove rigorously
that they are, in fact, true. Until then any result which relies on
the theorems in this section should be considered contingent.
</p>
<p>


In <xref ref="CHAPTERlimits-lhop-rule">Chapter</xref> we stated the following three
theorems about limits <q>at infinity.</q>

<ol>
  <li>
    <p>
      the limit of a sum is the sum of the limits  (<xref ref="THEOREMLimSum1">Theorem</xref>),
    </p>
  </li>
  <li>
    <p>
      the limit of a product is the  product of the limits (<xref ref="THEOREMLimProd1">Theorem</xref>) and, 
    </p>
  </li>
  <li>
    <p>
      the limit of a quotient  is the  quotient of the limits (<xref ref="THEOREMLimQuot1">Theorem</xref>).
    </p>
  </li>
</ol>

All three of these theorems remain true if <m>x</m> is approaching some
finite number, <m>a</m>, instead of infinity.
</p>

<theorem xml:id="THEOREMLimSum2">
<title>The Limit of a Sum is the Sum of the Limits</title>
<p>
  <!-- \label{THEOREMLimSum2} -->
  Let <m>a</m> be some real number. Suppose that the functions <m>f(x)</m> and
  <m>g(x)</m> are defined on some open interval about <m>a</m> except, possibly,
  at <m>a</m> itself.
  
  Then 
  if <m>\limit{x}{a}{f(x)}=L</m> and
  <m>\limit{x}{a}{g(x)}=M,</m> then
  <me>
  \limit{x}{a}{\left[f(x)+g(x)\right]}=L+M
  =\limit{x}{a}{f(x)}+\limit{x}{a}{g(x)}.
  </me>
</p>
</theorem>


<theorem xml:id="THEOREMLimProd2">
<title>The Limit of a Product is the Product of the Limits</title>
<p>
  <!-- \label{THEOREMLimProd2} -->
  Let <m>a</m> be some real number. Suppose  that the functions  <m>f(x)</m> and <m>g(x)</m> are
  defined on some open interval about <m>a</m> except, possibly, at <m>a</m>
  itself.
  
  If 
  <m>\limit{x}{a} {f(x)}=L</m> and <m>\limit{x}{a}{g(x)}=M</m> then
  <me>
  \limit{x}{a} {\left(f(x)\cdot g(x)\right)}=\left(\limit{x}{a}{f(x)}\right)\cdot\left(\limit{x}{a}{g(x)}\right)
  = L\cdot M.
  </me>
</p>
</theorem>


<theorem xml:id="THEOREMLimQuot2">
<title>The Limit of a Quotient is the Quotient of the Limits</title>
<p>
  <!-- \label{THEOREMLimQuot2} -->
  Let <m>a</m> be some real number. Suppose  that the functions  <m>f(x)</m> and <m>g(x)</m> are
  defined on some open interval about <m>a</m> except, possibly, at <m>a</m>
  itself.
  
  Then if
  <m>\limit{x}{a} {f(x)}=L</m> and <m>\limit{x}{a}{g(x)}=M\neq0</m> then
  <me>
  \limit{x}{a}
  {\frac{f(x)}{g(x)}}=\frac{\limit{x}{a}{f(x)}}{\limit{x}{a}{g(x)}}
  = \frac{L}{M}.
  </me>
</p>
</theorem>

<p>

Notice that in addition to changing <m>\infty</m> to some real number, <m>a</m>,
we have added two qualifications to the statement of each of these
theorems from <xref ref="CHAPTERlimits-lhop-rule">Chapter</xref>:

<ul>
  <li>
    <p>
      <q>Suppose that the functions <m>f(x)</m> and <m>g(x)</m> are
      defined on some open interval about <m>a</m></q> and,
    </p>
  </li>
  <li>
    <p>
      <q>except, possibly, at <m>a</m> itself</q>
    </p>
  </li>
</ul>

To see why these are necessary recall that we're going to use limits
to define the derivative as in <xref
ref="DEFINITIONDerivative">Theorem</xref> so we'll need to evaluate
the limit <m>f^\prime(x)=\tlimit{h}{0}{\frac{f(x+h)-f(x)}{h}}</m>.
 Clearly the expression <m>\frac{f(x+h)-f(x)}{h}</m> is not defined at
<m>h=0</m>. But we're only interested in its value <term>in the
limit</term> as <m>h\rightarrow0</m> which means that <m>h</m> must be
able to get close to <m>0</m>. That is, there must be an open interval
around <m>0</m> where the expression <m>\frac{f(x+h)-f(x)}{h}</m> is
defined.
</p>
<p>

But we don't care if <m>\frac{f(x+h)-f(x)}{h}</m> is defined at <m>h=0</m> or
not. That is irrelevant to our purpose.  So we state explicitly that we
do not consider whether <m>h=0</m>.
</p>
<p>


Ok, but why did we insert the word <q>possibly</q>? Wouldn't it be enough
to simply say <q>except at <m>a</m></q>?
</p>

<p>
We need to say <q>possibly</q> because these theorems, like all theorems,
are stated with as much generality as possible.  For example, consider
the function <m>f(x)=2x</m>. Had we not included <q>possibly</q> in the
conditions of our theorems the limit: <m>\tlimit{x}{3}{2x}</m>, which is
clearly equal to <m>6</m>, would have to be considered undefined because
<m>f(x)=2x</m> is defined at <m>x=3</m>. This distinction  may seem like
a very fussy, and unimportant detail right now, but it will be
important when we discuss the meaning of continuity in
<xref ref="SUBSECTIONlimit-comp-cont"></xref>.
</p>
<aside>
<title>Historical Background</title>
<p>
In the
  eighteenth century there was a, public, protracted, and vitriolic
  argument between <url href="https://mathshistory.st-andrews.ac.uk/Biographies/Robins/" visual="https://mathshistory.st-andrews.ac.uk/Biographies/Robins/">Benjamin Robins</url> and <url href="https://en.wikipedia.org/wiki/James_Jurin" visual="https://en.wikipedia.org/wiki/James_Jurin">James Jurin</url> over exactly this
  point. Robins would have claimed that the statement
  <m>\tlimit{x}{3}{2x} = 6</m> is meaningless. Jurin would have said it
  has meaning because it is obviously true. The point here is not that
  either man was right or wrong, but rather that it depends on how we
  define limits. By one definition Robins was correct, by another
  Jurin was. Their controversy was the result of the incomplete
  understanding of limits that prevailed at the time.
</p>
</aside>


<example xml:id="EXAMPLEDiscontLimit1">
<p>
Suppose
  <me>
  f(x)=
  \begin{cases}
    2x \amp  \text{if } x\neq3\\
    10  \amp  \text{if } x=3
  \end{cases}.
  </me>
  Then
  <me>
  \limit{x}{3}{f(x)}=  6.
  </me> In particular the limit is not <m>10</m>.

  Here is how we would evaluate this limit using the tools we
  currently have at our disposal.

We're interested in the limit as <m>x\rightarrow3</m> so
  in particular we do not need to consider the case when <m>x=3</m>. But as
  long as <m>x\neq3</m> we have <m>f(x)=2x</m> so
  <me>
  \limit{x}{3}{f(x)}=   \limit{x}{3}{2x}.
  </me>
 As <m>x</m> gets close to <m>3</m> it is clear that <m>2x</m> gets close to
  <m>6</m>.
  Therefore 
  <me>
  \limit{x}{3}{f(x)}=  6.
  </me>

  Notice that our reasoning is a little vague in the last step because
  we had to resort to the phase <q>gets close to,</q> and we know from
  our work in <xref ref="SECTIONmore-indet-forms"></xref> that this is not a
  precise phrase. This is the best we can do now because we have not
  yet rigorously defined a limit. We will do that in
  <xref ref="CHAPTERformal-limits"></xref>.
</p>
</example>




<problem>
<introduction>
<p>
  By reasoning in a manner similar to
  <xref ref="EXAMPLEDiscontLimit1"></xref> show that
  <m>\tlimit{x}{3}{f(x)}=9</m> for each function.
</p>
</introduction>
<task>
<statement>
<p>
 <m>f(x)=x^2</m>
         </p>
</statement>
</task>
<task>
<statement>
<p>
 <m>f(x)=
           \begin{cases}
             x^2 \amp  \text{if } x\neq3\\
            \text{undefined }  \amp  \text{if } x=3
           \end{cases}</m>
       </p>
</statement>
</task>
</problem>
<!-- \begin{embeddedproblem} -->
<!--   By reasoning in a manner similar to -->
<!--   <xref ref="EXAMPLEexample:DiscontLimit1"></xref> show that -->
<!--   <m>\tlimit{x}{3}{f(x)}=9</m> for each function. -->
<!--   \begin{enumerate}[label={  (\alph*)}]  -->
<!--          \item <m>f(x)=x^2</m> -->
<!--          \item <m>f(x)= -->
<!--            \begin{cases} -->
<!--              x^2 \amp  \text{if } x\neq3\\ -->
<!--             \text{undefined }  \amp  \text{if } x=3 -->
<!--            \end{cases}</m> -->
<!--        \end{enumerate} -->

<!-- \end{embeddedproblem} -->

<p>
It will be tedious to write (and to read) the phrase <q>Suppose that
<m>f(x)</m> is defined on some open interval about <m>a</m> except, possibly, at
<m>a</m> itself</q> every time we need it so it is customary to say something
more like <q>Suppose <m>f(x)</m> is defined <term>near</term> <m>a</m></q>
instead. Because we are trying to be as precise, and rigorous as
possible we will formalize this by redefining the word
<term>near</term>.
</p>
<definition xml:id="DEFINITIONnear">
<title>Near</title>
<statement>
<p>
  <!-- \label{def:near} -->
  We say that <m>f(x)</m> has some property <term>near</term> <m>x=a</m> if <m>f(x)</m> has that
  property on an open interval about <m>x=a</m>, except possibly at <m>a</m>
  itself.
</p>
</statement>
</definition>
<p>
Notice that this is not what <q>near</q> means in
  ordinary speech.  This is one of the things that makes it difficult
  to read mathematics. We routinely co-opt words from natural
  languages (like English) and redefine them to fit our needs. In this
  case our purpose requires that we change the definition of <q>near</q>
  slightly as you've seen.  Because <q>near</q> is a common word and you
  have a lifetime of experience using it, it can be very difficult to
  cast off your preconceptions.  The familiar definition you learned
  in childhood will intrude and cause confusion. It is hard to
  overcome this. Refer back to the definition frequently until you
  have internalized the mathematical definition.  
</p>


<p>
We  have the following theorem.
</p>
<theorem xml:id="THEOREMConstLimit">
<title>The Limit of a Constant is the Constant</title>
<p>
  Suppose <m>a</m> and  <m>L</m> are real  numbers, and <m>f(x)=L</m> near <m>a</m>. Then
  <me>
  \limit{x}{a}{f(x)}=L.
  </me>
</p>
</theorem>
<p>

Notice that <xref ref="THEOREMConstLimit">Theorem</xref> would be considerably less
useful had we not required that <m>f(x)=L</m> for <m>x</m> near <m>a</m>, rather than
<m>f(x)=L</m> on its entire domain. For example, as stated
<xref ref="THEOREMConstLimit">Theorem</xref> allows us to conclude that if
<me>
H(x) =
\begin{cases}
  1\amp  \text{ if } x\ge0\\
  -1\amp  \text{ if } x\lt0
\end{cases}
</me>
then
<me>\limit{x}{5}{H(x)}=1</me> and  <me>\limit{x}{-5}{H(x)}=-1</me> because
there are open intervals about <m>x=5</m> and <m>x=-5</m> where <m>H(x)=1</m>, and
<m>H(x)=-1</m>, respectively. 
</p>
<p>

Since there is no such interval about <m>x=0</m>, <m>\tlimit{x}{0}{H(x)}</m> is
undefined.
</p>



<exercise>
<ol cols="2" marker="i">
  <li>
    <p>
      Find an open interval about <m>5</m> where <m>H(x)=1</m>.
    </p>
  </li>
  <li>
    <p>
      Find an open interval about <m>-5</m> where <m>H(x)=-1</m>.
    </p>
  </li>
</ol>
</exercise>
<!-- begin{ProficiencyDrill-enumerate} -->
 <!--       \begin{enumerate}[label={  (\alph*)}]  -->
 <!--         \item   Find an open interval about <m>5</m> where <m>H(x)=1</m>. -->
 <!--        \item   Find an open interval about <m>-5</m> where <m>H(x)=-1</m>. -->
 <!--      \end{enumerate} -->
 <!--      \comment{There are many to choose from in both parts. Choose -->
 <!--        only one for each.} -->
 <!--  \end{ProficiencyDrill-enumerate} -->


 <exercise>
   <p>
     Suppose
     <me>
       f(x)=
       \begin{cases}
       \ \ 1;\amp  \text{ if } x\gt 1 \text{ or if } x=-2\\
       -2;\amp  \text{ if } x\le1 \text{ and } x=-2.
       \end{cases}
     </me>
     Determine whether the following statements are true or false.
   </p>
   <ol cols="2" marker="i">
     <li>
       <p>
         <m>f(x)= 1</m> near <m>x= 4</m>.
       </p>
     </li>
     <li>
       <p>
         <m>f(x)= 1</m> near <m>x= 1</m>.
       </p>
     </li>
     <li>
       <p>
         <m>f(x)= -2</m> near <m>x=1 </m>.
       </p>
     </li>
     <li>
       <p>
         <m>f(x)= 1</m> near <m>x=-2 </m>.
       </p>
     </li>
     <li>
       <p>
         <m>f(x)= -2</m> near <m>x=-2 </m>.
       </p>
     </li>
     <li>
       <p>
         <m>f(x)= 0</m> near <m>x= 0</m>.
       </p>
     </li>
   </ol>
 </exercise>
<!-- begin{ProficiencyDrill} -->
<!--   Suppose -->
<!--     <me> -->
<!--   f(x)= -->
<!--   \begin{cases} -->
<!--     \ \ 1;\amp  \text{ if } x\gt 1 \text{ or if } x=-2\\ -->
<!--     -2;\amp  \text{ if } x\le1 \text{ and } x=-2. -->
<!--   \end{cases} -->
<!--   </me> -->
<!--   Determine whether the following statements are true or false. -->
<!--   \begin{enumerate}[label={  (\alph*)}] -->
<!--     \item <m>f(x)= 1</m> near <m>x= 4</m>. -->
<!--     \item <m>f(x)= 1</m> near <m>x= 1</m>. -->
<!--     \item <m>f(x)= -2</m> near <m>x=1 </m>. -->
<!--     \item <m>f(x)= 1</m> near <m>x=-2 </m>. -->
<!--     \item <m>f(x)= -2</m> near <m>x=-2 </m>. -->
<!--     \item <m>f(x)= 0</m> near <m>x= 0</m>. -->
<!--   \end{enumerate} -->
<!-- \end{ProficiencyDrill} -->






<problem>
<introduction>
  <p>
    Explain, that the following statements are true by citing <xref
    ref="THEOREMLimSum2"></xref> through <xref
    ref="THEOREMConstLimit"></xref> as needed.
  </p>
</introduction>
<task>
<statement>
<p>
 <m>\limit{x}{0}{\left(\frac{5x}{x}+\frac{\pi x}{x}\right)}=5+\pi</m>
    </p>
</statement>
</task>
<task>
<statement>
<p>

      <m>\limit{x}{2}{\left(\frac{x-2}{x-2}+\frac{3x-6}{x-2}\right)}=4</m>
    </p>
</statement>
</task>
<task>
<statement>
<p>
 <m>\limit{x}{5}{\left(2x+3x^2\right)}=85</m>
    </p>
</statement>
</task>
<task>
<statement>
<p>

      <m>\limit{x}{-1}{\left(\frac{x^2-1}{x+1}+\frac{x^2+3x+2}{x+1}\right)}=-1</m>
  </p>
</statement>
</task>
</problem>
<!-- \begin{embeddedproblem} -->
<!--     \noindent  Explain, that  the following -->
<!--   statements are true by citing <xref ref="THEOREMLimSum2">~</xref> through -->
<!--   <xref ref="THEOREMConstLimit">~</xref> as needed. -->
<!--   \begin{enumerate}[label={  (\alph*)}] -->
<!--     \item <m>\limit{x}{0}{\left(\frac{5x}{x}+\frac{\pi x}{x}\right)}=5+\pi</m> -->
<!--     \item -->
<!--       <m>\limit{x}{2}{\left(\frac{x-2}{x-2}+\frac{3x-6}{x-2}\right)}=4</m> -->
<!--     \item <m>\limit{x}{5}{\left(2x+3x^2\right)}=85</m> -->
<!--     \item -->
<!--       <m>\limit{x}{-1}{\left(\frac{x^2-1}{x+1}+\frac{x^2+3x+2}{x+1}\right)}=-1</m> -->
<!--   \end{enumerate} -->
<!--   \end{embeddedproblem} -->

<problem>
<statement>
<p>
  Notice that neither <m>\tlimit{x}{0}{\frac{1}{x}}</m> nor
  <m>\tlimit{x}{0}{\left(\frac{-1}{x}\right)}</m> exists. However their
  sum,
  <me>
  \tlimit{x}{0}{\left(\frac{1}{x}+\frac{-1}{x}\right)}=\tlimit{x}{0}{0}=0
  </me>
  does
  exist. Explain why this does not contradict
  <xref ref="THEOREMLimSum2">Theorem</xref>.
</p>
</statement>
</problem>


<problem xml:id="PROBLEMDiffImpCont">
<statement>
<p>
  Suppose <m>g(x)\neq0</m> near <m>x=a</m> and
  <m>\tlimit{x}{a}{\frac{f(x)}{g(x)}}</m> exists. Use
  <xref ref="THEOREMLimProd2"></xref> to show that if
  <m>\limit{x}{a}{g(x)}=0</m>         then <m>\limit{x}{a}{f(x)}=0</m>.
</p>

</statement>
<hint>
<p>
Consider
      <m>f(x)=\frac{f(x)}{g(x)}\cdot g(x)</m> for <m>x</m> near <m>a</m>.
</p>
</hint>
</problem>
<aside>
<title>Comment</title>
<p>
This problem shows that if <m>\limit{x}{a}{g(x)}=0</m>
then the only way that <m>\limit{x}{a}{\frac{f(x)}{g(x)}}</m> can exist is
if we have a L<rsq/>Hôpital  Indeterminate. It can also be used to prove
<xref ref="LEMMADiffImpliesCont">Lemma</xref> as you will see when we get to it.
</p>
</aside>


<p>
The following Corollary says that if <m>f(x)</m> is approaching <m>L_f</m> and
we multiply <m>f(x)</m> by a number, <m>k</m>, then the product <m>kf(x)</m>
approaches <m>kL_f</m>. It follows from <xref ref="THEOREMConstLimit">Theorem</xref> and
<xref ref="THEOREMLimProd2">Theorem</xref>.
</p>

<corollary xml:id="COROLLARYConstMultLimit">
<p>
  If <m>\limit{x}{a}{f(x)}=L_f</m> and <m>k</m> is a real number then
  <me>\limit{x}{a}{kf(x)}=k\limit{x}{a}{f(x)}=kL_f.</me>
</p>
</corollary>



<exercise>
<statement>
<p>
  Prove <xref ref="COROLLARYConstMultLimit">Corollary</xref>.
</p>
</statement>
</exercise>


<p>
The concept of <term>continuity</term> is essential to Calculus, but you may
have noticed that we have carefully avoided it as much as possible
until now. This is because defining continuity is similar to defining
the line tagent to a curve (<xref ref="DEFINITIONTangentLine"></xref>). We
need to think carefully about what we want the term <term>continuous</term>
to mean, and then craft our definition to capture that meaning. This
would have been very difficult to do without a fairly sophisticated
understanding of the limit concept.
</p>
<p>
  
So stop and think about this for a moment. What do we mean when we say
a curve is <term>continuous</term>? A first, intuitive definition usually
goes something like this: <q>A function is continuous if you can draw
its graph without lifting your pencil from the paper,</q> but this is
unsatisfactory for a number of reasons. In particular, it is
impossible to apply in most cases. Think about it. How often have you seen
the entire graph of any function? Usually we just draw the part
neat the origin and put arrowheads on both ends of the graph. We need
something more precise.
</p>
<p>

At the end of <xref ref="EXAMPLELimComp1"></xref> we remarked that it is
only when <m>f(x)</m> is continuous at <m>x=g(a)</m> that
<m>\textcolor{blue}{\tlimit{x}{a}{\textcolor{red}{f(g(x))}}}</m> is equal
to
<m>\textcolor{red}{f}\textcolor{blue}{\left(\tlimit{x}{a}{g(x)}\right)}</m>,
but we did not discuss the matter any further. It is time for that
discussion.
</p>
<p>


First, notice that when you think closely about the statement <q><m>f(x)</m>
is continuous at <m>g(a)</m></q> it appears to be nonsense, because <m>g(a)</m> is
the value of <m>f</m> at the single value <m>g(a)</m>.  Does it make sense to
you that a curve can be continuous at a single value of its domain?
In ordinary usage  the concept of continuity requires an interval to
be continuous on, doesn't it?
</p>
<p>
  
            
            
      
Since we need the concept of <q>continuity at a point,</q> we define it.
</p>
  
  <definition xml:id="DEFINITIONcontinuity">
<title>Continuity at a Point</title>
<statement>
<p>
    <!-- \label{def:continuity} -->
    A function <m>f</m>, whose domain is an interval in <m>\RR</m>, is
    continuous at <m>x=a</m> in the interval, if and only if
    <m>\limit{x}{a}{f(x)}=f(a)</m>, (alternatively, if
    <m>\limit{h}{0}{f(a+h)}=f(a)</m>).
  
</p>
</statement>
</definition>
<!-- \begin{wrapfigure}[]{r}{2in} -->
<!--   \captionsetup{labelformat=empty} -->
<!--   \vskip-4mm  -->
<!--     \centerline{\includegraphics*[height=2in,width=1.5in]{../Figures/ContDiscExamp}} -->
<!-- \label{fig:ContDiscExamp} -->
<!-- \end{wrapfigure} -->
<p>

If <m>f</m> is continuous at every point in its domain we'll just call
it a continuous function.
</p>
<p>

The sketch below shows that <xref ref="DEFINITIONcontinuity"></xref>
recovers the intuitive notion that a function is continuous if we can
draw its graph without lifting pen from page.  Both of the functions,
<m>f(x)</m> and <m>g(x)</m> are identical everywhere except at
<m>x=1</m>. Clearly, to draw <m>g(x)</m>, which is discontinuous at
<m>x=1</m>  since <m>\limit{x}{0}{g(x)} \neq g(1)</m>.  We
must lift our pen from the page to draw the graph of 
<m>g(x)</m>. None of this is true of the graph of
<m>f(x)</m>, which is continuous.
</p>
<image source="images/ContDiscExamp.png" width="50%"/>

    
<p>
The following lemma is true and the proof will be valid once the limit
theorems have been proven in <xref ref="CHAPTERformal-limits"></xref>.
</p>
  
  <lemma xml:id="LEMMADiffImpliesCont">
<title>Differentiability Implies Continuity</title>
<p>
    If <m>f(x)</m> is differentiable at <m>x=a</m> then <m>f(x)</m> is also
    continuous at <m>x=a.</m>
  </p>
</lemma>


                       
      
 <problem>
<statement>
<p>
    Use the result of <xref ref="PROBLEMDiffImpCont"></xref> to prove
    <xref ref="LEMMADiffImpliesCont"></xref>.
  </p>
</statement>
</problem>
<p>  
  When we were studying horizontal asymptotes in
  <xref ref="SUBSECTIONlhop-rule-horiz"></xref> we encountered
  <xref ref="THEOREMInfSqueeze"></xref> (the  Squeeze
  Theorem <q>at</q> Infinity). But the Squeeze Theorem is also valid if
  <m>x\rightarrow a</m>, where <m>a</m> is a real number.
</p>    
  
  <theorem xml:id="THEOREMFiniteSqueeze">
<title>The Squeeze Theorem (The Finite Case)</title>
<p>
    If <m>\alpha(x)\le f(x)\le \beta(x)</m> for <m>x</m> near <m>a</m>
    and <me>\limit{x}{a}{\beta(x)} = \limit{x}{a}{\alpha(x)} = L</me> then
    <m>\limit{x}{a}{f(x)} = L</m> also.
  </p>
</theorem>

<p>
    <xref ref="THEOREMFiniteSqueeze"></xref> is illustrated below, but a formal
    proof will not be given until <xref ref="CHAPTERformal-limits"></xref>.
Notice that on the interval <m>(c,d)</m> <m>f(x)</m> is caught between
(<q>squeezed by</q>) <m>\alpha{(x)}</m> and <m>\beta{(x)}</m>.

</p>
      
<image source="images/SqueezeThm1.png" width="40%" />
<!-- \centerline{\includegraphics*[height=1.4in,width=2.6in]{../Figures/SqueezeThm1}} -->
<problem>
<introduction>
<p>
        Consider the two functions defined in the sketch below:
</p>
<image source="images/TopSinCurve.png" width="100%" />

<p>
<!-- \includegraphics*[height=2.5in,width=5.5in]{../Figures/TopSinCurve} -->
</p>
</introduction>
<task>
<statement>
<p>
 Use <xref ref="THEOREMFiniteSqueeze"></xref> to show that <m>U(x)</m> is
      continuous at <m>x=0</m>. 
    </p>
</statement>
<hint>
<p>
What functions is <m>U(x)</m> caught between?
</p>
</hint>
</task>
<task>
<statement>
<p>
 Use <xref ref="DEFINITIONcontinuity"></xref> to show that <m>T(x)</m> is
      not
      continuous at <m>x=0</m>.  
    </p>
</statement>
<hint>
<p>
      Try the substitution <m>z=\frac1x</m> for <m>x\neq0</m>.  What would
        <m>\limit{x}{0}{T(x)}</m> look like in terms of <m>z</m>?
</p>
</hint>
</task>
</problem>
<!-- \begin{embeddedproblem} -->
<!--         Consider the two functions defined in the sketch below:\\ -->
<!--         \centerline{\includegraphics*[height=2.5in,width=5.5in]{../Figures/TopSinCurve}} -->
<!--     \begin{enumerate}[label={  (\alph*)}] -->
<!--     \item Use <xref ref="THEOREMFiniteSqueeze">~</xref> to show that <m>U(x)</m> is -->
<!--       continuous at <m>x=0</m>. \\ -->
<!--       \hint{What functions is <m>U(x)</m> caught between?} -->
<!--     \item Use <xref ref="DEFINITIONdef:continuity">~</xref> to show that <m>T(x)</m> is -->
<!--       not -->
<!--       continuous at <m>x=0</m>.  \\ -->
<!--       \hint{Try the substitution <m>z=\frac1x</m> for <m>x\neq0</m>.  What would -->
<!--         <m>\limit{x}{0}{T(x)}</m> look like in terms of <m>z</m>?} -->
<!--     \end{enumerate} -->
<!--   \end{embeddedproblem} -->



<theorem xml:id="THEOREMCompositionLimit">
<title>The Limit of a Composition is the Composition of the
Limits</title> 
  <!-- \label{THEOREMCompositionLimit} -->
<p>
  Suppose <m>\limit{x}{a}{g(x)}=L_g</m> and that <m>f(x)</m> is continuous at
  <m>L_g</m>. Then
  <me>
  \limit{x}{a}{f(g(x))}=f\left(\limit{x}{a}{g(x)}\right)=f(L_g).
  </me>  
</p>
</theorem>
<p>

Essentially this says that we can interchange the function <m>f</m> and
the <q><m>\lim</m></q> symbols if <m>f</m> is continuous at <m>g(a)</m>.
</p>




</subsection>
</section>
<section xml:id="SECTIONdiff-rules-via">
<title>The General Differentiation Theorems, via Limits</title>

<blockquote>
<p>
. . . one way in math to take care of destabilizing
  problems is to legislate them out of existence . . . by loading
  theorems with stipulations and exclusions designed to head off crazy
  results.
</p>
<attribution>
<url href="https://en.wikipedia.org/wiki/David_Foster_Wallace" visual="https://en.wikipedia.org/wiki/David_Foster_Wallace">David
    Foster Wallace</url> (1962-2008)
</attribution>
</blockquote>

<p>

Since we will now be proving the the differentiation rules rigorously
we will call them what they really are: Theorems. 
Because limits are much less intuitive than differentials we'll want
to be as efficient as possible when using them. The sooner we can
build up some tools to make things easier, the better. 
</p>
<p>
 
Also, in this section we will add a new differentiation rule
(theorem): The Chain Rule. Or rather, we will give a name to an
already familiar technique and elevate it's status by providing a
formal proof.  Proving the Chain, Product, and Quotient
Differentiation Rules using limits will require a good deal of
cleverness. These proofs will also uncover some unexpected subtleties
along the way.
</p>
<p>

Before we begin there is one more point that needs to be 
clear. Because differentiation is now defined via a limit and limits
are defined at a point we can only differentiate a function
at a point. We usually say that limit evaluation and
differentiability are <term>local properties</term>.  If we don't specify
the <q>at <m>x</m></q> the convention is that the function is differentiable
at every point in its domain.
</p>
<p>


The proofs of the Constant, Sum, and Constant Multiple
Differentiation Rules are all completely straightforward so we
will leave them as exercises for you.
</p>
<aside>
<title>Comment</title>
<p>
Frequently students will simply
  ignore problems that are described as straightforward. Don't make
  that mistake. Straightforward does not mean easy, and it does not
  mean unimportant. We are leaving these problems for you so you can
  gain experience using limits in the simplest cases, not because they
  are unimportant.
</p>
</aside>


<theorem xml:id="THEOREMLimConstantRule">
<title>The Constant Rule for Differentiation</title>
<p>
  If <m>L</m> is some number and <m>f(x)=L</m> for all real values of <m>x</m>
  near (on an open interval around)
  <m>L</m>, then
  <m>f^\prime(x)=0</m> at every real number <m>x</m> near (on the same open
  interval) <m>L</m>.
</p>

</theorem>
  <proof>
<p>
    By <xref ref="EQUATIONDefDerivative"></xref>
<me>
      f^\prime(x) =\limit{h}{0}{\frac{f(x+h)-f(x)}{h}}.
</me>
But <m>f(x+h)=L=f(x)</m> so
<me>      f^\prime(x) =\limit{h}{0}{\frac{L-L}{h}}=\limit{h}{0}{\frac{0}{h}}=0.</me>
  </p>
<p>
<alert><m>\halmos</m></alert>
</p>
</proof>








<theorem>
<title>The Sum Rule for Differentiation</title>
<p>
  If <m>\alpha(x)</m> and <m>\beta(x)</m> are differentiable at <m>x</m> and
  <m>f(x)=\alpha(x)+\beta(x)</m>, then <m>f(x)</m> is also differentiable at <m>x</m>
  and
  <me>
  f^\prime(x)=\alpha^\prime(x)+ \beta^\prime(x).
  </me>
</p>
</theorem>



<problem>
<statement>
<p>
  Use <xref ref="DEFINITIONDerivative">Definition</xref> to prove the Sum Rule for
  Differentiation.
</p>
</statement>
</problem>
<p>


Recall that when we first established the  General Differentiation Rules using
differentials in <xref ref="CHAPTERdifferentials">Chapter</xref> we said that the
Constant Multiple,  Power and Quotient Rules for differentiation were just
conveniences because they depend on the other rules.
</p>
<p>

This is still true of course (except for the caveat in the point of
rigor below) which means that we don't have to prove any of them using
limits. For example, since the Constant Rule and the Product Rule are
now established theorems we can use these to prove the Constant
Multiple Rule directly, without having to resort to using limits.
</p>


<theorem  xml:id="THEOREMLimitConstMult">
<title>The Constant Multiple Rule for Differentiation</title>
<p>
  If <m>f(x)</m> is differentiable at <m>x</m> and <m>K</m> is a constant then <m>\alpha(x)=Kf(x) </m> is
  also differentiable and
  <me>
  \alpha^\prime(x)=Kf^\prime(x).
  </me>
</p>
</theorem>



<problem xml:id="PROBLEMConstMultRule">
<statement>
<p>
 Use the <xref ref="DEFINITIONDerivative"></xref> to prove the Constant Multiple Rule.
</p>
</statement>
</problem>



</section>
<section xml:id="SECTIONChainRule">
<title>The Chain Rule</title>

<p>
To understand the Chain Rule we will need to slightly blur the
distinction between function and variable.
</p>
<example xml:id="EXAMPLEchain-rule1">
<p>
  Here's what we mean: The formula <m>y=(2x^2-6x)^3</m>, is given
    entirely in terms of the variables <m>x</m>, and <m>y</m>. To differentiate
    using differentials we would make the (variable) substitution
    <m>z=3x^2+6x</m> so that <m>y=z^3</m>. In that case,
    <m>\dx{y}=3z\dx{z}=3\left(3x^2+6x\right)^2(6x+6)\dx{x}</m>, and
    dividing through by <m>\dx{x}</m> gives us the derivative of <m>y</m> with
    respect to <m>x</m>,
    <men  xml:id="EQUATIONCR1">
      <!-- \label{eq:CR1} -->
      \dfdx{y}{x}=3z\dx{z}=3\left(3x^2+6x\right)^2(6x+6).
  </men>

</p>
<p>

      But <xref ref="DEFINITIONDerivative"></xref> requires that we think
      about functions, not variables so let's translate this problem
      into the language of functions. If <m>y=\left(2x^2-6x\right)^3</m>,
      clearly <m>y</m> is a function of (depends on) <m>x</m>. Naming that
      function <m>f</m>, we have <m>y=f(x)</m>.
      Replacing <m>y</m> with <m>f(x)</m>, we get
      <m> f(x)=(2x^2-6x)^3</m>.

</p>
<p>

      Similarly, if <m>z=3x^2+6x</m> then <m>z</m> is also a function of
      (depends on) <m>x</m>, and naming that function <m>\beta</m> we have
      <m>z=\beta(x)</m>. Replacing <m>z</m> with <m>\beta(x)</m> we have
      <m>f(x)=(\beta(x))^3</m>. If we suppress the <q><m>(x)</m></q> part of
      <m>\beta(x)</m>, we see that <m>f</m> can also be thought of as a function
      of (depends on) <m>\beta</m> so that
        <me>
        f(\beta)= \beta^3
        </me>
        is also a valid representation of our function. If we now
        define <m>\alpha(\beta)=\beta^3</m> we see that
        <me>
        f(x)= \alpha(\beta).
        </me>

</p>
<p>

        Looking again at <xref ref="EQUATIONCR1">equation</xref>, and mixing the
        differential and functional notations a bit we see that 
        <me>
        f^\prime(x)=\dfdx{y}{x}=3z\dx{z}=\underbrace{3\left(3x^2+6x\right)^2}_{\dfdx{\alpha}{\beta}=\alpha^\prime(\beta)}\underbrace{(6x+6)}_{\dfdx{\beta}{x}=\beta^\prime(x)}=\alpha^\prime(\beta)\cdot\beta^\prime(x).
        </me>

</p>
<p>
        
        
        Thus if <m>f(x)=\alpha(\beta(x))</m> is the composition of <m>\alpha(x)</m> and
        <m>\beta(x)</m>  then
        <me>
        f^\prime(x)=\alpha^\prime(\beta(x))\beta^\prime(x).
        </me>
        This is the Chain Rule.  We have expressed the Chain Rule in
        this form so that we can prove it rigorously, not so that we
        can use it. The substitution process using differentials still
        works so there is no reason to stop using substitution when
        you are actually computing derivatives.
      </p>
</example>


      
<theorem xml:id="THEOREMChainRule">
<title>The Chain Rule</title>
<p>
  Suppose that <m>\beta(x)</m> is differentiable at <m>x</m>, that <m>\alpha(x)</m>
  is differentiable at <m>\beta(x)</m> and that <m>\Delta\beta\neq0</m> near
  <m>x</m>.
  Then the composition,\\
  \centerline{<m> f(x) = \alpha(\beta(x)) </m>}
  is also differentiable, and
  <men  xml:id="EQUATIONChainRule">
    <!-- \label{eq:ChainRule} -->
    f^\prime(x) =\alpha^\prime(\beta(x))\cdot\beta^\prime(x).
</men>
</p>
</theorem>


<paragraphs  xml:id="DIGRESSIONCROrigins">
<title>DIGRESSION: The Origins of the Chain Rule</title>
<p>

  Before the invention of Calculus, arithmetic primers gave the name
  <q>The Chain Rule</q> to the computational technique that is used to,
  among other things, convert money from one currency to another. For
  example if we need to convert <m>30</m> American dollars ($) to British
  pounds (£) but we only know their values ie euros (€). Specifically we know that
<md>
<mrow>1 \text{ dollar} = 0.86\text{ euros,} \amp{}\amp{}\text{ and that }
    \amp{}\amp{} 1\text{ euro} = 0.9 \text{ pounds.}
</mrow>
</md>

(These numbers were accurate
    on the day this passage was written. They are almost certainly
    wrong on
    the day you are reading it. Don't use them to convert currency.)
  Then the conversion is
<md>
<mrow>30$ = 30  \textcolor{red}{\cancel{\text{dollars}}} \times
\frac{0.86}{1} 
  \frac{\textcolor{blue}{\cancel{ \text {euros}} } }
       {\textcolor{red}{\cancel{\text{dollars}}} } 
\amp{}   \times 
\frac{0.9}{1}
  \frac{\text{pounds}}{\textcolor{blue}{\cancel{\text{euros}}}}</mrow>
<mrow>
\amp{}=30\times0.86\times0.9\text{
  pounds}
</mrow>
<mrow>\amp{}=23.22£</mrow>
</md>
</p>

<p>
We've actually seen this type of conversion before.  In
<xref ref="CHAPTERcalc-trig">Section</xref>  we converted angular velocity to
linear velocity via the formula:
<md>
  <mrow>
    \left(\frac{3}{1}\frac{\cancel{\text{revolution}}}{{\cancel{\text{minute}}}}\right)\cdot
    \left(\frac{2\pi}{1}
    \frac{\cancel{\text{radians}}}{\cancel{\text{revolution}}}\right)\cdot
    \left(\frac{10}{1}\frac{\text{meters}}{\cancel{\text{radians}}}\right)\amp{}\cdot
    \left(\frac{1}{60}\frac{\cancel{\text{minute}}}{\text{second}}\right)
  </mrow>
  <mrow>
    \amp{}=\frac{\pi}{1} \frac{\text{meters}}{\text{second}} 
  </mrow>
  <mrow>
    \amp{}\approx 3.14
    \frac{\text{meters}}{\text{second}}.
  </mrow>
</md>
</p>
<p>

A similar chain of cancellations will occur when we differentiate a
function composition of the form
<m>\alpha(t)=\alpha(\beta(y(x(t))))</m>. We think of
<md>
 <mrow>\alpha \text{ as a function of }\beta \left(\text{ so that }
  \alpha^\prime(\beta)=\dfdx{\alpha}{\beta}\right)
</mrow>
<mrow>
\beta \text{ as a function of }y \left(\text{ so that
}\beta^\prime(y)=\dfdx{\beta}{y}\right)
</mrow>
<mrow>
 y \text{ as a function of }x \left(\text{so that}
  y^\prime(x)=\dfdx{y}{x}\right),
</mrow>
<intertext>and</intertext>
<mrow>
 x \text{ as a function of }t \left(\text{so that }
 y^\prime(x)=\dfdx{x}{t}\right).
</mrow>
</md>

</p>
<p>

Putting this all together we see that
<me>
\alpha^\prime(t) =\frac{\dx{\alpha} }{\cancel{\dx{\beta }}}
\cdot\frac{\cancel{\dx{\beta}}}{\cancel{\dx{y}}}\cdot\frac{\cancel{\dx{y}}}{\cancel{\dx{x}}}
\cdot \frac{\cancel{\dx{x} }}{\dx{t} }
= \dfdx{\alpha}{t}.
</me>
</p>
<p>

The substitution we used to make things <q>easier on your eyes</q> in
<xref ref="SECTIONAn-easy-problem-alg">Section</xref> is equivalent this chain of
cancellations.  With the invention of Calculus the older Chain Rule
for unit conversion was extended to the differentiation by
substitution technique using differentials. Eventually the older usage
was dropped and this became the
only Chain Rule. When the limit was used to provide rigor to Calculus
 the name was also applied to <xref ref="EQUATIONChainRule">equation</xref>).
</p>
<p><alert>END OF DIGRESSION</alert></p>
</paragraphs>

<p>
Understanding the Chain Rule in this form requires that we blur the
distinction between function and variable a bit. When we compute
<m>\dfdx{\alpha}{\beta}=\alpha^\prime(\beta)</m> (the derivative of
<m>\alpha</m> with with respect to <m>\beta</m>)  we view <m>\beta</m> as a
variable, but when we compute <m>\dfdx{\beta}{x}=\beta^\prime(x)</m> (the
derivative of <m>\beta</m> with respect to <m>x</m>) we view it as a function.
  
</p>
<p>
  As far as the Chain Rule is concerned it is both.
</p>
  <proof>
<p>
    Before we begin take specific notice of the assumption
    <q><m>\Delta\beta\neq0</m> near <m>x</m></q> in the statement of the Chain
    Rule. We will have
    a few comments about this in
    <xref ref="DIGRESSIONDeltaBetaNotZero">Digression: Why Assume That <m>\Delta\beta\neq0</m> Near Zero?</xref> after the proof is completed.
</p>                            
  
<p> 
 We will first establish that
  <men  xml:id="EQUATIONDeltaBetaEq0">
    <!-- \label{eq:DeltaBetaEq0} -->
    \limit{h}{0}{\Delta\beta}=0.
  </men>
</p>
<p>    
  Suppose that
  <men xml:id="EQUATIONChainRule1">
    \beta(x+h)-\beta(x)=\Delta\beta.
    <!-- \label{eq:ChainRule1} -->
  </men>
</p>  
<aside>
<title>Comment</title>
<p>
Because asserting the equality of non-existing objects
    would be meaningless we assume, implicitly that all limits in this
    argument exist.
</p>
</aside>

<p>Then

<md>
<mrow>
    \limit{h}{0}{\beta(x+h)}\amp =\limit{h}{0}{(\beta(x)+\Delta\beta)}.
</mrow>
<intertext>
By <xref ref="THEOREMLimSum2">Theorem</xref>
    we have
</intertext>
<mrow> 
   \limit{h}{0}{\beta(x+h)}\amp =\limit{h}{0}{\beta(x)}+\limit{h}{0}{\Delta\beta},
</mrow>
<intertext>
and since <m>\beta(x)</m> is differentiable at <m>x</m> we see from
    <xref ref="LEMMADiffImpliesCont">Lemma</xref> 
    that
</intertext>
<mrow> 
   \beta(x)\amp =\beta(x) +\limit{h}{0}{\Delta\beta}
</mrow>
</md>
  from  which <xref ref="EQUATIONDeltaBetaEq0"></xref> follows. 
  
  To prove the Chain Rule recall that 

<md>
<mrow>
    f^\prime(x)\amp =\limit{h}{0}{\frac{f(x+h)-f(x)}{h}}
</mrow>
<mrow>   \amp =\limit{h}{0}{\frac{\alpha(\beta(x+h))-\alpha(\beta(x))}{h}}.</mrow>
</md>
</p>

<aside>
<title>Comment</title>
<p>
In the past we
    have called this <q>uncancelling</q> <m>\Delta\beta</m>. Also, notice that
  this is where we use the assumption, <q><m>\Delta\beta\neq0</m> near
  <m>x</m>.</q>
</p>
</aside>

<p>
  Multiplying  by <m>1</m> in the form 
    <m>\textcolor{red}{\frac{\Delta\beta}{\Delta\beta}}</m>   gives
<mdn>
<mrow xml:id="EQUATIONUncancelDeltaBeta">
  <!-- \label{eq:UncancelDeltaBeta}  -->
  f^\prime(x)    \amp =\limit{h}{0}{\left(\frac{\alpha(\beta(x+h))-\alpha(\beta(x))}{\textcolor{red}{\Delta\beta}}\cdot\frac{\textcolor{red}{\Delta\beta}}{h}\right)}.
</mrow>
<intertext>From <xref ref="EQUATIONChainRule1"></xref> we see that
    <m>\textcolor{red}{\Delta\beta}=\textcolor{blue}{\beta(x+h)-\beta(x)}</m>
so</intertext>
<mrow number="no">
   \amp =\limit{h}{0}{\left(\frac{\alpha(\beta(x+h))-\alpha(\beta(x))}{\Delta\beta}\cdot\frac{\textcolor{blue}{\beta(x+h)-\beta(x)}}{h}\right)}.
</mrow>
<intertext>By <xref ref="THEOREMLimProd2">Theorem</xref> we have:</intertext>
<mrow number="no">
    \amp =\limit{h}{0}{\left(\frac{\alpha(\beta(x+h))-\alpha(\beta(x))}{\Delta\beta}\right)}\cdot\limit{h}{0}{\left(\frac{\beta(x+h)-\beta(x)}{h}\right)}.
</mrow>
<intertext>
  <xref ref="EQUATIONDeltaBetaEq0"></xref> says that <m>h\rightarrow 0</m>
    is equivalent to
  <m>\Delta\beta\rightarrow 0</m> so we have
</intertext>
<mrow number="no">
    f^\prime(x)\amp =\lim_{\textcolor{red}{\underset{\Delta\beta\rightarrow0}{\cancel{h\rightarrow0}}}}{\left(\frac{\alpha(\beta+\Delta\beta)-\alpha(\beta)}{\Delta\beta}\right)}\cdot\limit{h}{0}{\left(\frac{\beta(x+h)-\beta(x)}{h}\right)},
</mrow>
<mrow xml:id="EQUATIONChainRuleProof1">
<!-- \label{eq:ChainRuleProof1} -->
    f^\prime(x)           \amp =\underbrace{\limit{\Delta\beta}{0}{\left(\frac{\alpha(\beta+\Delta\beta)-\alpha(\beta)}{\Delta\beta}\right)}}_{=\alpha^\prime(\beta)}\cdot\underbrace{\limit{h}{0}{\left(\frac{\beta(x+h)-\beta(x)}{h}\right)}}_{=\beta^\prime(x)}.
</mrow>
<mrow xml:id="EQUATIONChainRuleBetaIsVariable">
   <!-- \label{eq:ChainRuleBetaIsVariable} -->
    f^\prime(x)
    \amp =\alpha^\prime(\beta)\cdot\beta^\prime(x).
</mrow>
</mdn>
  In <xref ref="EQUATIONChainRuleBetaIsVariable">equation</xref> <m>\beta</m> is first used
  as a variable in <m>\alpha^\prime(\beta)</m>, and then as the function
  <m>\beta(x)</m>. While this is correct, it is also poor form because it
  accentuates the dual use of <m>\beta</m>. To avoid this we usually
  express the Chain Rule as
  <me>
  f^\prime(x) =\alpha^\prime(\beta(x))\cdot\beta^\prime(x)
  </me>
  to
  emphasize that <m>x</m>, not <m>\beta</m>, is the variable.
</p>
<p>
<alert><m>\halmos</m></alert>
</p>
</proof>


<paragraphs xml:id="DIGRESSIONDeltaBetaNotZero">
<title>DIGRESSION: Why Assume That <m>\Delta\beta\neq0</m> Near Zero?</title>
<p>

  Do you see why we had to assume that <m>\Delta\beta\neq0</m> near <m>x</m>?
</p>
<p>
  
  Observe that in <xref ref="EQUATIONChainRuleProof1">equation</xref> <m>\Delta\beta</m>
  plays the same role the <m>h</m> plays in
  <xref ref="DEFINITIONDerivative"></xref>. In
  <xref ref="DEFINITIONDerivative"></xref> we were careful to insist that <m>h</m>
  could never equal zero,so if we are going to interpret
  <me>\limit{\Delta\beta}{0}{\left(\frac{\alpha(\beta+\Delta\beta)-\alpha(\beta)}{\Delta\beta}\right)}</me>
  as the derivative of <m>\alpha</m> with respect to <m>\beta</m>, as we did in
  <xref ref="EQUATIONChainRuleProof1">equation</xref>, we need to know that
  <m>\Delta\beta\neq0</m> when <m>h</m> is near zero.
</p>
<p>

  Our imposition of that constraint means that
  <xref ref="THEOREMChainRule"></xref> does not apply to any function
  <m>f(x)=\alpha(\beta(x))</m> where <m>\Delta\beta</m> might be equal to zero
  no matter how close <m>h</m> is to zero.  Fortunately, functions of that
  sort are generally the kinds of <q>pathological functions</q> that
  Poincarè is complained about in the quote at the beginning of this
  chapter. A valid proof of the Chain Rule without that constraint is
  possible, but since it would have very little relevance to anything
  we'll be doing we have chosen to prove only this weaker form of the
  Chain Rule
  </p>
  <aside>
<title>Comment</title>
<p>
You may be wondering if our choice  to go with a
    weaker form of the Chain Rule means we've given Bishop Berkeley
    cause for complaint. The answer
    is no, we haven't. If we'd left off the condition that
    <m>\Delta\beta\neq0</m>, then our proof would not have been rigorous
    because we'd have ended by claiming more than we'd proved. As it
    is, we've only claimed what we have proved.
</p>
<p>

    Rigorous does not mean perfect, it means logical.
</p>
</aside>

<p>

  If you are unsatisfied with this proof and want to see a proof of
  the stronger version of the Chain Rule, consider majoring in
  mathematics. You'll see that and much, much more. In the meantime
  try working through the following problem.
</p>

<problem>
  <introduction>
    <p>
    </p>
  </introduction>
  <task>
    <statement>
      <p>
        Show that the function
        <m>
          \beta(x)=\sin\left(\frac1x\right)
        </m>
        does not satisfy the constraint <m>\Delta\beta\neq0</m> when <m>x</m> is near
        zero.
      </p>
    </statement>
      <hint>
        <p>
          Recall <xref ref="DEFINITIONnear"></xref>.
        </p>
      </hint>
  </task>
  <task>
    <statement>
      <p>
        As a result of part (a) <xref ref="THEOREMChainRule"></xref> does
        not apply to any of the following functions at
        <m>x=0</m>. Nevertheless one of them is differentiable at <m>x=0</m>. Use
        <xref ref="DEFINITIONDerivative"></xref> to find out which one.
        
        <ol marker="i">
          <li>
            <p>
              <m>T(x)=
              \begin{cases}
              \sin\left(\frac1x\right) \amp  x\neq0\\
              0                        \amp  x=0
              \end{cases}.</m>
            </p>
          </li>
          <li>
            <p>
              <m>U(x)=
              \begin{cases}
              x\sin\left(\frac1x\right) \amp  x\neq0\\
              0                        \amp  x=0
              \end{cases}.</m>
            </p>
          </li>
          <li>
            <p>
              <m>V(x)=
              \begin{cases}
              x^2\sin\left(\frac1x\right) \amp  x\neq0\\
              0                        \amp  x=0
              \end{cases}.</m>
            </p>
          </li>
        </ol>

      </p>
    </statement>
  </task>
</problem>
<!-- \begin{embeddedproblem}  ;;; MULTILEVEL PROBLEM -->
<!--     \begin{enumerate}[label={  (\alph*)}] -->
<!--           \item   Show that the function -->
<!--       <m> -->
<!--       \beta(x)=\sin\left(\frac1x\right) -->
<!--       </m> -->
<!--       does not satisfy the constraint <m>\Delta\beta\neq0</m> when <m>x</m> is near -->
<!--       zero.\\ -->
<!--       \hint{Recall <xref ref="DEFINITIONdef:near"></xref>.} -->
<!--     \item As a result of part (a) <xref ref="THEOREMChainRule">~</xref> does -->
<!--       not apply to any of the following functions at -->
<!--       <m>x=0</m>. Nevertheless one of them is differentiable at <m>x=0</m>. Use -->
<!--       <xref ref="DEFINITIONdef:Derivative"></xref> to find out which one. -->
      
<!-- <ol> -->
<!--       <li> -->
<!-- <p> -->
<!--  <m>T(x)= -->
<!-- </p> -->
<!-- </li> -->
<!--         \begin{cases} -->
<!--           \sin\left(\frac1x\right) \amp  x\neq0\\ -->
<!--           0                        \amp  x=0 -->
<!--         \end{cases}.</m> -->
<!--       <li> -->
<!-- <p> -->
<!--  <m>U(x)= -->
<!-- </p> -->
<!-- </li> -->
<!--         \begin{cases} -->
<!--           x\sin\left(\frac1x\right) \amp  x\neq0\\ -->
<!--           0                        \amp  x=0 -->
<!--         \end{cases}.</m> -->
<!--       <li> -->
<!-- <p> -->
<!--  <m>V(x)= -->
<!-- </p> -->
<!-- </li> -->
<!--         \begin{cases} -->
<!--           x^2\sin\left(\frac1x\right) \amp  x\neq0\\ -->
<!--           0                        \amp  x=0 -->
<!--         \end{cases}.</m> -->
<!--     </ol> -->

<!--           \end{enumerate} -->
<!-- \end{embeddedproblem} -->
<p><alert>END OF DIGRESSION</alert></p>
</paragraphs>

<example xml:id="EXAMPLEDiffByCR">
<p>
  Suppose that <m>f(x)=\left(\sin(x)+\cos(x)\right)^2</m>. To use the Chain
  Rule to compute the derivative of <m>f(x)</m> we need to recognize that
  <m>f(x)</m> is the composition of <m>\alpha(x)=x^2</m>, and
  <m>\beta(x)=\sin(x)+\cos(x)</m> and then apply
  <xref ref="THEOREMChainRule"></xref> as follows.
  <md>
<mrow>
    f^\prime(x) \amp = \alpha^\prime(\beta(x))\cdot\beta^\prime(x)
</mrow>
<mrow>
    \amp = \alpha^\prime(\beta(x))\cdot(\cos(x)-\sin(x))
</mrow>
<mrow>
    \amp = \alpha^\prime(\sin(x)+\cos(x))\cdot(\cos(x)-\sin(x))
</mrow>
<mrow>
    f^\prime(x)\amp = 2
    (\sin(x)+\cos(x))\cdot(\cos(x)-\sin(x)).
</mrow>
</md>

</p>
</example>

<p>
In our opinion the Chain Rule leaves a lot to be desired as a
computational technique. But we don't have to use it that way since
<xref ref="THEOREMChainRule"></xref> validates the substitutions  we have
always used.
</p>

<exercise>
<statement>
<p>
  Suppose <m>y=f(x)=\left(\sin(x)+\cos(x)\right)^2</m>.  Compute the
  differential <m>\dx{y}</m> and then divide through by <m>\dx{x}</m> to find the
  derivative <m>\dfdx{y}{x}</m>. Confirm that it is the same as the
  derivative we found in <xref ref="EXAMPLEDiffByCR"></xref>.
</p>
</statement>
</exercise>

<problem>
  <introduction>
    <p>
      Compute <m>\dfdx{y}{x}</m> for each of the following functions by
      identifying <m>\alpha(x)</m> and <m>\beta(x)</m> such that
      <m>y(x) = \alpha(\beta(x))</m> and applying the Chain Rule. You may have
      to do this more than once for a given problem.
      
      In each case confirm that your computation is correct with an
      appropriate differential substitution.
    </p>
  </introduction>
  <task>
    <statement>
      <p>
        <m>y=(3x+5)^6</m>
      </p>
    </statement>
  </task>
  <task>
    <statement>
      <p>
        <m>y=\sec(\tan(x))</m>
      </p>
    </statement>
  </task>
  <task>
    <statement>
      <p>
        <m>y=\sqrt[7]{\frac{1}{x} +x^3}</m>
      </p>
    </statement>
  </task>
  <task>
    <statement>
      <p>
        <m>y=\left(\frac{x-x^{\frac12}}{x^3-1}\right)^2</m>
      </p>
    </statement>
  </task>
  <task>
    <statement>
      <p>
        <m>y=e^{x-\cos^2(x)}+(2x^2-3)^{\frac15}</m>
      </p>
    </statement>
  </task>
  <task>
    <statement>
      <p>
        <m>y=\sqrt{x+\sqrt[3]{2+\sqrt[4]{3-x^2}}}</m>
      </p>
    </statement>
  </task>
</problem>


</section>
<section xml:id="SECTIONproduct-rule-limits">
<title>The Product Rule</title>

<p>
A rigorous proof of the Product Rule is also fairly complex, but it
does not suffer from the kind of technical problems we encountered in
the proof of the Chain Rule.
</p>
<theorem xml:id="THEOREMLimitProdRule">
<title>The Product Rule for Differentiation</title>
<p>
  If <m>\alpha(x)</m> and <m>\beta(x)</m> are differentiable at <m>x</m> then
  <m>f(x)=\alpha(x)\cdot\beta(x)</m> is differentiable and
  <men xml:id="EQUATIONPRviaLimit2">
    <!-- \label{eq:PRviaLimit2} -->
    f^\prime(x)=\alpha(x)\cdot\beta^\prime(x)+\beta(x)\cdot\alpha^\prime(x).
  </men>
  </p>
</theorem>
<proof xml:id="PROOFLimitProdRule">
<!-- <title>\textcolor{blue}{-\beta(x)\left(\alpha(x+h)-\alpha(x)\right)}\right</title> -->
<p>
  We start with two observations. The first is that
  <men xml:id="EQUATIONPRviaLimit">
    f^\prime(x)=\limit{h}{0}{\frac{\textcolor{red}{f(x+h)}-\textcolor{blue}{f(x)}}{h}}
    =  \limit{h}{0}{\frac{\textcolor{red}
        {\alpha(x+h)\beta(x+h)}-\textcolor{blue}{\alpha(x)\beta(x)}}{h}}
    <!-- \label{eq:PRviaLimit} -->
  </men>
  and the second is that, in limit form,
  <xref ref="EQUATIONPRviaLimit2">equation</xref> is
  <men xml:id="EQUATIONPRviaLimit3">
    <!-- \label{eq:PRviaLimit3} -->
    \begin{split}f^\prime(x)=\alpha(x)\amp{}\left(\limit{h}{0}{\frac{\beta(x+h)-\beta(x)}{h}}\right)\\
\amp{}+\beta(x)\left(\limit{h}{0}{\frac{\alpha(x+h)-\alpha(x)}{h}}\right).
\end{split}
  </men>
  It appears then that our goal is simply to reorganize
  <xref ref="EQUATIONPRviaLimit">equation</xref> until it looks like
  <xref ref="EQUATIONPRviaLimit3">equation</xref>.  We say <q>simply</q> but it will only
  appear to be simple after we have succeeded. We will proceed slowly.
  </p>

<p>
  Observe that if we subtract <m>\alpha(x+h)\beta(x)</m> from the blue part
  of the numerator
  in <xref ref="EQUATIONPRviaLimit">equation</xref> we get
  <me>
  \alpha(x)\beta(x)-\alpha(x+h)\beta(x)=
  \textcolor{blue}{-\beta(x)\left(\alpha(x+h)-\alpha(x)\right)},
  </me>
  whereas if we add <m>\alpha(x+h)\beta(x)</m> to the red part of
  the numerator in
  <xref ref="EQUATIONPRviaLimit">equation</xref>  we get
  <me>
  \alpha(x+h)\beta(x+h)+\alpha(x+h)\beta(x) =
  \textcolor{red}{\alpha(x+h)\left(\beta(x+h)-\beta(x)\right)}.
  </me>
  
  This suggests that we should both add and subtract the expression
  <m>\alpha(x+h)\beta(x)</m> to the numerator of
  <xref ref="EQUATIONPRviaLimit">equation</xref>.  Doing this and factoring as we've
  indicated above we get
  <me>
  f^\prime(x)=\limit{h}{0}{ \frac{ \textcolor {red}
      {\alpha(x+h)\left(\beta(x+h)-\beta(x)\right)} -
      \left[\textcolor{blue}{-\beta(x)\left(\alpha(x+h)-\alpha(x)\right)}\right]
    }{h} },
  </me>
  
  By <xref ref="THEOREMLimSum2"></xref> we can separate this into the limit of
  the two fractions as follows:
  <me>
  f^\prime(x)=\tlimit{h}{0}{\left(\textcolor{red}{\frac{\textcolor{green}{\alpha(x+h)}\left(\beta(x+h)-{\beta(x)}\right)}{h}}\right)}
  +
  \rlimit{h}{0}{\left(\textcolor{blue}{\frac{\textcolor{green}{\beta(x)}\left(\alpha(x+h)-\alpha(x)\right)}{h}}\right)},
  </me>
  and by <xref ref="THEOREMLimProd2"></xref> we see that
  <me>
\begin{split}
  f^\prime(x)=
  \underbrace{\left[ \tlimit{h}{0}{\textcolor{green}{\alpha(x+h)}}
    \right]}_{=\alpha(x)}\amp{} \underbrace{\left[ \tlimit{h}{0}{ \left(
          \textcolor{red}{\frac{\beta(x+h)-\beta(x)}{h}} \right) }
    \right]}_{=\beta^{\prime}(x)}\\
\amp{} + \underbrace{\left[
      \tlimit{h}{0}{\textcolor{green}{\beta(x)}} \right]}_{\beta(x)}
  \underbrace{\left[ \limit{h}{0}{ \left(
          \textcolor{blue}{\frac{\alpha(x+h)-\alpha(x)}{h}} \right) },
    \right]}_{\alpha^\prime(x)}
    \end{split}
  </me>
  and therefore
  <me> f^\prime(x)=\alpha(x)\beta^\prime(x)+\beta(x)\alpha^\prime(x).</me>
</p>
<p>
<alert><m>\halmos</m></alert>
</p>
</proof>

<!-- \begin{myproof} -->
<!--   We start with the two observations. The first is that -->
<!--   <me> -->
<!--     f^\prime(x)=\limit{h}{0}{\frac{\textcolor{red}{f(x+h)}-\textcolor{blue}{f(x)}}{h}} -->
<!--     =  \limit{h}{0}{\frac{\textcolor{red} -->
<!--         {\alpha(x+h)\beta(x+h)}-\textcolor{blue}{\alpha(x)\beta(x)}}{h}} -->
<!--     \label{eq:PRviaLimit} -->
<!--   </me> -->
<!--   and the second is that, in limit form, -->
<!--   <xref ref="EQUATIONPRviaLimit2"></xref> is -->
<!--   <me> -->
<!--     \label{eq:PRviaLimit3} -->
<!--     f^\prime(x)=\alpha(x)\left(\limit{h}{0}{\frac{\beta(x+h)-\beta(x)}{h}}\right)+\beta(x)\left(\limit{h}{0}{\frac{\alpha(x+h)-\alpha(x)}{h}}\right). -->
<!--   </me> -->
<!--   It appears then that our goal is to simply reorganize -->
<!--   <xref ref="EQUATIONPRviaLimit"></xref> until it looks like -->
<!--   <xref ref="EQUATIONPRviaLimit3"></xref>.  We say <q>simply</q> but it will only -->
<!--   appear to be simple after we have succeeded. We will proceed slowly. -->
  
<!--   Observe that if we subtract <m>\alpha(x+h)\beta(x)</m> from the blue part -->
<!--   of the numerator -->
<!--   in <xref ref="EQUATIONPRviaLimit"></xref> we get -->
<!--   <me> -->
<!--   \alpha(x)\beta(x)-\alpha(x+h)\beta(x)= -->
<!--   \textcolor{blue}{-\beta(x)\left(\alpha(x+h)-\alpha(x)\right)}, -->
<!--   </me> -->
<!--   whereas if we add <m>\alpha(x+h)\beta(x)</m> to the red part of -->
<!--   the numerator in -->
<!--   <xref ref="EQUATIONPRviaLimit"></xref>  we get -->
<!--   <me> -->
<!--   \alpha(x+h)\beta(x+h)+\alpha(x+h)\beta(x) = -->
<!--   \textcolor{red}{\alpha(x+h)\left(\beta(x+h)-\beta(x)\right)}. -->
<!--   </me> -->
  
<!--   This suggests that we should both add and subtract the expression -->
<!--   <m>\alpha(x+h)\beta(x)</m> to the numerator of -->
<!--   <xref ref="EQUATIONPRviaLimit"></xref>.  Doing this and factoring as we've -->
<!--   indicated above we get -->
<!--   <me> -->
<!--   f^\prime(x)=\limit{h}{0}{ \frac{ \textcolor {red} -->
<!--       {\alpha(x+h)\left(\beta(x+h)-\beta(x)\right)} - -->
<!--       \left[\textcolor{blue}{-\beta(x)\left(\alpha(x+h)-\alpha(x)\right)}\right] -->
<!--     }{h} }, -->
<!--   </me> -->
  
<!--   By <xref ref="THEOREMLimSum2">~</xref> we can separate this into the limit of -->
<!--   the two fractions as follows: -->
<!--   <me> -->
<!--   f^\prime(x)=\tlimit{h}{0}{\left(\textcolor{red}{\frac{\textcolor{green}{\alpha(x+h)}\left(\beta(x+h)-{\beta(x)}\right)}{h}}\right)} -->
<!--   + -->
<!--   \rlimit{h}{0}{\left(\textcolor{blue}{\frac{\textcolor{green}{\beta(x)}\left(\alpha(x+h)-\alpha(x)\right)}{h}}\right)}, -->
<!--   </me> -->
<!--   and by <xref ref="THEOREMLimProd2">~</xref> we see that -->
<!--   <me> -->
<!--   f^\prime(x)= -->
<!--   \underbrace{\left[ \tlimit{h}{0}{\textcolor{green}{\alpha(x+h)}} -->
<!--     \right]}_{=\alpha(x)} \underbrace{\left[ \tlimit{h}{0}{ \left( -->
<!--           \textcolor{red}{\frac{\beta(x+h)-\beta(x)}{h}} \right) } -->
<!--     \right]}_{=\beta^{\prime}(x)} + \underbrace{\left[ -->
<!--       \tlimit{h}{0}{\textcolor{green}{\beta(x)}} \right]}_{\beta(x)} -->
<!--   \underbrace{\left[ \limit{h}{0}{ \left( -->
<!--           \textcolor{blue}{\frac{\alpha(x+h)-\alpha(x)}{h}} \right) }, -->
<!--     \right]}_{\alpha^\prime(x)} -->
<!--   </me> -->
<!--   and therefore -->
<!--   <me> f^\prime(x)=\alpha(x)\beta^\prime(x)+\beta(x)\alpha^\prime(x).</me> -->
<!-- \end{myproof} -->

</section>
<section xml:id="SECTIONother-gener-diff">
<title>The Other General Differentiation Rules</title>

<theorem xml:id="THEOREMQuotientRuleLimit">
<title>The Quotient Rule for Differentiation</title>
<p>
  <!-- \label{THEOREMQuotientRuleLimit} -->
  We assume that <m>\alpha(x)</m>, <m>\beta(x)</m>, and
  <m>f(x)=\frac{\alpha(x)}{\beta(x)}</m> are all differentiable
  functions\aside{Notice that we have explicitly assumed that the
    quotient }.
  Assume further that <m>\beta(x)\neq0</m>.  Then
  <me>
  f^\prime(x)=\frac{\beta(x)\alpha^\prime(x)-\alpha(x)\beta^\prime(x)}{\left[\beta(x)\right]^2}.
  </me>
</p>
</theorem>
<p>
Proving this directly by using limits would be unpleasant, but as we
observed in <xref ref="CHAPTERdifferentials"></xref> the Quotient Rule can be
viewed as a rearranged version of the Product Rule.
</p>

<problem>
<statement>
<p>
  Use the Product Rule to derive the Quotient Rule.
</p>
</statement>
<hint>
<p>
First solve <m>f(x)=\frac{\alpha(x)}{\beta(x)}</m> for
    <m>\alpha(x)</m>.
</p>
</hint>
</problem>

<p>
With the Product Rule for Differentiation in place we now have the
tools needed to prove the Power Rule for Positive Integer Exponents.
The method of proof we outline in the following problem is called
<term>Mathematical Induction</term>  and it can be used in other
contexts as well. In fact, most of the <q>Find the Pattern</q> problems
in this text require an Induction argument for full rigor.
</p>

<problem xml:id="PROBLEMPowRulInduction">
  <title>The Power Rule for Positive Integer Exponents</title>
<introduction>
    <p>
      Assume  that <m>\alpha(x)=x^n</m> is differentiable at <m>x</m> for
      any positive integer <m>n</m>.
    </p>
</introduction>
  <task>
    <statement>
      <p>
        Assume that <m>n=1</m>.  Use the limit definition to    show
        that <m>\alpha^\prime(x) = nx^{n-1}.</m> (This says, <q>The
        Power Rule holds for <m>k=1</m>.</q>)
      </p>
    </statement>
  </task>
  <task>
    <statement>
      <p>
        Now assume that the Power Rule for Positive Integer Exponents
        holds for <m>n=k</m>, where <m>k</m> is an arbitrary, fixed
        positive integer.  Let <m>\beta(x)=x^{k+1}</m> and show that
        <m>\beta^\prime(x)=(k+1)x^k.</m> (This says, <q>If the Power
        Rule holds for <m>k</m> then it must also hold for
        <m>k+1</m>.</q>)
      </p>
    </statement>
  </task>


<!--   \begin{description} -->
<!--   \item[   Part 1:] Assume that <m>n=1</m>.  Use the limit definition to -->
<!--     show that <m>\alpha^\prime(x) = nx^{n-1}.</m> \\ -->
<!--     \comment{This says, <q>The Power Rule holds for <m>k=1</m>.</q>} -->
<!-- \item[   Part 2:] Now assume that the Power Rule for Positive -->
<!--   Integer Exponents holds for <m>n=k</m>, where <m>k</m> is an arbitrary, fixed -->
<!--   positive integer. -->
    
<!--     Let <m>\beta(x)=x^{k+1}</m> and show that <m>\beta^\prime(x)=(k+1)x^k.</m>\\ -->
<!--     \comment{This says, <q>If the Power Rule holds for <m>k</m> then it -->
<!--     must also hold for <m>k+1</m>.</q>} -->
<!--   \end{description} -->
<task>
<statement>
<p>
  Do you see how this proves that the Power Rule holds for any
  positive integer, <m>n</m>? Write a short paragraph explaining the logic
  behind this.
</p>
</statement>
</task>
</problem>

<p>
  Parts (a) and (b) of this problem constitute a
  proof of the Power Rule for Positive Integer Exponents by
  <term>Mathematical Induction</term>, and part (c) asks you to
  explain the underlying logic. Although we didn<rsq/>t  mention it at
  the time, Mathematical
  Induction was the underlying idea in part (d) of <xref
  ref="PROBLEMPR-integer"></xref> as well.
</p>



<p>
With the Power Rule for Positive Integer Exponents in place we can
extend it to both negative and rational exponents in the same way we
did it in <xref ref="CHAPTERdifferentials"></xref>. The following
problem is essentially a repeat of <xref ref="PROBLEMPRPosRat"></xref>
<xref ref="PROBLEMRecip">and </xref>, using Lagrange's prime notation,
and function notation, rather than differentials.
</p>

<problem>
<title>The Power Rule for Rational and Negative Exponents</title>

<introduction>
<p>
</p>
</introduction>
<task>
<statement>
<p>
 Assume <m>n</m> is a positive integer and that <m>\alpha(x)=x^{-n}</m> is
    differentiable.  Show that
    <me>\alpha^\prime(x) = -nx^{-(n+1)}.</me>
  </p>
</statement>
<hint>
<p>
Rewrite <m>\alpha(x)=x^{-n}</m> as <m>\frac{1}{x^n}</m> and use the
      Quotient Rule for Differentiation and the Power Rule for positive integers.
</p>
</hint>
</task>
<task>
<statement>
<p>
 Assume that <m>q</m> is a non-zero integer and that
    <m>\alpha(x)=x^{1/q}</m> is differentiable at <m>x</m>.  Show
    that <me>\alpha^\prime(x) = (1/q)x^{(1/q-1)}.</me>
  </p>
</statement>
<hint>
<p>
Rewrite <m>\alpha(x)=x^{1/q}</m> as
      <me>\left[\alpha(x)\right]^q=x</me> and use the Chain Rule and
      the Power Rule for positive integers.
</p>
</hint>
</task>
<task>
<statement>
<p>
 Assume that <m>p</m> and <m>q</m> are integers,  <m>q\neq0</m>, and that
    <m>\alpha(x)=x^{p/q}</m> is differentiable at <m>x</m>.  Show
    that <me>\alpha^\prime(x) = (p/q)x^{(p/q-1)}.</me>
  </p>
</statement>
<hint>
<p>
Rewrite <m>\alpha(x)=x^{p/q}</m> as
      <m>\alpha(x)=\left(x^{1/q}\right)^p</m> and use the Chain Rule and
      part (b).
</p>
</hint>
</task>
</problem>
<!-- \begin{embeddedproblem-enumerate}[The Power Rule for Rational and Negative Exponents] -->
<!--   \begin{enumerate}[label={  (\alph*)}] -->
<!--   \item Assume <m>n</m> is a positive integer and that <m>\alpha(x)=x^{-n}</m> is -->
<!--     differentiable.  Show that -->
<!--     <me>\alpha^\prime(x) = -nx^{-(n+1)}.</me> -->
    
<!--     \hint{Rewrite <m>\alpha(x)=x^{-n}</m> as <m>\frac{1}{x^n}</m> and use the -->
<!--       Quotient Rule for Differentiation and the Power Rule for positive integers.} -->
<!--   \item Assume that <m>q</m> is a non-zero integer and that -->
<!--     <m>\alpha(x)=x^{1/q}</m> is differentiable at <m>x</m>.  Show -->
<!--     that <me>\alpha^\prime(x) = (1/q)x^{(1/q-1)}.</me> -->
    
<!--     \hint{Rewrite <m>\alpha(x)=x^{1/q}</m> as -->
<!--       <me>\left[\alpha(x)\right]^q=x</me> and use the Chain Rule and -->
<!--       the Power Rule for positive integers.} -->
<!--   \item Assume that <m>p</m> and <m>q</m> are integers,  <m>q\neq0</m>, and that -->
<!--     <m>\alpha(x)=x^{p/q}</m> is differentiable at <m>x</m>.  Show -->
<!--     that <me>\alpha^\prime(x) = (p/q)x^{(p/q-1)}.</me> -->
    
<!--     \hint{Rewrite <m>\alpha(x)=x^{p/q}</m> as -->
<!--       <m>\alpha(x)=\left(x^{1/q}\right)^p</m> and use the Chain Rule and -->
<!--       part (b).} -->
<!--   \end{enumerate} -->
<!-- \end{embeddedproblem-enumerate} -->
<p>
 Together the previous two problems prove the Power Rule
for rational exponents:
</p>

<theorem xml:id="THEOREMPowerRuleLimit">
<title>The Power Rule for Rational Exponents</title>
<p>
  Assume that <m>p</m> and <m>q</m> are integers, <m>q\neq0</m>, and that
  <m>\alpha(x)=x^{p/q}</m> is differentiable at <m>x</m>. Then
  <me>
  \alpha^\prime(x) = (p/q)x^{(p/q-1)}.
  </me>
</p>
</theorem>

<p>

In the statement of <xref ref="THEOREMQuotientRuleLimit"></xref> we
explicitly assumed that the quotient, <m>\frac{\alpha(x)}{\beta(x)}</m>, is
differentiable at <m>x</m>. This has the effect that the theorem does not
necessarily apply to all possible quotients, in the same way that when
we add <m>\Delta\beta\neq0</m> to the statement of the Chain Rule, the
theorem applies to fewer compositions. And just like the Chain Rule
the functions that <xref ref="THEOREMQuotientRuleLimit"></xref> does not
apply to are mostly pathological, and of no use to us right now.
</p>
<p>

We added the same assumption to <xref ref="THEOREMPowerRuleLimit"></xref> for
similar reasons.
</p>

<paragraphs  xml:id="DIGRESSIONRUAMathematician">
<title>DIGRESSION: Are You a Mathematician?</title>
<p>

  If leaving these theorems incomplete in this way is troubling to
  you then you are almost certainly a mathematician by temperament. If
  you haven't decided on a major yet, consider mathematics. You
  obviously like it. Why not learn more?
</p>
<p>

  If you find that you simply don't care about completing all of the
  details and you are not majoring in mathematics, congratulations!
  You've made the right choice.
</p>
<p>

  <xref ref="PROBLEMQR-rigor"></xref>  will lead
  you through the steps necessary to prove the Quotient
  Rule for Differentiation without the assumption that
  <m>\frac{\alpha(x)}{\beta(x)}</m> is differentiable. Have fun!
</p>
  
<problem xml:id="PROBLEMQR-rigor">
<introduction>
<p>
  Assume that <m>\alpha(x)</m> and <m>\beta(x)</m> are
  differentiable and that <m>\beta(x)\neq0</m>, but we make no assumption
  about the differentiability of <m>f(x)=\frac{\alpha(x)}{\beta(x)}</m>.
</p>
</introduction>
<task>
<statement>
<p>
 First prove the special case of the Quotient Rule where
    <m>g(x)=\frac{1}{\beta(x)}</m>.
    
<ol marker="i">
    <li>
<p>
 Use the limit definition to show that
      <m>g^\prime(x)=\limit{h}{0}{\frac{\beta(x)-\beta(x+h)}{h\beta(x)\beta(x+h)}}</m>.
</p>
</li>
    <li>
<p>
 Now evaluate the limit in part (i) to show that
      <m>f^\prime(x)=
      \frac{-\beta^\prime(x)}{\left[\beta(x)\right]^2}</m>.
</p>
</li>
    </ol>

  </p>
</statement>
</task>
<task>
<statement>
<p>
 Use the Product Rule for Differentiation and the Chain Rule (along
 with the result of part a) to show that
 <m>f(x)=\frac{\alpha(x)}{\beta(x)}</m> is differentiable at <m>x</m>
 and that
 <me>\displaystyle f^\prime(x) =
 \frac{\beta(x)\alpha^\prime(x) -
 \alpha(x)\beta^\prime(x)}{\left[\beta(x)\right]^2}.  </me>
  </p>
</statement>
</task>
</problem>
<!-- \begin{embeddedproblem} -->
<!--   <!-\- \label{problem:QR-rigor} -\-> -->
<!--   Assume that <m>\alpha(x)</m> and <m>\beta(x)</m> are -->
<!--   differentiable and that <m>\beta(x)\neq0</m>, but we make no assumption -->
<!--   about the differentiability of <m>f(x)=\frac{\alpha(x)}{\beta(x)}</m>. -->
<!--   \begin{enumerate}[label={  (\alph*)}] -->
<!--   \item First prove the special case of the Quotient Rule where -->
<!--     <m>f(x)=\frac{1}{\beta(x)}</m>. -->
    
<!-- <ol> -->
<!--     <li> -->
<!-- <p> -->
<!--  Use the limit definition to show that -->
<!-- </p> -->
<!-- </li> -->
<!--       <m>\beta^\prime(x)=\limit{h}{0}{\frac{\beta(x)-\beta(x+h)}{h\beta(x)\beta(x+h)}}</m>. -->
<!--     <li> -->
<!-- <p> -->
<!--  Now evaluate the limit in part (i) to show that -->
<!-- </p> -->
<!-- </li> -->
<!--       <m>f^\prime(x)= -->
<!--       \frac{-\beta^\prime(x)}{\left[\beta(x)\right]^2}</m>. -->
<!--     </ol> -->

<!--   \item Use the Product Rule for Differentiation and the  Chain Rule (along with the result -->
<!--     of part a) to show that <m>f(x)=\frac{\alpha(x)}{\beta(x)}</m> is differentiable at <m>x</m> and that\\ -->
<!--     \centerline{<m>\displaystyle f^\prime(x) = -->
<!--       \frac{\beta(x)\alpha^\prime(x) - -->
<!--         \alpha(x)\beta^\prime(x)}{\left[\beta(x)\right]^2}.  </m>} -->
<!--   \end{enumerate} -->
<!-- \end{embeddedproblem} -->
<p>
  <xref ref="PROBLEMPR-rigor2"></xref> will lead
  you through the steps necessary to prove the Product
  Rule for Rational Exponents without the assumption that
  <m>x^{\frac{p}{q}}</m> is differentiable. It relies on the result of
    <xref ref="PROBLEMPR-rigor1"></xref>.
    Have fun!
</p>  
  <problem xml:id="PROBLEMPR-rigor1">
    <introduction>
      <p>
        To prove  <xref ref="THEOREMPowerRuleLimit"></xref> we will first focus
        on the special case of 
        <me>
          \beta(x)=x^{\frac{1}{q}},
        </me> 
        where 
        <m>q</m> is a
        non-negative integer.
      </p>
      <p>
        The key to proving this special case is a generalization of the
        difference of squares formula: <me>(a-b)(a+b)=a^2-b^2.</me>
        
      </p>
    </introduction>
<task>
<statement>
<p>
 Show that <m>(a-b)(a^2+ab+b^2)=a^3-b^3</m>.
    </p>
</statement>
</task>
<task>
<statement>
<p>
 Show that <m>(a-b)(a^3+a^2b+ab^2+b^3)=a^4-b^4</m>.
    </p>
</statement>
</task>
<task>
<statement>
<p>
 Use <term>Mathematical Induction</term> to show that
      <me>(a-b)(a^{q-1}+a^{q-2}b+a^{q-3}b^2+\cdots+ab^{q-2}+b^{q-1})=a^q-b^q</me>.
    </p>
</statement>
</task>
</problem>
<!-- \begin{embeddedproblem} -->
  <!--       <!-\- \label{problem:PR-rigor1} -\-> -->
  <!--   To prove  <xref ref="THEOREMPowerRuleLimit">~</xref> we will first focus -->
  <!--   on the special case of <m>\beta(x)=x^{\frac{1}{q}}</m>, <m>q</m> is a -->
  <!--   non-negative integer. -->
    
  <!--   The key to proving this special case is a generalization of the -->
  <!--   difference of squares formula: <m>(a-b)(a+b)=a^2-b^2.</m> -->
    
  <!--   \begin{enumerate}[label={  (\alph*)}] -->
  <!--   \item Show that <m>(a-b)(a^2+ab+b^2)=a^3-b^3</m>. -->
  <!--   \item Show that <m>(a-b)(a^3+a^2b+ab^2+b^3)=a^4-b^4</m>. -->
  <!--   \item Use \term{Mathematical Induction} to show that\\ -->
  <!--     \centerline{<m>(a-b)(a^{q-1}+a^{q-2}b+a^{q-3}b^2+\cdots+ab^{q-2}+b^{q-1})=a^q-b^q</m>.} -->
  <!--   \end{enumerate} -->
  <!--     \end{embeddedproblem} -->
  

  <problem xml:id="PROBLEMPR-rigor2">
<introduction>
<p>
    Assume that <m>p</m> and <m>q</m> are integers and that <m>q\neq0</m>. If we
    apply <xref ref="DEFINITIONDerivative"></xref> to <m>f(x)=x^\frac{1}{q}</m>, we
    get
<me>
      f^\prime(x)=\limit{h}{0}{\frac{(x+h)^\frac1q-x^\frac1q}{h}}.  
</me>
</p>
</introduction>
<task>
<statement>
<p>
 Use the substitutions <m>a=(x+h)^\frac1q</m>, <m>b=x^\frac1q</m>,
      and part (c) of the previous problem to show that
      <me>
      f^\prime(x)=\limit{a}{b}{\frac{a-b}{a^q-b^q}}=\frac{1}{qb^{q-1}}.
      </me>
    </p>
</statement>
</task>
<task>
<statement>
<p>
 Substitute <m>b=x^{\frac1q}</m> into the result of part a
      to obtain
      <me>
      f^\prime(x)=\frac1qx^{\frac1q-1}.
      </me>
    </p>
</statement>
</task>
<task>
<statement>
<p>
 Use the Chain Rule to show that for
      <m>\alpha(x)=x^{\frac{p}{q}}</m>
      <me>
      \alpha^\prime(x)=\frac{p}{q}x^{\frac{p}{q}-1}.
      </me>
    </p>
</statement>
</task>
</problem>
<!-- \begin{embeddedproblem} -->
  <!--   <!-\- \label{problem:PR-rigor2} -\-> -->
  <!--   Assume that <m>p</m> and <m>q</m> are integers and that <m>q\neq0</m>. If we -->
  <!--   apply <xref ref="DEFINITIONdef:Derivative"></xref> to <m>f(x)=x^\frac{1}{q}</m>, we -->
  <!--   get\\ -->
  <!--   \centerline{<m> -->
  <!--     f^\prime(x)=\limit{h}{0}{\frac{(x+h)^\frac1q-x^\frac1q}{h}}.  </m>} -->
  <!--   \begin{enumerate}[label={  (\alph*)}] -->
  <!--   \item Use the substitutions <m>a=(x+h)^\frac1q</m>, <m>b=x^\frac1q</m>, -->
  <!--     and part (c) of the previous problem to show that -->
  <!--     <me> -->
  <!--     f^\prime(x)=\limit{a}{b}{\frac{a-b}{a^q-b^q}}=\frac{1}{qb^{q-1}}. -->
  <!--     </me> -->
  <!--   \item Substitute <m>b=x^{\frac1q}</m> into the result of part a -->
  <!--     to obtain -->
  <!--     <me> -->
  <!--     f^\prime(x)=\frac1qx^{\frac1q-1}. -->
  <!--     </me> -->
  <!--   \item Use the Chain Rule to show that for -->
  <!--     <m>\alpha(x)=x^{\frac{p}{q}}</m> -->
  <!--     <me> -->
  <!--     \alpha^\prime(x)=\frac{p}{q}x^{\frac{p}{q}-1}. -->
  <!--     </me> -->
  <!--   \end{enumerate} -->
  <!-- \end{embeddedproblem} -->
<p><alert>END OF DIGRESSION</alert></p>
</paragraphs>



</section>
<section xml:id="SECTIONdiff-trig-funct">
<title>Derivatives of the Trigonometric Functions, via Limits</title>

<p>
\TLogo{PSP:deriv-sine-cosine}
</p>

<theorem xml:id="THEOREMSineDeriv">
<title> Derivative of <m>\sin(x)</m></title>
<p>
  Suppose <m>\alpha(\theta)=\sin(\theta)</m>. Then
  <m> \alpha^\prime(\theta)=\cos(\theta).  </m>
</p>
</theorem>

<proof xml:id="PROOFSineDeriv">
<p>
  Showing that the derivative of <m>\sin(\theta)</m> is <m>\cos(\theta)</m> is
  mostly straightforward but we're going to hit a snag partway
  through. We'll proceed for a bit to see where the trouble is.
  
  Start with the limit definition:
  <me>    \alpha^\prime(\theta) =
  \limit{h}{0} { \frac{\sin(\theta+h)-\sin(\theta)}{h}}
  </me>
  In the numerator we see the expression <m>\sin(\theta+h)</m>.  Recall the
  sum formula for the Sine:
  <me>
  \sin(A+B)=\sin(A)\cos(B)+\cos(A)\sin(B).
  </me>
  Taking <m>A=\theta</m> and
  <m>B=h</m> we have:

<md>
  <mrow>
    \alpha^\prime(x)  \amp =    \limit{h}{0}{\frac{\textcolor{red}{\sin(\theta)}\cos(h)+\cos(\theta)\sin(h)-\textcolor{red}{\sin(\theta)}}{h}}
  </mrow> 
  <intertext>
Next, if we factor \textcolor{red}{<m>\sin(\theta)</m>} out
    of the terms where it appears and rearrange the numerator a bit we
    have:
  </intertext>
<mrow>    
\amp =
    \limit{h}{0}{\frac{\textcolor{red}{\sin(\theta)}(\cos(h)-1)+\cos(\theta)\sin(h)}{h}}.
</mrow>
<mrow>
    \amp =
    \limit{h}{0}{\left(\frac{\sin(\theta)(\cos(h)-1)}{h}+\frac{\cos(\theta)\sin(h)}{h}\right)}.
</mrow>
<intertext>
By <xref ref="THEOREMLimSum2"></xref>:
</intertext>
<mrow>
    \amp =
    \limit{h}{0}{\frac{\sin(\theta)(\cos(h)-1)}{h}}+\limit{h}{0}{\frac{\cos(\theta)\sin(h)}{h}}
</mrow>
<intertext>and by <xref ref="COROLLARYConstMultLimit"></xref>:</intertext>
<mrow>
    \amp =
    \sin(\theta)\underbrace{\left(\limit{h}{0}{\frac{(\cos(h)-1)}{h}}\right)}_{=0}+\cos(\theta)\underbrace{\left(\limit{h}{0}{\frac{\sin(h)}{h}}\right)}_{=1}.
</mrow>
</md>
  If the values of the two limits are <m>0</m> and <m>1</m> respectively as we've
  indicated we can conclude that <m>\alpha^\prime(\theta)=\cos(\theta)</m>.
  
  
  Unfortunately this proof cannot be considered complete until we have
  shown that these last two limits are what we claim they are. We will
  do this via the two  lemmas below.
  
</p>
<p>
<alert><m>\halmos</m></alert>
</p>
</proof>

<p>
It is tempting to use L<rsq/>Hôpital<rsq/>s Rule to evaluate the two
limits at the end of <xref ref="PROOFSineDeriv"></xref>,
especially since it is so very easy to do. See <xref
ref="DRILLCircularReasoning"></xref> below.
</p>

<exercise xml:id="DRILLCircularReasoning">
<statement>
<p>
  Use L<rsq/>Hôpital<rsq/>s Rule to show
<ol cols="2" marker="i">
<li>
  <m>\tlimit{h}{0}{\frac{(\cos(h)-1)}{h}}=0</m> 
</li>
<li><m>\tlimit{h}{0}{\frac{\sin(h)}{h}}=1</m>.</li>
</ol>
</p>
</statement>
</exercise>
<p>
Sadly, using <xref ref="DRILLCircularReasoning"></xref> to finish the proof
of <xref ref="THEOREMSineDeriv"></xref> is an example of circular
reasoning. We can't use the fact that the derivative of <m>\sin(x)</m> is
<m>\cos(x)</m> to prove that the derivative of <m>\sin(x)</m> is <m>\cos(x)</m>. So
we will have to find a way to evaluate these limits without using
L<rsq/>Hôpital<rsq/>s Rule.
</p>

<lemma xml:id="LEMMASinOverh">
<p>
  <m>\limit{h}{0}{\frac{\sin(h)}{h}}=1</m>
</p>
</lemma>

<proof xml:id="PROOFSinOverh">
<title>Proof</title>
  <image source="images/SqueezeThm2.png" width="50%"/>
<!-- \begin{wrapfigure}[]{r}{3in} -->
<!--      \vskip-8mm  -->
<!--     \captionsetup{labelformat=empty} -->
<!--     \centerline{\includegraphics*[height=2in,width=3in]{../Figures/SqueezeThm2}} -->
<!--     \label{fig:SqueezeThm2} -->
<!--   \end{wrapfigure} -->

<p>
  There are two cases:
</p>


<p>
  <alert>Case 1</alert> <m>\theta\ge0</m>: We will use the
  Squeeze Theorem.  Recall that in <xref ref="SECTIONtrig-interl"></xref> we
  observed that the lengths of certain line segments associated with
  the unit circle in the first quadrant are equal to the trigonometric
  functions. The figure at the right shows the relationship between
  <m>\theta</m>, <m>\sin(\theta)</m>, and <m>\tan(\theta)</m>. Notice in particular
  that
  <me>
  \sin(\theta)\le\theta\le\tan(\theta).
  </me>
  
  Dividing each expression in the inequality by <m>\sin(\theta)</m>
  almost does the trick:
  <me>
  1\le\frac{\theta}{\sin(\theta)}\le\frac{1}{\cos(\theta)}.
  </me>
  In the center we now have the reciprocal of what we need, so we need
  to invert each expression.
  
  However, keep in mind that these are not equations they are inequalities. When we
  invert
  an inequality we must reverse its sense. This gives
  <me>
  1\ge\frac{\sin(\theta)}{\theta}\ge\frac{\cos(\theta)}{1},
  </me>
  and
  this is true on the interval <m>\left[0,\frac{\pi}{2}\right]</m>.
  Since
  <me>
  \rlimit{\theta}{0}{1}=\rlimit{\theta}{0}{\cos(\theta)}=1
  </me>
  the Squeeze Theorem applies, and we conclude that
  <me>
  \rlimit{\theta}{0}{\frac{\sin(\theta)}{\theta}}=1.
  </me>
</p>
<p>
  <alert>Case 2</alert> <m>\theta\lt0</m>: For this case
  notice that <m>\sin(-\theta)=-\sin(\theta)</m> so that
  <m> \frac{\sin(-\theta)}{-\theta}=\frac{\sin(\theta)}{\theta}</m>. We
  make the substitution <m>\theta=-\phi</m> where <m>\phi\gt0</m>.  Therefore
  when <m>\theta\lt0</m> we have
  <me>
  \llimit{\theta}{0}{\frac{\sin(\theta)}{\theta}}
  =\rlimit{\phi}{0}{\frac{\sin(-\phi)}{-\phi}}
  =\rlimit{\phi}{0}{\frac{\sin(\phi)}{\phi}} =1
  </me>
  by Case 1.
</p>

<p>
<alert><m>\halmos</m></alert>
</p>
</proof>


<problem xml:id="PROBLEMCosM1Overh">
<statement>
<p>
  Show that <m>\tlimit{h}{0}{\frac{(\cos(h)-1)}{h}}=0</m>.\\
  \hint{It is tempting to model this proof on the proof of
    <xref ref="LEMMASinOverh"></xref>. While this can be done, it is 
    delicate. It is simpler to multiply by <m>1</m> in the form
    <m>\frac{\cos(h)+1}{\cos(h)+1}</m>. Try that instead.}
</p>
</statement>
</problem>
<p>
Once <xref ref="PROBLEMCosM1Overh"></xref> has been solved the 
proof that <m>\dfdx{(\sin(x))}{x}= \cos(x)</m> is complete.
</p>

<problem>
<statement>
<p>
  Prove that <m>\dfdx{(\cos(\theta))}{\theta} = -\sin(\theta)</m>, using  the proof of <xref ref="THEOREMSineDeriv"></xref> as a guide.
</p>
</statement>
</problem>
<p>
Assuming that <m>\tan(\theta)</m>, <m>\cot(\theta)</m>, <m>\sec(\theta)</m>, and
<m>\csc(\theta)</m> are differentiable we can now use
<xref ref="PROBLEMQR-rigor"></xref> to find their derivatives as
well. Since this is exactly what we did in
<xref ref="SECTIONdiff-other-trig"></xref> we have the derivatives of all of
the trigonometric functions.
</p>

</section>
<section xml:id="SECTIONinverse-functions">
<title>Inverse Functions</title>

<p>
Although we have worked with the inverses of some specific functions
we have not formally defined what we mean by an inverse. We will
remedy that now. We have seen that not all functions can be inverted
(see for example, <xref ref="DIGRESSIONTangentHasNoInverse"></xref>)
so the first step is to define which functions are invertible.
</p>
<p>

Informally a function that never takes the same value twice is called
a <term>one-to-one function</term>. Formally we have the following.
</p>
<aside>
<title>Mathematical Terminology</title>
<p>
They are also called <term>injective</term>.
</p>
</aside>

<definition xml:id="DEFINITIONBijectiveFunctions">
<title>One-To-One Functions</title>
<statement>
<p>
  A function, <m>f(x)</m>, defined on a domain, <m>D</m>, is said to be
  one-to-one if, whenever <m>x_1</m> and <m> x_2</m> are in <m>D</m> and <m>x_1\neq
  x_2</m> then, <m>f(x_1)\neq f(x_2)</m>. 

</p>
</statement>
</definition>
<p>
Recall that
when we tried to invert <m>\tan(x)</m> (which is not one-to-one) in
<xref ref="SECTIONwitch-agnesi-inverse"></xref> we got the multifunction
<m>\arctan(x)</m>. We had to restrict the domain of the tangent function to
<m>\frac{-\pi}{2}\le x \le \frac{\pi}{2}</m>, in order to find an
inverse. That restriction gave us a one-to-one function which we could
invert because one-to-one functions are the only functions with inverses. 
</p>

<definition xml:id="DEFINITIONFunctionInverse">
<title>Inverse Functions</title>
<statement>
<p>
  <!-- \label{def:FunctionInverse} -->
  Suppose <m>f(x)</m>, with domain <m>D</m> and range, <m>R</m> is a one-to-one
  function. Then the inverse of <m>f(x)</m> is the function <m>\inverse f(x)</m>
  with domain <m>R</m> and range <m>D</m> which satisfies the
  following properties (Notice that the domain and range
    have been swapped.):
    <ol>
      <li>
        <p>
          <m>f\left(\inverse f(x)\right)=x</m>
        </p>
      </li>
      <li>
        <p>
          <m>\inverse f\left( f(x)\right)=x</m>
        </p>
      </li>
    </ol>
    for every value of <m>x</m> in the domain of <m>f</m>
    (equivalently, in the range of <m>\inverse f</m>).

</p>
</statement>
</definition>

<p>
Loosely speaking, <xref ref="DEFINITIONFunctionInverse"></xref> says that two
functions are mutually inverse if they <q>undo</q> each other.
</p>
<p>

Our next task is to show that the derivatives of the in inverse
trigonometric functions are what we expect them to be. Given that we
have now obtained the derivatives of all of the trigonometric
functions it appears that we could proceed just as we did in
<xref ref="SECTIONdiff-inverse-tancot"></xref> and
<xref ref="SECTIONother-inverse-trig"></xref>.
</p>
<p>

But that would require that we explicitly assume that each of the
inverse trigonometric functions is differentiable, similar to the way
we found the derivative of a quotient. This is a valid approach of
course, but proceeding in that manner would mask some issues that will
be of interest to us later. So we will approach the derivatives of
inverse functions abstractly by (rigorously) finding a formula for the
derivative of the inverse of a generic, invertible function. After
that we'll only need to apply the formula to each of the inverse
trigonmetric formulas.
</p>
<paragraphs>
<title>DIGRESSION: Inverse and Derivative Notation</title>
<p>

  As we saw in <xref
  ref="DIGRESSIONInverse-function-notation">DIGRESSION: Inverse
  Function Notation</xref> there are some
  difficulties with the notation we use to indicate inverse
  functions. These problems only get worse when we mix the standard
  derivative notations with the inverse function notation.
  Lagrange's prime notation is especially problematic.
</p>
<p>
  
  For example if <m>f(x)</m> is an invertible function the
  derivative of <m>\inverse{f}(x)</m> could be denoted either  as:
 <me>\dfdx{(\inverse{f})}{x}</me> 
 or
    <m>{\inverse{f}}^\prime(x)</m>.
</p>
<p>
  
  But both of these are somewhat awkward. Mathematicians also
  sometimes use the <term>operator</term> notation:
  <me>
  \DD(f(x)) =  f^\prime(x)=\dfdx{f}{x}
  </me>
  and in this situation it minimizes the awkwardness a bit.
</p>
<p>
  
  As we've seen there can also be some vagueness involving the
  distinction between functions and variables. For example  suppose we
  want to sketch a graph of this relation between <m>x</m> and <m>y</m>:
  <me>
  y-x^3=0.
  </me>
  The simplest thing to do is to choose a value for either <m>x</m> or <m>y</m>
  and then figure out what the corresponding <m>y</m> or <m>x</m> is. This is
  simpler to do that if we rearrange the relation so that we
  have one variable strictly in terms of (<q>as a function of</q>) the
  other. For this particular relation it is easiest to choose a value
  for <m>x</m> and compute the corresponding <m>y</m> value so we would normally
  rearrange it as
  <men  xml:id="EQUATIONInv1">
    y(x)=x^3.
    <!-- \label{eq:Inv1} -->
  </men>
  <xref ref="EQUATIONInv1">equation</xref> defines <m>y</m> as a function of <m>x</m>.
</p>
<p>
  
  But we only solved for <m>y</m> because we could see it was a little
  easier to do. Otherwise our choice was completely arbitrary. We could also
  have solved for <m>x</m> giving,
  <men xml:id="EQUATIONInv2">
    x(y)=\sqrt[3]{y}.
    <!-- \label{eq:Inv2} -->
  </men>
  In this case we have <m>x</m> as a function of <m>y</m>.
</p>
<p>
  
  The two functions, <m>y(x)</m>, (<q>cube</q>) and <m>x(y)</m> (<q>cube
  root</q>), clearly contain the same information as the original
  relation  <m>y-x^3=0</m>. But they are different, related,
  functions. They are in fact mutually inverse.
</p>
<p>

  For example suppose we choose <m>x=2</m> and use <xref ref="EQUATIONInv1">equation</xref>
  to find <m>y=8</m>. If we then take <m>y=8</m> and use
  <xref ref="EQUATIONInv2">equation</xref> we find that <m>x=2</m>. That is, <m>x(y)</m> has
  <q>undone</q> <m>y(x)</m> for the single
  pair <m>(2,8)</m>. <xref ref="DRILLCubeRoot"></xref> asks you to show that it is
  true for every pair <m>(x, y(x))</m>. This <q>undoing</q> makes <m>y(x)</m> and
  <m>x(y)</m> a pair of mutually inverse functions.
</p>
<p>
          
  But in function notation the variable (frequently <m>x</m> or <m>t</m>) is a
  placeholder. For example, each of <m>y(x)=x^3,</m> <m>y(t)=t^3</m>,
  <m>f(\alpha)=\alpha^3</m>, or even <m>f(\halmos)=\halmos^3</m> defines
  exactly the same function: The function which cubes its
  input. It doesn't matter what we call the variable. It just holds a
  place in the formula that tells us what the input  is
  and what to do with it. Since it doesn't matter what we call
  the variable we usually call it <m>x</m> unless there is some compelling
  reason to use something else.
</p>
<p>
  
  To avoid confusing variable names with function names we usually
  denote <m>y(x)</m> as <m>f(x)</m>. It's inverse, <m>x(y)</m> should probably be
  denoted as <m>\inverse{f}(y)</m>.  But sadly, recognizing that the
  variable is just a placeholder in function notation we use the same
  variable name in both the function and it's inverse. So we denote
  the inverse of <m>f(x)</m> as <m>\inverse{f}(x)</m>, even though it would
  probably make it easier for beginners to use <m>\inverse{f}(y)</m>, as a
  reminder that both functions come from the same original
  relation.
</p>

    <exercise xml:id="DRILLCubeRoot">
<statement>
<p>
    <!-- \label{drill:CubeRoot} -->
Prove that <m>f(x)=x^3</m> and     <m>\inverse{f}(x)=\sqrt[3]{x}</m> are
mutually inverse by showing that they satisfy the conditions stated in
<xref ref="DEFINITIONFunctionInverse"></xref>
  </p>
</statement>
</exercise>

<p>          
  The notation for inverse functions is not great. It can be very
  confusing, especially for beginners. Be careful with it.
</p>
<p><alert>END OF DIGRESSION</alert></p>
</paragraphs>

<p>
Our next task is to show that if <m>f(x)</m> is invertible and
differentiable, then <m>\inverse{f}</m> is also differentiable\aside{<m>\inverse{f}</m> is
  obviously invertible}. We do this by showing that the limit
<men xml:id="EQUATIONInvFuncDerivLimit">
  <!-- \label{eq:InvFuncDerivLimit} -->
  \DD\left(\inverse{f}(x)\right)=\limit{h}{0}{\frac{\inverse{f}(x+h)-\inverse{f}(x)}{h}}
</men>
exists.
</p>
<p>

In general this is true but there is one exception that has to be
addressed. When <m>f</m> is differentiable at <m>a</m> and <m>f^\prime(a) = 0</m>
then the limit in <xref ref="EQUATIONInvFuncDerivLimit">equation</xref> does not
exist. Hence <m>\inverse f</m> is not differentiable at <m>f(a)</m>.
More formally, we have the following lemma.
</p>


<lemma xml:id="LEMMAInvDerivAtZero">
<p>
  If  <m>f</m> is an invertible function, <m>f(a)=b</m>, <m>f</m> is differentiable
  at <m>x=a</m>, and <m>f^\prime(a)=0</m>, then <m>\inverse f</m> is not
  differentiable at  <m>x=b</m>. That is <m>\DD\left(\inverse f(b)\right)</m>
  does not exist.
</p>
</lemma>
<p>

The following proof of this lemma is very challenging to read and
understand for several reasons.
</p>
<p>

First, it is quite abstract. We don't have a particular function to
think about so we can't simply write down formulas for the function
and its inverse. Instead we have only the generic function, <m>f</m> and
its inverse <m>\inverse{f}</m>, and we'll need to remember
what these symbols represent.
</p>
<p>

Second, we need to think about the functions <m>f</m> and
<m>\inverse{f}</m> as well as their derivatives.
</p>
<p>

Third, instead of using the differential notation, <m>\dfdx{f}{x}</m> that we've grown very
comfortable with 
we'll be using the less familiar Lagrange prime notation  and the
operator notation we just introduced.
</p>
<p>

Finally, the nature of the problem forces us to mix these last two
notations, using one here and the other there. This can make for
difficult reading.
</p>
<p>

Read slowly. Remember that each symbol has meaning. Take time to
understand that meaning and what each formula as a whole is telling
you.
</p>
<p>


We include this proof in its full abstraction for two reasons:
<ol>
  <li>
    <p>
      To be as precise and as rigorous and we can.
    </p>
  </li>
  <li>
    <p>
      We want to give you practice with higher level abstract  reasoning in this (fairly) simple case.
    </p>
  </li>
</ol>
</p>


<p>

The strategy behind the following proof follows the same general
scheme as the <xref ref="QUOTEHolmesQuote">Sherlock Holmes Maxim</xref> that we referred to in  
<xref ref="DRILLMaximalTriangle"></xref>. We will eliminate the
impossible so that <q>whatever remains, however improbable, must be the
truth.</q>
</p>
<p>

There are two possibilities: Either the derivative of <m>\inverse{f}(b)</m>
exists or it does not exist. There are two steps:
<ol>
  <li>
    <p>
      Assume that the derivative of <m>\inverse{f}</m> does exist at <m>x=b</m>
      and calculate what <m>\DD\left(\inverse{f}(b)\right)</m> must be.
    </p>
  </li>
  <li>
    <p>
      Show that our computed value is impossible. Then <foreign>á
      la</foreign> Holmes' Maxim the only possibility left will be that the
      derivative of <m>\inverse{f}</m> does not exist at <m>x=b</m>.
    </p>
  </li>
</ol>
</p>



<proof>

<p>
  Assume that <m>\inverse{f}</m> is differentiable at <m>x=b</m>.
</p>
<aside>
<title>Comment</title>
<p>
  To be clear, we don't really believe this assumption. Be sure you
  are very clear on this point. We make this assumption so that we can
  use it to derive an absurd result; a contradiction. If there are no
  errors in our reasoning then the only possible conclusion will be
  that this assumption is false: <m>\inverse{f}</m> is not
  differentiable at <m>x=b</m>.
</p>
</aside>
<p>
 Because <m>f</m> and <m>\inverse{f}</m> are mutually
  inverse we know that
  <me> f\left(\inverse{f}(x)\right)=x.</me>
  Therefore
  <me> \DD\left(f\left(\inverse{f}(x)\right)\right)=\DD\left(x\right).</me>
  On the right we have
  <me>
  \DD(x)=1.
  </me>
   On the left apply the Chain Rule:
  <men xml:id="EQUATIONInvDeriv2">
    <!-- \label{eq:InvDeriv2} -->
    f^\prime\left(\inverse{f}(x)\right)\cdot\DD\left(\inverse{f}(x)\right)=1.
  </men>
  But when <m>x=b</m> we find that 
  <m>f^\prime\left(\inverse{f}(b)\right)=f^\prime\left(a\right)=0</m>,
  so that
  <me>
  0=\underbrace{f^\prime\left(\inverse{f}(b)\right)}_{=0}\cdot\DD\left(\inverse{f}(b)\right)=1
  </me>
  or
  <me>  0=1</me>
  which is ridiculous or in Holmes' word, impossible.
  
  Therefore our assumption cannot true so <m>\inverse{f}</m> is not
  differentiable at <m>x=b</m>.
</p>
<p>
<alert><m>\halmos</m></alert>
</p>
</proof>

<p>
While valid and correct, this proof is not very enlightening.  A well
chosen sketch would be much more convincing, if less rigorous.
</p>

<problem>
<statement>
<p>
  Choose a function whose derivative is equal to zero at some point
  and sketch the graph of your function and its inverse on the same
  set of axes. Be sure to include the point where the derivative is
  zero.
  
  Use your graph to explain why the derivative of the inverse of your
  function does not exist.
</p>
</statement>
</problem>
<p>

We now understand what conditions are necessary for an arbitrary
function, <m>f(x)</m>, to have a differentiable inverse.
</p>
<p>

 Also, from <xref ref="EQUATIONInvDeriv2"></xref> we know what
the derivative of the inverse will be if it exists:
<me>
\DD\left(\inverse{f}(x)\right)=\frac{1}{f^\prime\left(\inverse{f}(x)\right)}.
</me>
</p>


<exercise>
<statement>
<p>
  Let <m>y=\inverse{f}(x)</m> and explain how the formula above is
  equivalent to
  <men xml:id="EQUATIONInvDeriv">
    \dfdx{y}{x}=\frac{1}{\dfdx{x}{y}}
    <!-- \label{eq:InvDeriv} -->
  </men>
  </p>
</statement>
</exercise>

<p>
The only thing left is to show that under the conditions on <m>f</m> in
<xref ref="LEMMAInvDerivAtZero"></xref> the
derivative (that is, the limit which defines the derivative) of the
inverse does in fact exist.
</p>

<theorem xml:id="THEOREMDerivInvFunc">
<title>The Derivative of Inverse Functions</title>
<p>
  Suppose that
  
  <ol cols="2">
    <li>
      <p>
        <m>f</m> is differentiable at <m>x=a</m>,
      </p>
    </li>
    <li>
      <p>
        <m>f(a)=b</m>,
      </p>
    </li>
    <li>
      <p>
        <m>f^\prime(a)\neq0</m>,
      </p>
    </li>
    <li>
      <p>
        <m>\inverse{f}</m> is continuous at <m>x=b</m>.
      </p>
    </li>
  </ol>
</p>

<aside>
<title>Comment</title>
<p>
  In fact, the continuity of <m>\inverse{f}</m> follows from the
  continuity of <m>f</m> at <m>x=a</m>. We do not have all of the
  tools necessary to prove this so we must include it in the
  assumptions of our theorem.
</p>
</aside>
<p>
  Then the inverse of <m>f</m> is differentiable at <m>x=b</m> and 
  <me>
  \DD\left(\inverse{f}(b)\right)=\frac{1}{f^\prime\left(\inverse{f}(b)\right)}.
  </me>
  </p>
</theorem>

<p>
Reading and understanding the notation in <xref
ref="THEOREMDerivInvFunc"></xref> presents the same difficulties we
saw in the proof of <xref
ref="LEMMAInvDerivAtZero"></xref>. Read it carefully.  Be
patient with yourself and do not rush.
</p>

<proof>
<title>Proof of <xref ref="THEOREMDerivInvFunc"></xref></title>
<p>
  We want to show that the limit
  <me>
  \DD\left(\inverse{f}(b)\right)=\limit{h}{0}{\frac{\inverse{f}(b+h)-\inverse{f}(b)}{h}}=\frac{1}{f^\prime\left(\inverse{f}(b)\right)}.
  </me>
  Since <m>f(a)=b</m> we know that
  <m>\textcolor{red}{\inverse{f}(b)=a}</m> so that
  <md>
    <mrow>
      \limit{h}{0}{\frac{\inverse{f}(b+h)-\textcolor{red}{\inverse{f}(b)}}{h}}
      \amp = \limit{h}{0}{\frac{\inverse{f}(b+h)-\textcolor{red}{a}}{h}}
    </mrow>
    <intertext>
      Observe that if <m>b+h</m> is in the domain of <m>\inverse{f}</m> then it is in the
      range of <m>f</m>. Thus there is some number in the domain   of
      <m>f</m> (for convenience we'll call it <m>a+k</m>) such that
      <m>\textcolor{blue}{b+h}=\textcolor{blue}{f(a+k)}</m>. Thus
    </intertext>
    <mrow>
      \limit{h}{0}{\frac{\inverse{f}(\textcolor{blue}{b+h})-\inverse{f}(b)}{h}}
      \amp = \limit{h}{0}
      {\frac{\inverse{f}(\textcolor{blue}{f(a+k)})-a}{h}}.
    </mrow>
    <intertext>
      Again since <m>f</m> and <m>\inverse{f}</m> are mutually inverse
      they <q>undo</q> each other, so  <m>\inverse{f}(f(a+k))=a+k</m> so that
    </intertext>
    <mrow>
      \limit{h}{0}{\frac{\inverse{f}(b+h)-\inverse{f}(b)}{h}} \amp =
      \limit{h}{0}{\frac{k}{h}}.
    </mrow>
    <intertext>
      Solving <m>{b+h=f(a+k)}</m> for <m>h</m> gives
      <m>\textcolor{blue}{h}=\textcolor{blue}{f(a+k)-b}</m> so  
    </intertext>
    <mrow>
      \limit{h}{0}{\frac{k}{h}}\amp = \limit{h}{0}{\frac{k}{\textcolor{blue}{f(a+k)-b}}}
    </mrow>
    <intertext>
      and since <m>b=f(a)</m> we have
    </intertext>
    <mrow>
      \amp = \limit{h}{0}{\frac{k}{f(a+k)-f(a)}}
    </mrow>
    <mrow>
      \amp = \limit{h}{0} {\frac{1} {\frac{f(a+k)-f(a)}{k}}}.
    </mrow>
    <mrow>
      \amp =  \frac{1} {\limit{h}{0}{\frac{f(a+k)-f(a)}{k}}}.
    </mrow>
  </md>
  
  The expression <m>\limit{h}{0}{\frac{f(a+k)-f(a)}{k}}</m> would be
  <m>f^\prime(a)</m> if only we had <m>k\rightarrow0</m> instead of
  <m>h\rightarrow0</m>.  What we need to show now is that if
  <m>h\rightarrow0</m> then <m>k\rightarrow0</m>. Then we could write
  <men xml:id="EQUATIONInvProof">
    <!-- \label{eq:InvProof} -->
    \DD\left(\inverse{f}(b)\right) = \frac{1}
    {\limit{k}{0}{\frac{f(a+k)-f(a)}{k}}}=\frac{1}{f^\prime(a)}
  </men>
  and our proof would be complete.
  
  Written a little more carefully, what we need to show is that
  <m>\tlimit{h}{0}{k}=0</m>. Recall that <m>a=\inverse{f}(b)</m>, and that
  <m>a+k=\inverse{f}(b+h)</m> so we need to show  that
  <me>
\tlimit{h}{0}{k}=\limit{h}{0}{\left[(a+k)-a\right]}=  \tlimit{h}{0}{\left[\inverse{f}(b+h)-\inverse{f}(b)\right]} = 0</me> or
        But we assumed that <m>\inverse{f}</m> is continuous at <m>x=b</m> which means
  that 
  <me>
  \tlimit{h}{0}{\left[\inverse{f}(b+h)-\inverse{f}(b)\right]}=0,
  </me>
   and the proof is complete.
  
  
  
          
          
   One last point: On the left side of <xref ref="EQUATIONInvProof"></xref> the
   variable is <m>b</m> and on the right it is <m>a</m>. While this is not
   strictly wrong it is a more useful theorem if we state it in terms
   of <m>b</m> alone.
  
  Since <m>f(a)=b</m> we see that <m>\inverse{f}(b)=a</m> so
  <me>
  \DD\left(\inverse{f}(b)\right) =\frac{1}{f^\prime(\inverse{f}(b))}
  </me>
  and the proof is complete.
</p>
<p>
<alert><m>\halmos</m></alert>
</p>
<p>
<alert><m>\halmos</m></alert>
</p>
</proof>

<p>
Using <xref ref="THEOREMDerivInvFunc"></xref> we can now show that the
derivatives of the inverse trigonometric functions and the natural
logarithm are exactly what we expect them to be. The difference is
that now there is no uncertainty or vagueness in our foundations. No
modern Bishop Berkeley can step in and sew doubt.
</p>

<example>
  <title>The Derivative of the Inverse Sine</title>
  <p>
    Suppose <m>f(x)=\sin(x).</m> Then <m>\inverse{f}(x)=\inverse{\sin}(x)</m> so
    <md>
      <mrow>
        \DD\left(\inverse{f}(x)\right) =     \DD\left(\inverse\sin(x)\right) 
      </mrow>
      <mrow>
        \amp = \frac{1}
        {\textcolor{red}{f^\prime}(\textcolor{blue}{\inverse{f}}(x))}
      </mrow>
      <mrow>
        \amp = \frac{1}{\textcolor{red}{\cos}(\textcolor{blue}{\inverse\sin}(x))}
      </mrow>
      <mrow>
        \DD\left(\inverse{f}(x)\right) \amp =  \frac{1}{\sqrt{1-x^2}}.
      </mrow>
    </md>

  </p>
</example>


<problem>
<introduction>
<p>
  Use <xref ref="THEOREMDerivInvFunc"></xref> to show that each of the following
  differentiation rules is correct:
</p>
</introduction>
<task>
<statement>
<p>

      <m>\DD\left(\inverse{\cos}(x)\right) = \frac{-1}{\sqrt{1-x^2}}</m>
    </p>
</statement>
</task>
<task>
<statement>
<p>
 <m>\DD\left(\inverse{\tan}(x)\right) = \frac{1}{1+x^2}</m>
    </p>
</statement>
</task>
<task>
<statement>
<p>
 <m>\DD\left(\inverse{\cot}(x)\right) = \frac{-1}{1+x^2}</m>
    </p>
</statement>
</task>
<task>
<statement>
<p>

      <m>\DD\left(\inverse{\sec}(x)\right) =
      \frac{1}{\abs{x}\sqrt{x^2-1}}</m>
    </p>
</statement>
</task>
<task>
<statement>
<p>

      <m>\DD\left(\inverse{\csc}(x)\right) =
      \frac{-1}{\abs{x}\sqrt{x^2-1}}</m>
    </p>
</statement>
</task>
<task>
<statement>
<p>
 <m>\DD\left(\inverse{\ln}(x)\right) = e^x</m>
  </p>
</statement>
</task>
</problem>
<!-- \begin{embeddedproblem} -->
<!--   Use <xref ref="THEOREMthm:DerivInvFunc">~</xref> to show that each of the following -->
<!--   differentiation rules is correct: -->
<!--   \begin{enumerate}[label={  (\alph*)}] -->
<!--     \item -->
<!--       <m>\DD\left(\inverse{\cos}(x)\right) = \frac{-1}{\sqrt{1-x^2}}</m> -->
<!--     \item <m>\DD\left(\inverse{\tan}(x)\right) = \frac{1}{1+x^2}</m> -->
<!--     \item <m>\DD\left(\inverse{\cot}(x)\right) = \frac{-1}{1+x^2}</m> -->
<!--     \item -->
<!--       <m>\DD\left(\inverse{\sec}(x)\right) = -->
<!--       \frac{1}{\abs{x}\sqrt{x^2-1}}</m> -->
<!--     \item -->
<!--       <m>\DD\left(\inverse{\csc}(x)\right) = -->
<!--       \frac{-1}{\abs{x}\sqrt{x^2-1}}</m> -->
<!--     \item <m>\DD\left(\inverse{\ln}(x)\right) = e^x</m> -->
<!--   \end{enumerate} -->
<!-- \end{embeddedproblem} -->
<p>
Wait a minute! Did we forget one? What about the natural exponential
function?  Don't we also have to show that <m>\DD\left(e^x\right)=e^x</m>?
</p>
<exercise>
<statement>
<p>
  Look back at <xref ref="DEFINITIONnatural-exponential-ivp"></xref> and
  explain why it is not necessary to use limits to show that
  <m>\DD\left(e^x\right)=e^x</m>.
</p>
</statement>
</exercise>



</section>
</chapter>
