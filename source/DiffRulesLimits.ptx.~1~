<chapter xmlns:xi="http://www.w3.org/2001/XInclude"  xml:id="CHAdiff-rules-with-limits">
 <title>The Differentiation Rules via Limits</title>

\label{CHAdiff-rules-with-limits}

  Rules via Limits}
<section xml:id="sec:limit-theorems-laws">
<title>The Limit Rules (Theorems)</title>

<!-- \label{sec:limit-theorems-laws} -->

<blockquote>
<p>
In the old days when people invented a new
  function they had something useful in mind. Now, they invent
  them deliberately just to invalidate our ancestors' reasoning,
  and that is all they are ever going to get out of
  them.
</p>
<attribution>
<url href="https://mathshistory.st-andrews.ac.uk/Biographies/Poincare" visual="https://mathshistory.st-andrews.ac.uk/Biographies/Poincare">Henri
    Poincar\'e ($1854$ -- $1912$)</url>
</attribution>
</blockquote>


In this section we will state several theorems about limits which we
will need in the sections following.  The limit concept is very subtle
and our understanding of it is still quite intuitive.  We are not yet
quite prepared to prove these theorems so we will leave these theorems
unproven for now. Our immediate goal is simply to understand what they
say and learn how to use them.  In the next section we will begin
using these theorems to show how the limit in
<xref ref="DEFDerivative">Definition~\</xref> allows us to recapture all of the
major results we used in Part I of this text.

In <xref ref="CHAformal-limits">Chapter</xref> we will finally discard our
intuitive definition of a limit
(<xref ref="DEFLimitAtInfinity-Intuitive">Definition~\</xref>) and formally define
both a limit at infinity (<xref ref="DEFlimit-at-neginfinity">Theorem</xref>) and
a limit at a point (<xref ref="DEFlimits-at-real">Theorem</xref>). Then we will
return to the theorems in this section and (finally) prove rigorously
that they are, in fact, true. Until then any result which relies on
the theorems in this section should be considered contingent.


In <xref ref="CHAlimits-lhop-rule">Chapter</xref> we stated the following three
theorems about limits ``at infinity.''
\begin{enumerate}
\item the limit of a sum is the sum of the limits
  (<xref ref="THEOREMLimSum1">Theorem</xref>),
\item  the limit of a product is the
  product of the limits (<xref ref="THEOREMLimProd1">Theorem</xref>) and, 
\item  the limit of a quotient  is the
  quotient of the limits (<xref ref="THEOREMLimQuot1">Theorem</xref>).
\end{enumerate}
All three of these theorems remain true if $x$ is approaching some
finite number, $a$, instead of infinity.

<theorem xml:id="theorem:LimSum2">
<title>The Limit of a Sum is the Sum of the Limits</title>
<p>
  <!-- \label{theorem:LimSum2} -->
  Let $a$ be some real number. Suppose that the functions $f(x)$ and
  $g(x)$ are defined on some open interval about $a$ except, possibly,
  at $a$ itself.
  
  Then 
  if $\limit{x}{a}{f(x)}=L$ and
  $\limit{x}{a}{g(x)}=M,$ then
  $$
  \limit{x}{a}{\left[f(x)+g(x)\right]}=L+M
  =\limit{x}{a}{f(x)}+\limit{x}{a}{g(x)}.
  $$
</p>
</theorem>


<theorem xml:id="theorem:LimProd2">
<title>The Limit of a Product is the Product of the Limits</title>
<p>
  <!-- \label{theorem:LimProd2} -->
  Let $a$ be some real number. Suppose  that the functions  $f(x)$ and $g(x)$ are
  defined on some open interval about $a$ except, possibly, at $a$
  itself.
  
  If 
  $\limit{x}{a} {f(x)}=L$ and $\limit{x}{a}{g(x)}=M$ then
  $$
  \limit{x}{a} {\left(f(x)\cdot g(x)\right)}=\left(\limit{x}{a}{f(x)}\right)\cdot\left(\limit{x}{a}{g(x)}\right)
  = L\cdot M.
  $$
</p>
</theorem>


<theorem xml:id="theorem:LimQuot2">
<title>The Limit of a Quotient is the Quotient of the Limits</title>
<p>
  <!-- \label{theorem:LimQuot2} -->
  Let $a$ be some real number. Suppose  that the functions  $f(x)$ and $g(x)$ are
  defined on some open interval about $a$ except, possibly, at $a$
  itself.
  
  Then if
  $\limit{x}{a} {f(x)}=L$ and $\limit{x}{a}{g(x)}=M\neq0$ then
  $$
  \limit{x}{a}
  {\frac{f(x)}{g(x)}}=\frac{\limit{x}{a}{f(x)}}{\limit{x}{a}{g(x)}}
  = \frac{L}{M}.
  $$
</p>
</theorem>


Notice that in addition to changing $\infty$ to some real number, $a$,
we have added two qualifications to the statement of each of these
theorems from <xref ref="CHAlimits-lhop-rule">Chapter</xref>:
\begin{enumerate}
\item ``Suppose that the functions $f(x)$ and $g(x)$ are defined on
some open interval about $a$'' and,
\item ``except, possibly, at $a$ itself''
\end{enumerate}
To see why these are necessary recall that we're going to use limits
to define the derivative as in <xref ref="DEFDerivative">Theorem</xref> so we'll
need to evaluate the limit
$f^\prime(x)=\tlimit{h}{0}{\frac{f(x+h)-f(x)}{h}}$ . Clearly the
expression $\frac{f(x+h)-f(x)}{h}$ is not defined at $h=0$. But we're
only interested in its value <term>in the limit</term> as $h\rightarrow0$
which means that $h$ must be able to get close to $0$. That is, there
must be an open interval around $0$ where the expression
$\frac{f(x+h)-f(x)}{h}$ is defined.

But we don't care if $\frac{f(x+h)-f(x)}{h}$ is defined at $h=0$ or
not. It is irrelevant to our purpose.  So we state explicitly that we
do not consider whether $h=0$.


Ok, but why did we insert the word ``possibly''? Wouldn't it be enough
to simply say ``except at $a$''?

We need to say ``possibly'' because these theorems, like all theorems,
are stated with as much generality as possible.  For example, consider
the function $f(x)=2x$. Had we not included ``possibly'' in the
conditions of our theorems the limit: $\tlimit{x}{3}{2x}$, which is
clearly equal to $6$, would have to be considered undefined because
$f(x)=2x$ is defined at $x=3$. This distinction\aside{In the
  eighteenth century there was a, public, protracted, and vitriolic
  argument between <url href="https://mathshistory.st-andrews.ac.uk/Biographies/Robins/" visual="https://mathshistory.st-andrews.ac.uk/Biographies/Robins/">Benjamin Robins</url> and <url href="https://en.wikipedia.org/wiki/James_Jurin" visual="https://en.wikipedia.org/wiki/James_Jurin">James Jurin</url> over exactly this
  point. Jurin would have claimed that the statement
  $\tlimit{x}{3}{2x} = 6$ is meaningless. Robins would have said it
  has meaning because it is obviously true. The point here is not that
  either man was right or wrong, but rather that it depends on how we
  define limits. By one definition Robins was correct, by another
  Jurin was. Their controversy was the result of the incomplete
  understanding of limits that prevailed at the time.}  may seem like
a very fussy, and unimportant detail right now, but it will be
important when we discuss the meaning of continuity in
<xref ref="SEClimit-comp-cont">Section</xref>.

<example xml:id="example:DiscontLimit1">
<p>
  <!-- \label{example:DiscontLimit1} -->
Suppose
  $$
  f(x)=
  \begin{cases}
    2x \amp  \text{if } x\neq3\\
    10  \amp  \text{if } x=3
  \end{cases}.
  $$
  Then
  $$
  \limit{x}{3}{f(x)}=  6.
  $$ In particular the limit is not $10$.

  Here is how we would evaluate this limit using the tools we
  currently have at our disposal.

We're interested in the limit as $x\rightarrow3$ so
  in particular we do not need to consider the case when $x=3$. But as
  long as $x\neq3$ we have $f(x)=2x$ so
  $$
  \limit{x}{3}{f(x)}=   \limit{x}{3}{2x}.
  $$
 As $x$ gets close to $3$ it is clear that $2x$ gets close to
  $6$.
  Therefore 
  $$
  \limit{x}{3}{f(x)}=  6.
  $$

  Notice that our reasoning is a little vague in the last step because
  we had to resort to the phase ``gets close to,'' and we know from
  our work in Chapter~\ref{sec:more-indet-forms} that this is not a
  precise phrase. This is the best we can do now because we have not
  yet rigorously defined a limit. We will do that in
  Chapter~\ref{cha:formal-limits}.
</p>
</example>




<problem xml:id="no-label">
<introduction>
<p>
  By reasoning in a manner similar to
  Example~\#\ref{example:DiscontLimit1} show that
  $\tlimit{x}{3}{f(x)}=9$ for each function.
</p>
</introduction>
<task>
<statement>
<p>
 $f(x)=x^2$
         </p>
</statement>
</task>
<task>
<statement>
<p>
 $f(x)=
           \begin{cases}
             x^2 \amp  \text{if } x\neq3\\
            \text{undefined }  \amp  \text{if } x=3
           \end{cases}$
       </p>
</statement>
</task>
</problem>
<!-- \begin{embeddedproblem} -->
<!--   By reasoning in a manner similar to -->
<!--   Example~\#\ref{example:DiscontLimit1} show that -->
<!--   $\tlimit{x}{3}{f(x)}=9$ for each function. -->
<!--   \begin{enumerate}[label={  (\alph*)}]  -->
<!--          \item $f(x)=x^2$ -->
<!--          \item $f(x)= -->
<!--            \begin{cases} -->
<!--              x^2 \amp  \text{if } x\neq3\\ -->
<!--             \text{undefined }  \amp  \text{if } x=3 -->
<!--            \end{cases}$ -->
<!--        \end{enumerate} -->

<!-- \end{embeddedproblem} -->


It will be tedious to write (and to read) the phrase ``Suppose that
$f(x)$ is defined on some open interval about $a$ except, possibly, at
$a$ itself'' every time we need it so it is customary to say something
more like ``Suppose $f(x)$ is defined <term>near</term> $a$''
instead. Because we are trying to be as precise, and rigorous as
possible we will formalize this by redefining the word
<term>near</term>.

<definition xml:id="DEFINITIONdef:near">
<title>Near</title>
<statement>
<p>
  <!-- \label{def:near} -->
  We say that $f(x)$ has some property \term{near}\aside{Notice that this is not what ``near'' means in
  ordinary speech.  This is one of the things that makes it difficult
  to read mathematics. We routinely co-opt words from natural
  languages (like English) and redefine them to fit our needs. In this
  case our purpose requires that we change the definition of ``near''
  slightly as you've seen.  Because ``near'' is a common word and you
  have a lifetime of experience using it, it can be very difficult to
  cast off your preconceptions.  The familiar definition you learned
  in childhood will intrude and cause confusion. It is hard to
  overcome this. Refer back to the definition frequently until you
  have internalized the mathematical definition.  } $x=a$ if $f(x)$ has that
  property on an open interval about $x=a$, except possibly at $a$
  itself.

</p>
</statement>
</definition>


We  have the following theorem.

<theorem xml:id="thm:ConstLimit">
<title>The Limit of a Constant is the Constant</title>
<p>
  <!-- \label{thm:ConstLimit} -->
  Suppose $a$ and  $L$ are real  numbers, and $f(x)=L$ near $a$. Then
  $$
  \limit{x}{a}{f(x)}=L.
  $$
</p>
</theorem>

Notice that <xref ref="THMConstLimit">Theorem</xref> would be considerably less
useful had we not required that $f(x)=L$ for $x$ near $a$, rather than
$f(x)=L$ on its entire domain. For example, as stated
<xref ref="THMConstLimit">Theorem</xref> allows us to conclude that if
$$
H(x) =
\begin{cases}
  1\amp  \text{ if } x\ge0\\
  -1\amp  \text{ if } x\lt0
\end{cases}
$$
then
$$\limit{x}{5}{H(x)}=1$$ and  $$\limit{x}{-5}{H(x)}=-1$$ because
there are open intervals about $x=5$ and $x=-5$ where $H(x)=1$, and
$H(x)=-1$, respectively. 

Since there is no such interval about $x=0$, $\tlimit{x}{0}{H(x)}$ is
undefined.



\<exercise xml:id="no-label">
<introduction>
<p>
</p>
</introduction>
<task>
<statement>
<p>
   Find an open interval about $5$ where $H(x)=1$.
        </p>
</statement>
</task>
<task>
<statement>
<p>
   Find an open interval about $-5$ where $H(x)=-1$.
      </p>
</statement>
</task>
</exercise>
<!-- begin{ProficiencyDrill-enumerate} -->
 <!--       \begin{enumerate}[label={  (\alph*)}]  -->
 <!--         \item   Find an open interval about $5$ where $H(x)=1$. -->
 <!--        \item   Find an open interval about $-5$ where $H(x)=-1$. -->
 <!--      \end{enumerate} -->
 <!--      \comment{There are many to choose from in both parts. Choose -->
 <!--        only one for each.} -->
 <!--  \end{ProficiencyDrill-enumerate} -->


\<exercise xml:id="no-label">
<introduction>
<p>
  Suppose
    $$
  f(x)=
  \begin{cases}
    \ \ 1;\amp  \text{ if } x\gt 1 \text{ or if } x=-2\\
    -2;\amp  \text{ if } x\le1 \text{ and } x=-2.
  \end{cases}
  $$
  Determine whether the following statements are true or false.
</p>
</introduction>
<task>
<statement>
<p>
 $f(x)= 1$ near $x= 4$.
    </p>
</statement>
</task>
<task>
<statement>
<p>
 $f(x)= 1$ near $x= 1$.
    </p>
</statement>
</task>
<task>
<statement>
<p>
 $f(x)= -2$ near $x=1 $.
    </p>
</statement>
</task>
<task>
<statement>
<p>
 $f(x)= 1$ near $x=-2 $.
    </p>
</statement>
</task>
<task>
<statement>
<p>
 $f(x)= -2$ near $x=-2 $.
    </p>
</statement>
</task>
<task>
<statement>
<p>
 $f(x)= 0$ near $x= 0$.
  </p>
</statement>
</task>
</exercise>
<!-- begin{ProficiencyDrill} -->
<!--   Suppose -->
<!--     $$ -->
<!--   f(x)= -->
<!--   \begin{cases} -->
<!--     \ \ 1;\amp  \text{ if } x\gt 1 \text{ or if } x=-2\\ -->
<!--     -2;\amp  \text{ if } x\le1 \text{ and } x=-2. -->
<!--   \end{cases} -->
<!--   $$ -->
<!--   Determine whether the following statements are true or false. -->
<!--   \begin{enumerate}[label={  (\alph*)}] -->
<!--     \item $f(x)= 1$ near $x= 4$. -->
<!--     \item $f(x)= 1$ near $x= 1$. -->
<!--     \item $f(x)= -2$ near $x=1 $. -->
<!--     \item $f(x)= 1$ near $x=-2 $. -->
<!--     \item $f(x)= -2$ near $x=-2 $. -->
<!--     \item $f(x)= 0$ near $x= 0$. -->
<!--   \end{enumerate} -->
<!-- \end{ProficiencyDrill} -->






<problem xml:id="no-label">
<introduction>
<p>
    \noindent  Explain, that  the following
  statements are true by citing Theorem~\ref{theorem:LimSum2} through
  Theorem~\ref{thm:ConstLimit} as needed.
</p>
</introduction>
<task>
<statement>
<p>
 $\limit{x}{0}{\left(\frac{5x}{x}+\frac{\pi x}{x}\right)}=5+\pi$
    </p>
</statement>
</task>
<task>
<statement>
<p>

      $\limit{x}{2}{\left(\frac{x-2}{x-2}+\frac{3x-6}{x-2}\right)}=4$
    </p>
</statement>
</task>
<task>
<statement>
<p>
 $\limit{x}{5}{\left(2x+3x^2\right)}=85$
    </p>
</statement>
</task>
<task>
<statement>
<p>

      $\limit{x}{-1}{\left(\frac{x^2-1}{x+1}+\frac{x^2+3x+2}{x+1}\right)}=-1$
  </p>
</statement>
</task>
</problem>
<!-- \begin{embeddedproblem} -->
<!--     \noindent  Explain, that  the following -->
<!--   statements are true by citing Theorem~\ref{theorem:LimSum2} through -->
<!--   Theorem~\ref{thm:ConstLimit} as needed. -->
<!--   \begin{enumerate}[label={  (\alph*)}] -->
<!--     \item $\limit{x}{0}{\left(\frac{5x}{x}+\frac{\pi x}{x}\right)}=5+\pi$ -->
<!--     \item -->
<!--       $\limit{x}{2}{\left(\frac{x-2}{x-2}+\frac{3x-6}{x-2}\right)}=4$ -->
<!--     \item $\limit{x}{5}{\left(2x+3x^2\right)}=85$ -->
<!--     \item -->
<!--       $\limit{x}{-1}{\left(\frac{x^2-1}{x+1}+\frac{x^2+3x+2}{x+1}\right)}=-1$ -->
<!--   \end{enumerate} -->
<!--   \end{embeddedproblem} -->

<problem xml:id="no-label">
<statement>
<p>
  Notice that neither $\tlimit{x}{0}{\frac{1}{x}}$ nor
  $\tlimit{x}{0}{\left(\frac{-1}{x}\right)}$ exists. However their
  sum,
  $$
  \tlimit{x}{0}{\left(\frac{1}{x}+\frac{-1}{x}\right)}=\tlimit{x}{0}{0}=0
  $$
  does
  exist. Explain why this does not contradict
  <xref ref="THEOREMLimSum2">Theorem</xref>.
</p>
</statement>
</problem>


<problem xml:id="prob:DiffImpCont">
<statement>
<p>
  <!-- \label{prob:DiffImpCont} -->
  Suppose $g(x)\neq0$ near $x=a$ and
  $\tlimit{x}{a}{\frac{f(x)}{g(x)}}$ exists. Use
  <xref ref="THEOREMLimProd2">Theorem~\</xref> to show that if
  $\limit{x}{a}{g(x)}=0$         then $\limit{x}{a}{f(x)}=0$.\\
     \hint{Consider
      $f(x)=\frac{f(x)}{g(x)}\cdot g(x)$ for $x$ near $a$.}\\
    \comment{This problem shows that if $\limit{x}{a}{g(x)}=0$
then the only way that $\limit{x}{a}{\frac{f(x)}{g(x)}}$ can exist is
if we have a \lhop  Indeterminate. It can also be used to prove
<xref ref="LEMMADiffImpliesCont">Lemma</xref> as you will see when we get to it.}
</p>
</statement>
</problem>



The following Corollary says that if $f(x)$ is approaching $L_f$ and
we multiply $f(x)$ by a number, $k$, then the product $kf(x)$
approaches $kL_f$. It follows from <xref ref="THMConstLimit">Theorem</xref> and
<xref ref="THEOREMLimProd2">Theorem</xref>.


\begin{mycorollary-noname-1line}
  \label{cor:ConstMultLimit}
  If $\limit{x}{a}{f(x)}=L_f$ and $k$ is a real number then
  $\limit{x}{a}{kf(x)}=k\limit{x}{a}{f(x)}=kL_f.$
\end{mycorollary-noname-1line}


<exercise xml:id="no-label">
<statement>
<p>
  Prove <xref ref="CORConstMultLimit">Corollary</xref>.
</p>
</statement>
</exercise>

<subsection xml:id="sec:limit-comp-cont">
<title>The Limit of a Composition and Continuity at a Point</title>

<!-- \label{sec:limit-comp-cont} -->
The concept of <term>continuity</term> is essential to Calculus, but you may
have noticed that we have carefully avoided it as much as possible
until now. This is because defining continuity is similar to defining
the line tagent to a curve (<xref ref="DEFTangentLine">Definition~\</xref>). We
need to think carefully about what we want the term <term>continuous</term>
to mean, and then craft our definition to capture that meaning. This
would have been very difficult to do without a fairly sophisticated
understanding of the limit concept.
  
So stop and think about this for a moment. What do we mean when we say
a curve is <term>continuous</term>? A first, intuitive definition usually
goes something like this: ``A function is continuous if you can draw
its graph without lifting your pencil from the paper,'' but this is
unsatisfactory for a number of reasons. In particular, it is
impossible to apply in most cases. Think about it. Have often have you seen
the entire graph of any function? Usually we just draw the part
neat the origin and put arrowheads on both ends of the graph. We need
something more precise.

At the end of <xref ref="EXAMPLELimComp1">Example~\</xref> we remarked that it is
only when $f(x)$ is continuous at $x=g(a)$ that
$\textcolor{blue}{\tlimit{x}{a}{\textcolor{red}{f(g(x))}}}$ is equal
to
$\textcolor{red}{f}\textcolor{blue}{\left(\tlimit{x}{a}{g(x)}\right)}$,
but we did not discuss the matter any further. It is time for that
discussion.


First, notice that when you think closely about the statement ``$f(x)$
is continuous at $g(a)$'' it appears to be nonsense, because $g(a)$ is
the value of $f$ at the single value $g(a)$ but .  Does it make sense to
you that a curve can be continuous at a single value of its domain?
In ordinary usage  the concept of continuity requires an interval to
be continuous on, doesn't it?
  
            
            
      
Since we need the concept of ``continuity at a point,'' we define it.
  
  <definition xml:id="DEFINITIONdef:continuity">
<title>Continuity at a Point</title>
<statement>
<p>
    <!-- \label{def:continuity} -->
    A function $f$, whose domain is an interval in $\RR$, is
    continuous at $x=a$ in the interval, if and only if
    $\limit{x}{a}{f(x)}=f(a)$, (alternatively, if
    $\limit{h}{0}{f(a+h)}=f(a)$).
  
</p>
</statement>
</definition>
\begin{wrapfigure}[]{r}{2in}
  \captionsetup{labelformat=empty}
  \vskip-4mm 
    \centerline{\includegraphics*[height=2in,width=1.5in]{../Figures/ContDiscExamp}}
\label{fig:ContDiscExamp}
\end{wrapfigure}
If $f$ is continuous at every point in its domain we'll just call
it a continuous function.

The sketch at the right shows that Definition~\#\ref{def:continuity}
recovers the intuitive notion that a function is continuous if we can
draw its graph without lifting pen from page.  Both of the functions,
$f(x)$ and $g(x)$ are identical everywhere except at
$x=1$. Clearly, to draw $g(x)$, which is discontinuous at $x=1$, we
must lift our pen from the page. This is not true of the graph of
$f(x)$, which is continuous.

    
The following lemma is true and the proof will be valid once the limit
theorems have been proven in Chapter~\ref{cha:formal-limits}.
  
  \begin{mylemma}[Differentiability Implies Continuity]
    \label{lemma:DiffImpliesCont}
    If $f(x)$ is differentiable at $x=a$ then $f(x)$ is also
    continuous at $x=a.$
  \end{mylemma}

                       
      
  \begin{embeddedproblem-1line}
    Use the result of Problem~\#\ref{prob:DiffImpCont} to prove
    Lemma~\ref{lemma:DiffImpliesCont}.
              \end{embeddedproblem-1line}
  
  When we were studying horizontal asymptotes in
  Section~\ref{subsec:lhop-rule-horiz} we encountered
  Theorem~\ref{thm:InfSqueeze} (the  Squeeze
  Theorem ``at'' Infinity). But the Squeeze Theorem is also valid if
  $x\rightarrow a$, where $a$ is a real number.
    
  
  \begin{mytheorem}[The Squeeze Theorem (The Finite Case)]
    \label{thm:FiniteSqueeze}
                        
    If $\alpha(x)\le f(x)\le \beta(x)$ for $x$ near $a$
    and $$\limit{x}{a}{\beta(x)} = \limit{x}{a}{\alpha(x)} = L$$ then
    $\limit{x}{a}{f(x)} = L$ also.
  \end{mytheorem}

    Theorem~\ref{thm:FiniteSqueeze} is illustrated below, but a formal
    proof will not be given until Chapter~\ref{cha:formal-limits}.\\
      \centerline{\includegraphics*[height=1.4in,width=2.6in]{../Figures/SqueezeThm1}}
  
        \begin{embeddedproblem}
        Consider the two functions defined in the sketch below:\\
        \centerline{\includegraphics*[height=2.5in,width=5.5in]{../Figures/TopSinCurve}}
    \begin{enumerate}[label={  (\alph*)}]
    \item Use Theorem~\ref{thm:FiniteSqueeze} to show that $U(x)$ is
      continuous at $x=0$. \\
      \hint{What functions is $U(x)$ caught between?}
    \item Use Definition~\ref{def:continuity} to show that $T(x)$ is
      not
      continuous at $x=0$.  \\
      \hint{Try the substitution $z=\frac1x$ for $x\neq0$.  What would
        $\limit{x}{0}{T(x)}$ look like in terms of $z$?}
    \end{enumerate}
  \end{embeddedproblem}



\begin{mytheorem}[The Limit of a Composition is the Composition of the
  Limits]
  \label{thm:CompositionLimit}
  Suppose $\limit{x}{a}{g(x)}=L_g$ and that $f(x)$ is continuous at
  $L_g$. Then
  $$
  \limit{x}{a}{f(g(x))}=f\left(\limit{x}{a}{g(x)}\right)=f(L_g).
  $$  
\end{mytheorem}

Essentially this says that we can interchange the function $f$ and
the ``$\lim$'' symbols if $f$ is continuous at $g(a)$.





</subsection>
</section>
\section{The General Differentiation Theorems, via Limits}
\label{sec:diff-rules-via}
\markright{\sc The General Differentiation Rules, via Limits}
\mycenterquote {.\ .\ .\ one way in math to take care of destabilizing
  problems is to legislate them out of existence .\ .\ .\ by loading
  theorems with stipulations and exclusions designed to head off crazy
  results.}{\href{https://en.wikipedia.org/wiki/David_Foster_Wallace}{David
    Foster Wallace} ($1962$ - $2008$)}{.85}

Since we will now be proving the the differentiation rules rigorously
we will call them what they really are: Theorems. 
Because limits are much less intuitive than differentials we'll want
to be as efficient as possible when using them. The sooner we can
build up some tools to make things easier, the better. %  Recall that in
 
Also, in this section we will add a new differentiation rule
(theorem): The Chain Rule. Or rather, we will give a name to an
already familiar technique and elevate it's status by providing a
formal proof.  Proving the Chain, Product, and Quotient
Differentiation Rules using limits will require a good deal of
cleverness. These proofs will also uncover some unexpected subtleties
along the way.

Before we begin there is one more point that needs to be 
clear. Because differentiation is now defined via a limit and limits
are defined at a point we can only differentiate a function
at a point. We usually say that limit evaluation and
differentiability are \term{local properties}.  If we don't specify
the ``at $x$'' the convention is that the function is differentiable
at every point in its domain.


The proofs of the Constant, Sum, and Constant Multiple
Differentiation Rules are all completely straightforward so we
will leave them as exercises\aside{Frequently students will simply
  ignore problems that are described as straightforward. Don't make
  that mistake. Straightforward does not mean easy, and it does not
  mean unimportant. We are leaving these problems for you so you can
  gain experience using limits in the simplest cases, not because they
  are unimportant.} for you.


\begin{mytheorem}[The Constant Rule for Differentiation]
  \label{thm:LimConstantRule}
  If $L$ is some number and $f(x)=L$ for all real values of $x$
  near\aside{``Near'' means on an open interval about $L$. Recall Definition~\ref{def:near}.  }
  $L$, then
  $f^\prime(x)=0$ at every real number $x$ near\aside{On the same open
  interval.} $L$ .
  \begin{myproof}
    By Definition~\#\ref{eq:DefDerivative}
    \begin{align*}
      f^\prime(x)\amp =\limit{h}{0}{\frac{f(x+h)-f(x)}{h}}.\\
      \intertext{But $f(x+h)=L=f(x)$ so}
      f^\prime(x)\amp =\limit{h}{0}{\frac{L-L}{h}}=\limit{h}{0}{\frac{0}{h}}=0.
    \end{align*}
  \end{myproof}
\end{mytheorem}





\begin{mytheorem}[The Sum Rule for Differentiation]
  If $\alpha(x)$ and $\beta(x)$ are differentiable at $x$ and
  $f(x)=\alpha(x)+\beta(x)$, then $f(x)$ is also differentiable at $x$
  and
  $$
  f^\prime(x)=\alpha^\prime(x)+ \beta^\prime(x).
  $$
\end{mytheorem}


\begin{embeddedproblem-1line}
  Use Definition~\ref{def:Derivative} to prove the Sum Rule for
  Differentiation.
\end{embeddedproblem-1line}


Recall that when we first established the  General Differentiation Rules using
differentials in Chapter~\ref{chapt:differentials} we said that the
Constant Multiple,  Power and Quotient Rules for differentiation were just
conveniences because they depend on the other rules.

This is still true of course (except for the caveat in the point of
rigor below) which means that we don't have to prove any of them using
limits. For example, since the Constant Rule and the Product Rule are
now established theorems we can use these to prove the Constant
Multiple Rule directly, without having to resort to using limits.


\begin{mytheorem}[The Constant Multiple Rule for Differentiation]
  If $f(x)$ is differentiable at $x$ and $K$ is a constant then $\alpha(x)=Kf(x) $ is
  also differentiable and
  $$
  \alpha^\prime(x)=Kf^\prime(x).
  $$
\end{mytheorem}


\begin{embeddedproblem-1line}
  \label{PIC:ConstMultRule}
 Use the Definition~\#\ref{def:Derivative} to prove the Constant Multiple Rule.
\end{embeddedproblem-1line}


\section{The Chain Rule}

To understand the Chain Rule we will need to slightly blur the
distinction between function and variable.
\begin{myexample}
  \label{example:chain-rule1}
  Here's what we mean: The formula $y=(2x^2-6x)^3$, is given
    entirely in terms of the variables $x$, and $y$. To differentiate
    using differentials we would make the (variable) substitution
    $z=3x^2+6x$ so that $y=z^3$. In that case,
    $\dx{y}=3z\dx{z}=3\left(3x^2+6x\right)^2(6x+6)\dx{x}$, and
    dividing through by $\dx{x}$ gives us the derivative of $y$ with
    respect to $x$,
    <me>
      \label{eq:CR1}
      \dfdx{y}{x}=3z\dx{z}=3\left(3x^2+6x\right)^2(6x+6).% = \dfdx{y}{z}\cdot\dfdx{z}{x}.
  </me>

      But Definition~\#\ref{def:Derivative} requires that we think
      about functions, not variables so let's translate this problem
      into the language of functions. If $y=\left(2x^2-6x\right)^3$,
      clearly $y$ is a function of (depends on) $x$. Naming that
      function $f$, we have $y=f(x)$.
      Replacing $y$ with $f(x)$, we get
      $ f(x)=(2x^2-6x)^3$.

      Similarly, if $z=3x^2+6x$ then $z$ is also a function of
      (depends on) $x$, and naming that function $\beta$ we have
      $z=\beta(x)$. Replacing $z$ with $\beta(x)$ we have
      $f(x)=(\beta(x))^3$. If we suppress the ``$(x)$'' part of
      $\beta(x)$, we see that $f$ can also be thought of as a function
      of (depends on) $\beta$ so that
        $$
        f(\beta)= \beta^3
        $$
        is also a valid representation of our function. If we now
        define $\alpha(\beta)=\beta^3$ we see that
        $$
        f(x)= \alpha(\beta).
        $$

        Looking again at Equation~\ref{eq:CR1}, and mixing the
        differential and functional notations a bit we see that 
        $$
        f^\prime(x)=\dfdx{y}{x}=3z\dx{z}=\underbrace{3\left(3x^2+6x\right)^2}_{\dfdx{\alpha}{\beta}=\alpha^\prime(\beta)}\underbrace{(6x+6)}_{\dfdx{\beta}{x}=\beta^\prime(x)}=\alpha^\prime(\beta)\cdot\beta^\prime(x).
        $$
        
        
        Thus if $f(x)=\alpha(\beta(x))$ is the composition of $\alpha(x)$ and
        $\beta(x)$  then
        $$
        f^\prime(x)=\alpha^\prime(\beta(x))\beta^\prime(x).
        $$
        This is the Chain Rule.  We have expressed the Chain Rule in
        this form so that we can prove it rigorously, not so that we
        can use it. The substitution process using differentials still
        works so there is no reason to stop using substitution when
        you are actually computing derivatives.
      \end{myexample}

      
\begin{mytheorem}[The Chain Rule]
  \label{thm:ChainRule}
  Suppose that $\beta(x)$ is differentiable at $x$, that $\alpha(x)$
  is differentiable at $\beta(x)$ and that $\Delta\beta\neq0$ near
  $x$.
  Then the composition,\\
  \centerline{$ f(x) = \alpha(\beta(x)) $}
  is also differentiable, and
  <me>
    \label{eq:ChainRule}
    f^\prime(x) =\alpha^\prime(\beta(x))\cdot\beta^\prime(x).
</me>
\end{mytheorem}

\begin{Digression}[The Origins of the Chain Rule]
  \label{digression:ChainRule}
  Before the invention of Calculus, arithmetic primers gave the name
  ``The Chain Rule'' to the computational technique that is used to,
  among other things, convert money from one currency to another. For
  example if we need to convert $30$ American dollars (\$) to British
  pounds (\pounds) and we know that\aside{These numbers were accurate
    on the day this passage was written. They are almost certainly
    wrong on
    the day you are reading it. Don't use them to convert currency.}\\
  \centerline{ $1 \text{ dollar} = 0.86$ euros, and that
    $1 \text{ euro} = 0.9$ pounds.  }
  Then the conversion is
$$
\$30= 30 \text{ \textcolor{red}{\cancel{dollars}}} \times
\frac{0.86}{1} \frac { \text { \textcolor{blue}{\cancel{euros}} } } {
  \text{\textcolor{red}{\cancel{dollars}}} } \times \frac{0.9}{1}
\frac{\text{pounds}}{\text{\textcolor{blue}{\cancel{euros}}}}
=30\times0.86\times0.9\text{ pounds}=23.22 \pounds
$$



We've actually seen this type of conversion before. We used in in
Section~\ref{cha:calc-trig} when we converted angular velocity to
linear velocity via the formula:
$$
\left(\frac{3}{1}\frac{\text{\cancel{revolution}}}{{\text{\cancel{minute}}}}\right)\cdot
\left(\frac{2\pi}{1}
  \frac{\text{\cancel{radians}}}{\text{\cancel{revolution}}}\right)\cdot
\left(\frac{10}{1}\frac{\text{meters}}{\text{\cancel{radians}}}\right)\cdot
\left(\frac{1}{60}\frac{\text{\cancel{minute}}}{\text{second}}\right)
=\frac{\pi}{1} \frac{\text{meters}}{\text{second}} \approx 3.14
\frac{\text{meters}}{\text{second}}.
$$

A similar chain of cancellations will occur when we differentiate a
function composition of the form
$\alpha(t)=\alpha(\beta(y(x(t))))$. We think of\\\\
\centerline{ $\alpha$ as a function of $\beta$ (so that
  $\alpha^\prime(\beta)=\dfdx{\alpha}{\beta}$)} \\\\
\centerline{ $\beta$
  as a function of $y$ (so that $\beta^\prime(y)=\dfdx{\beta}{y}$),}\\\\
\centerline{ $y$ as a function of $x$ (so that
  $y^\prime(x)=\dfdx{y}{x}$),}\\\\
\centerline{ and $x$ as a function of
  $t$ (so that $y^\prime(x)=\dfdx{x}{t}$).}\\\\

Putting this all together we see that
$$
\alpha^\prime(t) =
\dfdx{\alpha}{\cancel{\beta}}\cdot\dfdx{\cancel{\beta}}{\cancel{y}}\cdot\dfdx{\cancel{y}}{\cancel{x}}\cdot\dfdx{\cancel{x}}{t}
= \dfdx{\alpha}{t}.
$$

The substitutions we used to make things ``easier on your eyes'' in
Section~\ref{sec:an-easy-problem-alg} is equivalent this chain of
cancellations.  With the invention of Calculus the older Chain Rule
for unit conversion was extended to the differentiation by
substitution technique using differentials. Eventually it became the
only Chain Rule. When the limit was used to provide rigor to Calculus
 the name was also applied to Equation~(\ref{eq:ChainRule}).
\end{Digression}

  
Understanding the Chain Rule in this form requires that we blur the
distinction between function and variable a bit. When we compute
$\dfdx{\alpha}{\beta}=\alpha^\prime(\beta)$ (the derivative of
$\alpha$ with with respect to $\beta$)  we view $\beta$ as a
variable, but when we compute $\dfdx{\beta}{x}=\beta^\prime(x)$ (the
derivative of $\beta$ with respect to $x$) we view it as a function.
  


  As far as the Chain Rule is concerned it is both.

  \begin{myproof}
    Before we begin take specific notice of the assumption
    ``$\Delta\beta\neq0$ near $x$'' in the statement of the Chain
    Rule. We will have
    a few comments about this in
    Digression~\#\ref{digression:DeltaBetaNotZero} after the proof is completed.
                            
  
  We will first establish that
  <me>
    \label{eq:DeltaBetaEq0}
    \limit{h}{0}{\Delta\beta}=0.
  </me>
    
  Suppose\aside{Because asserting the equality of non-existing objects
    would be meaningless we assume, implicitly that all limits in this
    argument exist.} that
  <me>
    \beta(x+h)-\beta(x)=\Delta\beta\label{eq:ChainRule1}.
  </me>
  
Then
  \begin{align*}
    \limit{h}{0}{\beta(x+h)}\amp =\limit{h}{0}{(\beta(x)+\Delta\beta)}.
    \\
    \intertext{By Theorem~\ref{theorem:LimSum2}
    we have}
    \limit{h}{0}{\beta(x+h)}\amp =\limit{h}{0}{\beta(x)}+\limit{h}{0}{\Delta\beta},
    \\
    \intertext{and since $\beta(x)$ is differentiable at $x$ we see from
    Lemma~\ref{lemma:DiffImpliesCont} 
    that}
    \beta(x)\amp =\beta(x) +\limit{h}{0}{\Delta\beta}
  \end{align*}
  from  which Equation~(\ref{eq:DeltaBetaEq0}) follows. 
  
  To prove the Chain Rule recall that 
  \begin{align*}
    f^\prime(x)\amp =\limit{h}{0}{\frac{f(x+h)-f(x)}{h}}\\ 
   \amp =\limit{h}{0}{\frac{\alpha(\beta(x+h))-\alpha(\beta(x))}{h}}.
  \end{align*}
  Multiplying  by $1$ in the form\aside{In the past we
    have called this ``uncancelling'' $\Delta\beta$. Also, notice that
  this is where we use the assumption, ``$\Delta\beta\neq0$ near
  $x$.''}
    \textcolor{red}{$\frac{\Delta\beta}{\Delta\beta}$   gives}
  \begin{align}
  \label{eq:UncancelDeltaBeta}  f^\prime(x)    \amp =\limit{h}{0}{\left(\frac{\alpha(\beta(x+h))-\alpha(\beta(x))}{\textcolor{red}{\Delta\beta}}\cdot\frac{\textcolor{red}{\Delta\beta}}{h}\right)}.\\
    \intertext{From Equation~(\ref{eq:ChainRule1}) we see that
    $\textcolor{red}{\Delta\beta}=\textcolor{blue}{\beta(x+h)-\beta(x)}$ so}
\nonumber    \amp =\limit{h}{0}{\left(\frac{\alpha(\beta(x+h))-\alpha(\beta(x))}{\Delta\beta}\cdot\frac{\textcolor{blue}{\beta(x+h)-\beta(x)}}{h}\right)}.\\
    \intertext{By Theorem~\ref{theorem:LimProd2} we have:}
\nonumber    \amp =\limit{h}{0}{\left(\frac{\alpha(\beta(x+h))-\alpha(\beta(x))}{\Delta\beta}\right)}\cdot\limit{h}{0}{\left(\frac{\beta(x+h)-\beta(x)}{h}\right)}.\\
\intertext{Equation~(\ref{eq:DeltaBetaEq0}) says that $h\rightarrow 0$
    is equivalent to 
  $\Delta\beta\rightarrow 0$ so we have}
\nonumber    f^\prime(x)\amp =\lim_{\textcolor{red}{\underset{\Delta\beta\rightarrow0}{\cancel{h\rightarrow0}}}}{\left(\frac{\alpha(\beta+\Delta\beta)-\alpha(\beta)}{\Delta\beta}\right)}\cdot\limit{h}{0}{\left(\frac{\beta(x+h)-\beta(x)}{h}\right)},\\
\label{eq:ChainRuleProof1}    f^\prime(x)           \amp =\underbrace{\limit{\Delta\beta}{0}{\left(\frac{\alpha(\beta+\Delta\beta)-\alpha(\beta)}{\Delta\beta}\right)}}_{=\alpha^\prime(\beta)}\cdot\underbrace{\limit{h}{0}{\left(\frac{\beta(x+h)-\beta(x)}{h}\right)}}_{=\beta^\prime(x)}.\\
   \label{eq:ChainRuleBetaIsVariable}    f^\prime(x)
    \amp =\alpha^\prime(\beta)\cdot\beta^\prime(x).
  \end{align}
  In Equation~(\ref{eq:ChainRuleBetaIsVariable} $\beta$ is first used
  as a variable in $\alpha^\prime(\beta)$, and then as the function
  $\beta(x)$. While this is correct, it is also poor form because it
  accentuates the dual use of $\beta$. To avoid this we usually
  express the Chain Rule as
  $$
  f^\prime(x) =\alpha^\prime(\beta(x))\cdot\beta^\prime(x)
  $$
  to
  emphasize that $x$, not $\beta$, is the variable.
\end{myproof}

\begin{Digression}[\boldmath Why Assume That $\Delta\beta\neq0$ Near Zero?]
  \label{digression:DeltaBetaNotZero}
  Do you see why we had to assume that $\Delta\beta\neq0$ near $x$?
  
  Observe that in Equation~(\ref{eq:ChainRuleProof1}) $\Delta\beta$
  plays the same role the $h$ plays in
  Definition~(\ref{def:Derivative}). In
  Definition~(\ref{def:Derivative}) we were careful to insist that $h$
  could never equal zero,so if we are going to interpret
  $\limit{\Delta\beta}{0}{\left(\frac{\alpha(\beta+\Delta\beta)-\alpha(\beta)}{\Delta\beta}\right)}$
  as the derivative of $\alpha$ with respect to $\beta$, as we did in
  Equation~(\ref{eq:ChainRuleProof1}), we need to know that
  $\Delta\beta\neq0$ when $h$ is near zero.

  Our imposition of that constraint means that
  Theorem~\#\ref{thm:ChainRule} does not apply to any function
  $f(x)=\alpha(\beta(x))$ where $\Delta\beta$ might be equal to zero
  no matter how close $h$ is to zero.  Fortunately, functions of that
  sort are generally the kinds of ``pathological functions'' that
  Poincar\'e is complained about in the quote at the beginning of this
  chapter. A valid proof of the Chain Rule without that constraint is
  possible, but since it would have very little relevance to anything
  we'll be doing we have chosen to prove only this weaker form of the
  Chain Rule\aside{You may be wondering if our choice  to go with a
    weaker form of the Chain Rule means we've given Bishop Berkeley
    cause for compliant. The answer
    is no, we haven't. If we'd left off the condition that
    $\Delta\beta\neq0$, then our proof would not have been rigorous
    because we'd have ended by claiming more than we'd proved. As it
    is, we've only claimed what we have proved.\\
    Rigorous does not mean perfect, it means logical.
  }

  If you are unsatisfied with this proof and want to see a proof of
  the stronger version of the Chain Rule, consider majoring in
  mathematics. You'll see that and much, much more. In the meantime
  try working through the following problem.

  \begin{embeddedproblem}
    \begin{enumerate}[label={  (\alph*)}] 
          \item   Show that the function
      $
      \beta(x)=\sin\left(\frac1x\right)
      $
      does not satisfy the constraint $\Delta\beta\neq0$ when $x$ is near
      zero.\\
      \hint{Recall Definition~\#\ref{def:near}.}
    \item As a result of part (a) Theorem~\ref{thm:ChainRule} does
      not apply to any of the following functions at
      $x=0$. Nevertheless one of them is differentiable at $x=0$. Use
      Definition~\#\ref{def:Derivative} to find out which one.
      \begin{enumerate}[label={  (\roman*)}]       
         \begin{multicols}{2}
      \item $T(x)=
        \begin{cases}
          \sin\left(\frac1x\right) \amp  x\neq0\\
          0                        \amp  x=0
        \end{cases}.$
      \item $U(x)=
        \begin{cases}
          x\sin\left(\frac1x\right) \amp  x\neq0\\
          0                        \amp  x=0
        \end{cases}.$
      \item $V(x)=
        \begin{cases}
          x^2\sin\left(\frac1x\right) \amp  x\neq0\\
          0                        \amp  x=0
        \end{cases}.$
      \end{multicols}
    \end{enumerate}
          \end{enumerate}
  \end{embeddedproblem}
\end{Digression}


\begin{myexample}
  \label{example:DiffByCR}
  Suppose that $f(x)=\left(\sin(x)+\cos(x)\right)^2$. To use the Chain
  Rule to compute the derivative of $f(x)$ we need to recognize that
  $f(x)$ is the composition of $\alpha(x)=x^2$, and
  $\beta(x)=\sin(x)+\cos(x)$ and then apply
  Theorem~\ref{thm:ChainRule} as follows.
  \begin{align*}
    f^\prime(x) \amp = \alpha^\prime(\beta(x))\cdot\beta^\prime(x)\\
    \amp = \alpha^\prime(\beta(x))\cdot(\cos(x)-\sin(x))\\
    \amp = \alpha^\prime(\sin(x)+\cos(x))\cdot(\cos(x)-\sin(x))\\
    f^\prime(x)\amp = 2
    (\sin(x)+\cos(x))\cdot(\cos(x)-\sin(x)).
  \end{align*}
\end{myexample}

In our opinion the Chain Rule leaves a lot to be desired as a
computational technique. But we don't have to use it that way since
Theorem~\ref{thm:ChainRule} validates the substitutions  we have
always used.

\begin{ProficiencyDrill}
  Suppose $y=f(x)=\left(\sin(x)+\cos(x)\right)^2$.  Compute the
  differential $\dx{y}$ and then divide through by $\dx{x}$ to find the
  derivative $\dfdx{y}{x}$. Confirm that it is the same as the
  derivative we found in Example~\#\ref{example:DiffByCR}.
\end{ProficiencyDrill}

\begin{embeddedproblem}
  Compute $\dfdx{y}{x}$ for each of the following functions by
  identifying $\alpha(x)$ and $\beta(x)$ such that
  $y(x) = \alpha(\beta(x))$ and applying the Chain Rule. You may have
  to do this more than once for a given problem.
  
  In each case confirm that your computation is correct with an
  appropriate differential substitution.
  \begin{enumerate}[label={  (\alph*)}]
    \begin{multicols}{2}
    \item $y=(3x+5)^6$
    \item $y=\sec(\tan(x))$
    \item $y=\sqrt[7]{\frac{1}{x} +x^3}$
    \item $y=\left(\frac{x-x^{\frac12}}{x^3-1}\right)^2$
    \item $y=e^{x-\cos^2(x)}+(2x^2-3)^{\frac15}$
    \item $y=\sqrt{x+\sqrt[3]{2+\sqrt[4]{3-x^2}}}$
    \end{multicols}
  \end{enumerate}
  \end{embeddedproblem}

\section{The Product Rule}
\label{subsec:product-rule-limits}

A rigorous proof of the Product Rule is also fairly complex, but it
does not suffer from the kind of technical problems we encountered in
the proof of the Chain Rule.

\begin{mytheorem}[The Product Rule for Differentiation]
  If $\alpha(x)$ and $\beta(x)$ are differentiable at $x$ then
  $f(x)=\alpha(x)\cdot\beta(x)$ is differentiable and
  <me>
    \label{eq:PRviaLimit2}
    f^\prime(x)=\alpha(x)\cdot\beta^\prime(x)+\beta(x)\cdot\alpha^\prime(x).
  </me>
  \end{mytheorem}
\begin{myproof}
  We start with the two observations. The first is that
  <me>
    f^\prime(x)=\limit{h}{0}{\frac{\textcolor{red}{f(x+h)}-\textcolor{blue}{f(x)}}{h}}
    =  \limit{h}{0}{\frac{\textcolor{red}
        {\alpha(x+h)\beta(x+h)}-\textcolor{blue}{\alpha(x)\beta(x)}}{h}}
    \label{eq:PRviaLimit}
  </me>
  and the second is that, in limit form,
  Equation~(\ref{eq:PRviaLimit2}) is
  <me>
    \label{eq:PRviaLimit3}
    f^\prime(x)=\alpha(x)\left(\limit{h}{0}{\frac{\beta(x+h)-\beta(x)}{h}}\right)+\beta(x)\left(\limit{h}{0}{\frac{\alpha(x+h)-\alpha(x)}{h}}\right).
  </me>
  It appears then that our goal is to simply reorganize
  Equation~(\ref{eq:PRviaLimit}) until it looks like
  Equation~(\ref{eq:PRviaLimit3}).  We say ``simply'' but it will only
  appear to be simple after we have succeeded. We will proceed slowly.
  
  Observe that if we subtract $\alpha(x+h)\beta(x)$ from the blue part
  of the numerator
  in Equation~(\ref{eq:PRviaLimit}) we get
  $$
  \alpha(x)\beta(x)-\alpha(x+h)\beta(x)=
  \textcolor{blue}{-\beta(x)\left(\alpha(x+h)-\alpha(x)\right)},
  $$
  whereas if we add $\alpha(x+h)\beta(x)$ to the red part of
  the numerator in
  Equation~(\ref{eq:PRviaLimit})  we get
  $$
  \alpha(x+h)\beta(x+h)+\alpha(x+h)\beta(x) =
  \textcolor{red}{\alpha(x+h)\left(\beta(x+h)-\beta(x)\right)}.
  $$
  
  This suggests that we should both add and subtract the expression
  $\alpha(x+h)\beta(x)$ to the numerator of
  Equation~(\ref{eq:PRviaLimit}).  Doing this and factoring as we've
  indicated above we get
  $$
  f^\prime(x)=\limit{h}{0}{ \frac{ \textcolor {red}
      {\alpha(x+h)\left(\beta(x+h)-\beta(x)\right)} -
      \left[\textcolor{blue}{-\beta(x)\left(\alpha(x+h)-\alpha(x)\right)}\right]
    }{h} },
  $$
  
  By Theorem~\ref{theorem:LimSum2} we can separate this into the limit of
  the two fractions as follows:
  $$
  f^\prime(x)=\tlimit{h}{0}{\left(\textcolor{red}{\frac{\textcolor{green}{\alpha(x+h)}\left(\beta(x+h)-{\beta(x)}\right)}{h}}\right)}
  +
  \rlimit{h}{0}{\left(\textcolor{blue}{\frac{\textcolor{green}{\beta(x)}\left(\alpha(x+h)-\alpha(x)\right)}{h}}\right)},
  $$
  and by Theorem~\ref{theorem:LimProd2} we see that
  $$
  f^\prime(x)=
  \underbrace{\left[ \tlimit{h}{0}{\textcolor{green}{\alpha(x+h)}}
    \right]}_{=\alpha(x)} \underbrace{\left[ \tlimit{h}{0}{ \left(
          \textcolor{red}{\frac{\beta(x+h)-\beta(x)}{h}} \right) }
    \right]}_{=\beta^{\prime}(x)} + \underbrace{\left[
      \tlimit{h}{0}{\textcolor{green}{\beta(x)}} \right]}_{\beta(x)}
  \underbrace{\left[ \limit{h}{0}{ \left(
          \textcolor{blue}{\frac{\alpha(x+h)-\alpha(x)}{h}} \right) },
    \right]}_{\alpha^\prime(x)}
  $$
  and therefore
  $$ f^\prime(x)=\alpha(x)\beta^\prime(x)+\beta(x)\alpha^\prime(x).$$
\end{myproof}



\section{The Other General Differentiation Rules}
\label{sec:other-gener-diff}
\markright{The Other General Differentiation Rules}




\begin{mytheorem}[The Quotient Rule for Differentiation]
  \label{thm:QuotientRuleLimit}
  We assume that $\alpha(x)$, $\beta(x)$, and
  $f(x)=\frac{\alpha(x)}{\beta(x)}$ are all differentiable
  functions\aside{Notice that we have explicitly assumed that the
    quotient }.
  Assume further that $\beta(x)\neq0$.  Then
  $$
  f^\prime(x)=\frac{\beta(x)\alpha^\prime(x)-\alpha(x)\beta^\prime(x)}{\left[\beta(x)\right]^2}.
  $$
\end{mytheorem}
Proving this directly by using limits would be unpleasant, but as we
observed in Chapter~\ref{chapt:differentials} the Quotient Rule can be
viewed as a rearranged version of the Product Rule.

\begin{embeddedproblem-1line}
  Use the Product Rule to derive the Quotient Rule.\\
  \hint{First solve $f(x)=\frac{\alpha(x)}{\beta(x)}$ for
    $\alpha(x)$. }
\end{embeddedproblem-1line}


With the Product Rule for Differentiation in place we now have the
tools needed to prove the Power Rule for Positive Integer Exponents.
The method of proof we outline in the following problem is called
\term{Mathematical Induction}\aside{We mentioned \term{Mathematical
    Induction} In part (d) of Problem~\#\ref{problem:PR-integer} back
  in Chapter~\ref{chapt:differentials}.}  and it can be used in other
contexts as well. In fact, most of the ``Find the Pattern'' problems
in this text require an Induction argument for full rigor.

\begin{embeddedproblem}[The Power Rule for Positive Integer Exponents]
  Assume  that $\alpha(x)=x^n$ is differentiable at $x$ for
  any positive integer $n$.
  \begin{description}
  \item[   Part 1:] Assume that $n=1$.  Use the limit definition to
    show that $\alpha^\prime(x) = nx^{n-1}.$ \\
    \comment{This says, ``The Power Rule holds for $k=1$.''}
\item[   Part 2:] Now assume that the Power Rule for Positive
  Integer Exponents holds for $n=k$, where $k$ is an arbitrary, fixed
  positive integer.
    
    Let $\beta(x)=x^{k+1}$ and show that $\beta^\prime(x)=(k+1)x^k.$\\
    \comment{This says, ``If the Power Rule holds for $k$ then it
    must also hold for $k+1$.''}
  \end{description}
  
  Do you see how this proves that the Power Rule holds for any
  positive integer, $n$? Write a short paragraph explaining the logic
  behind this.
\end{embeddedproblem}

With the {  Power Rule for Positive Integer Exponents} in place we
can extend it to both negative and rational exponents in the same way
we did it in Chapter~\ref{chapt:differentials}. The following problem
is essentially a repeat of problems~\ref{EP:PRPosRat} and
\ref{EP:PRRecip}, using Lagrange's prime notation, and function
notation, rather than differentials.


\begin{embeddedproblem-enumerate}[The Power Rule for Rational and
  Negative Exponents]
  \begin{enumerate}[label={  (\alph*)}]
  \item Assume $n$ is a positive integer and that $\alpha(x)=x^{-n}$ is
    differentiable.  Show that
    $$\alpha^\prime(x) = -nx^{-(n+1)}.$$
    
    \hint{Rewrite $\alpha(x)=x^{-n}$ as $\frac{1}{x^n}$ and use the
      Quotient Rule for Differentiation and the Power Rule for positive integers.}
  \item Assume that $q$ is a non-zero integer and that
    $\alpha(x)=x^{1/q}$ is differentiable at $x$.  Show
    that $$\alpha^\prime(x) = (1/q)x^{(1/q-1)}.$$
    
    \hint{Rewrite $\alpha(x)=x^{1/q}$ as
      $$\left[\alpha(x)\right]^q=x$$ and use the Chain Rule and
      the Power Rule for positive integers.}
  \item Assume that $p$ and $q$ are integers,  $q\neq0$, and that
    $\alpha(x)=x^{p/q}$ is differentiable at $x$.  Show
    that $$\alpha^\prime(x) = (p/q)x^{(p/q-1)}.$$
    
    \hint{Rewrite $\alpha(x)=x^{p/q}$ as
      $\alpha(x)=\left(x^{1/q}\right)^p$ and use the Chain Rule and
      part (b).}
  \end{enumerate}
\end{embeddedproblem-enumerate}

\noindent Together the previous two problems prove the Power Rule
for rational exponents:

\begin{mytheorem}[The Power Rule for Rational Exponents]
  \label{thm:PowerRuleLimit}
  Assume that $p$ and $q$ are integers, $q\neq0$, and that
  $\alpha(x)=x^{p/q}$ is differentiable at $x$. Then
  $$
  \alpha^\prime(x) = (p/q)x^{(p/q-1)}.
  $$
\end{mytheorem}

In the statement of Theorem~\#\ref{thm:QuotientRuleLimit} we
explicitly assumed that the quotient, $\frac{\alpha(x)}{\beta(x)}$, is
differentiable at $x$. This has the effect that the theorem does not
necessarily apply to all possible quotients, in the same way that when
we add $\Delta\beta\neq0$ to the statement of the Chain Rule, the
theorem applies to fewer compositions. And just like the Chain Rule
the functions that Theorem~\#\ref{thm:QuotientRuleLimit} does not
apply to are mostly pathological, and of no use to us right now.

We added the same assumption to Theorem~\#\ref{thm:PowerRuleLimit} for
similar reasons.

\begin{Digression}[Are You a Mathematician?]
  If leaving these theorems incomplete in this way is troubling to
  you then you are almost certainly a mathematician by temperament. If
  you haven't decided on a major yet, consider mathematics. You
  obviously like it. Why not learn more?

  If you find that you simply don't care about completing all of the
  details and you are not majoring in mathematics, congratulations!
  You've made the right choice.

  Problem~\#\ref{problem:QR-rigor}  will lead 
  you through the steps necessary to prove the Quotient
  Rule for Differentiation without the assumption that
  $\frac{\alpha(x)}{\beta(x)}$ is differentiable. Have fun!
  
  \begin{embeddedproblem}
    \label{problem:QR-rigor}
     Assume that $\alpha(x)$ and $\beta(x)$ are
    differentiable and that $\beta(x)\neq0$, but we make no assumption
    about the differentiability of $f(x)=\frac{\alpha(x)}{\beta(x)}$.
    \begin{enumerate}[label={  (\alph*)}]
    \item First prove the special case of the Quotient Rule where
      $f(x)=\frac{1}{\beta(x)}$.
      \begin{enumerate}[label={  (\roman*)}]
      \item Use the limit definition to show that
        $\beta^\prime(x)=\limit{h}{0}{\frac{\beta(x)-\beta(x+h)}{h\beta(x)\beta(x+h)}}$.
      \item Now evaluate the limit in part (i) to show that
        $f^\prime(x)=
        \frac{-\beta^\prime(x)}{\left[\beta(x)\right]^2}$.
      \end{enumerate}
    \item Use the Product Rule for Differentiation and the  Chain Rule (along with the result
      of part a) to show that $f(x)=\frac{\alpha(x)}{\beta(x)}$ is differentiable at $x$ and that\\
      \centerline{$\displaystyle f^\prime(x) =
        \frac{\beta(x)\alpha^\prime(x) -
          \alpha(x)\beta^\prime(x)}{\left[\beta(x)\right]^2}.  $}
    \end{enumerate}
      \end{embeddedproblem}

  Problem~\#\ref{problem:PR-rigor2} will lead 
  you through the steps necessary to prove the Product 
  Rule for Rational Exponents without the assumption that
  $x^{\frac{p}{q}}$ is differentiable. It relies on the result of
    Problem~\#\ref{problem:PR-rigor1}.
    Have fun!
  
  \begin{embeddedproblem}
        \label{problem:PR-rigor1}
    To prove  Theorem~\ref{thm:PowerRuleLimit} we will first focus
    on the special case of $\beta(x)=x^{\frac{1}{q}}$, $q$ is a
    non-negative integer.
    
    The key to proving this special case is a generalization of the
    difference of squares formula: $(a-b)(a+b)=a^2-b^2.$
    
    \begin{enumerate}[label={  (\alph*)}]
    \item Show that $(a-b)(a^2+ab+b^2)=a^3-b^3$.
    \item Show that $(a-b)(a^3+a^2b+ab^2+b^3)=a^4-b^4$.
    \item Use \term{Mathematical Induction} to show that\\
      \centerline{$(a-b)(a^{q-1}+a^{q-2}b+a^{q-3}b^2+\cdots+ab^{q-2}+b^{q-1})=a^q-b^q$.}
    \end{enumerate}
      \end{embeddedproblem}
  

  \begin{embeddedproblem}
    \label{problem:PR-rigor2}
    Assume that $p$ and $q$ are integers and that $q\neq0$. If we
    apply Definition~\#\ref{def:Derivative} to $f(x)=x^\frac{1}{q}$, we
    get\\
    \centerline{$
      f^\prime(x)=\limit{h}{0}{\frac{(x+h)^\frac1q-x^\frac1q}{h}}.  $}
    \begin{enumerate}[label={  (\alph*)}]
    \item Use the substitutions $a=(x+h)^\frac1q$, $b=x^\frac1q$,
      and part (c) of the previous problem to show that
      $$
      f^\prime(x)=\limit{a}{b}{\frac{a-b}{a^q-b^q}}=\frac{1}{qb^{q-1}}.
      $$
    \item Substitute $b=x^{\frac1q}$ into the result of part a
      to obtain
      $$
      f^\prime(x)=\frac1qx^{\frac1q-1}.
      $$
    \item Use the Chain Rule to show that for
      $\alpha(x)=x^{\frac{p}{q}}$
      $$
      \alpha^\prime(x)=\frac{p}{q}x^{\frac{p}{q}-1}.
      $$
    \end{enumerate}
  \end{embeddedproblem}
\end{Digression}  


\section{Derivatives of the Trigonometric Functions, via Limits}
\label{sec:diff-trig-funct}
\markright{\sc Trigonometric Derivatives, via Limits}
\TLogo{PSP:deriv-sine-cosine}


\begin{mytheorem}[\boldmath Derivative of $\sin(x)$]
  \label{thm:SineDeriv}
  Suppose $\alpha(\theta)=\sin(\theta)$. Then
  $ \alpha^\prime(\theta)=\cos(\theta).  $
\end{mytheorem}
\begin{myproof}
  Showing that the derivative of $\sin(\theta)$ is $\cos(\theta)$ is
  mostly straightforward but we're going to hit a snag partway
  through. We'll proceed for a bit to see where the trouble is.
  
  Start with the limit definition:
  $$    \alpha^\prime(\theta) =
  \limit{h}{0} \frac{\sin(\theta+h)-\sin(\theta)}{h}
  $$
  In the numerator we see the expression $\sin(\theta+h)$.  Recall the
  sum formula for the Sine:
  $$
  \sin(A+B)=\sin(A)\cos(B)+\cos(A)\sin(B).
  $$
  Taking $A=\theta$ and
  $B=h$ we have:
  \begin{align*}
    \alpha^\prime(x)  \amp =
    \limit{h}{0}{\frac{\textcolor{red}{\sin(\theta)}\cos(h)+\cos(\theta)\sin(h)-\textcolor{red}{\sin(\theta)}}{h}}\\
    \intertext{Next, if we factor \textcolor{red}{$\sin(\theta)$} out
    of the terms where it appears and rearrange the numerator a bit we
    have:}
    \amp =
    \limit{h}{0}{\frac{\textcolor{red}{\sin(\theta)}(\cos(h)-1)+\cos(\theta)\sin(h)}{h}}.\\
    \amp =
    \limit{h}{0}{\left(\frac{\sin(\theta)(\cos(h)-1)}{h}+\frac{\cos(\theta)\sin(h)}{h}\right)}.\\
    \intertext{By Theorem~\ref{theorem:LimSum2}:}
    \amp =
    \limit{h}{0}{\frac{\sin(\theta)(\cos(h)-1)}{h}}+\limit{h}{0}{\frac{\cos(\theta)\sin(h)}{h}}\\
    \intertext{and by Corollary~\ref{cor:ConstMultLimit}:}
    \amp =
    \sin(\theta)\underbrace{\left(\limit{h}{0}{\frac{(\cos(h)-1)}{h}}\right)}_{=0}+\cos(\theta)\underbrace{\left(\limit{h}{0}{\frac{\sin(h)}{h}}\right)}_{=1}.
  \end{align*}
  If the values of the two limits are $0$ and $1$ respectively as we've
  indicated we can conclude that $\alpha^\prime(\theta)=\cos(\theta)$.
  
  
  Unfortunately this proof cannot be considered complete until we have
  shown that these last two limits are what we claim they are. We will
  do this via the two  lemmas below.
  
\end{myproof}

It is tempting to use L'H\^o pital's Rule to evaluate these limits,
especially since it is so very easy to do.

\begin{ProficiencyDrill-1line}
  \label{drill:CircularReasoning}
  Use L'H\^o pitals Rule to show that
  $\tlimit{h}{0}{\frac{(\cos(h)-1)}{h}}=0$ and that
  $\tlimit{h}{0}{\frac{\sin(h)}{h}}=1$.
\end{ProficiencyDrill-1line}

Sadly, using Drill~\#\ref{drill:CircularReasoning} to finish the proof
of Theorem~\#\ref{thm:SineDeriv} is an example of circular
reasoning. We can't use the fact that the derivative of $\sin(x)$ is
$\cos(x)$ to prove that the derivative of $\sin(x)$ is $\cos(x)$. So
we will have to find a way to evaluate these limits without using
L'H\^o pital's Rule.

\begin{mylemma-1line}
  \label{lemma:SinOverh}
  $\limit{h}{0}{\frac{\sin(h)}{h}}=1$
\end{mylemma-1line}
\begin{myproof}
  \begin{wrapfigure}[]{r}{3in}
     \vskip-8mm 
    \captionsetup{labelformat=empty}
    \centerline{\includegraphics*[height=2in,width=3in]{../Figures/SqueezeThm2}}
    \label{fig:SqueezeThm2}
  \end{wrapfigure}
  There are two cases:\\
  \underline{  \boldmath Case 1, $\theta\ge0$:} We will use the
  Squeeze Theorem.  Recall that in Section~\ref{subsec:trig-interl} we
  observed that the lengths of certain line segments associated with
  the unit circle in the first quadrant are equal to the trigonometric
  functions. The figure at the right shows the relationship between
  $\theta$, $\sin(\theta)$, and $\tan(\theta)$. Notice in particular
  that
  $$
  \sin(\theta)\le\theta\le\tan(\theta).
  $$
  
  Dividing each expression in the inequality by $\sin(\theta)$
  almost does the trick:
  $$
  1\le\frac{\theta}{\sin(\theta)}\le\frac{1}{\cos(\theta)}.
  $$
  In the center we now have the reciprocal of what we need, so we need
  to invert each expression.
  
  However, keep in mind that these are not equations they are inequalities. When we
  invert
  an inequality we must reverse its sense. This gives
  $$
  1\ge\frac{\sin(\theta)}{\theta}\ge\frac{\cos(\theta)}{1},
  $$
  and
  this is true on the interval $\left[0,\frac{\pi}{2}\right]$.
  Since
  $$
  \rlimit{\theta}{0}{1}=\rlimit{\theta}{0}{\cos(\theta)}=1
  $$
  the Squeeze Theorem applies, and we conclude that
  $$
  \rlimit{\theta}{0}{\frac{\sin(\theta)}{\theta}}=1.
  $$
  \underline{  \boldmath Case 2, $\theta\lt0$:} For this case
  notice that $\sin(-\theta)=-\sin(\theta)$ so that
  $ \frac{\sin(-\theta)}{-\theta}=\frac{\sin(\theta)}{\theta}$. We
  make the substitution $\theta=-\phi$ where $\phi\gt0$.  Therefore
  when $\theta\lt0$ we have
  $$
  \llimit{\theta}{0}{\frac{\sin(\theta)}{\theta}}
  =\rlimit{\phi}{0}{\frac{\sin(-\phi)}{-\phi}}
  =\rlimit{\phi}{0}{\frac{\sin(\phi)}{\phi}} =1
  $$
  by Case 1.
\end{myproof}

\begin{embeddedproblem-1line}
  \label{problem:CosM1Overh}
  Show that $\tlimit{h}{0}{\frac{(\cos(h)-1)}{h}}=0$.\\
  \hint{It is tempting to model this proof on the proof of
    Lemma~\ref{lemma:SinOverh}. While this can be done, it is 
    delicate. It is simpler to multiply by $1$ in the form
    $\frac{\cos(h)+1}{\cos(h)+1}$. Try that instead.}
\end{embeddedproblem-1line}
Once Problem~(\ref{problem:CosM1Overh}) has been solved the 
proof that $\dfdx{(\sin(x))}{x}= \cos(x)$ is complete.

\begin{embeddedproblem-1line}
  Prove that $\dfdx{(\cos(\theta))}{\theta} = -\sin(\theta)$, using
  the proof of Theorem~\ref{thm:SineDeriv} as a guide.
\end{embeddedproblem-1line}

Assuming that $\tan(\theta)$, $\cot(\theta)$, $\sec(\theta)$, and
$\csc(\theta)$ are differentiable we can now use
Theorem~\ref{problem:QR-rigor} to find their derivatives as
well. Since this is exactly what we did in
Section~\ref{subsec:diff-other-trig} we have the derivatives of all of
the trigonometric functions.

\section{Inverse Functions}
\label{sec:inverse-functions}
Although we have worked with the inverses of some specific functions
we have not formally defined what we mean by an inverse. We will
remedy that now. We have seen that not all functions can be inverted
(see for example, Digression~\#\ref{Digression:TangentHasNoInverse})
so the first step is to define which functions are invertible.

Informally a function that never takes the same value twice is called
a \term{one-to-one function}\aside{They are also called
  \term{injective}.}. Formally we have the following.

\begin{mydefinition}[One-To-One Functions]
  A function, $f(x)$, defined on a domain, $D$, is said to be
  one-to-one if, whenever $x_1$ and $ x_2$ are in $D$ and $x_1\neq
  x_2$ then, $f(x_1)\neq f(x_2)$. 
\end{mydefinition}
Recall that
when we tried to invert $\tan(x)$ (which is not one-to-one) in
Section~(\ref{sec:witch-agnesi-inverse}) we got the multifunction
$\arctan(x)$. We had to restrict the domain of the tangent function to
$\frac{-\pi}{2}\le x \le \frac{\pi}{2}$, in order to find an
inverse. That restriction gave us a one-to-one function which we could
invert because one-to-one functions are the only functions with inverses. 

\begin{mydefinition}[Inverse Functions]
  \label{def:FunctionInverse}
  Suppose $f(x)$, with domain $D$ and range, $R$ is a one-to-one
  function. Then the inverse of $f(x)$ is the function $\inverse f(x)$
  with domain $R$ and range\aside{Notice that the domain and range
    have been swapped.} $D$ which satisfies the following properties:
  \begin{enumerate}
  \item $f\left(\inverse f(x)\right)=x$
  \item $\inverse f\left( f(x)\right)=x$
  \end{enumerate}
  for every value of $x$ in the domain of $f$ (equivalently, in the
  range of $\inverse f$).
\end{mydefinition}
Loosely speaking, Definition~\#\ref{def:FunctionInverse} says that two
functions are mutually inverse if they ``undo'' each other.

Our next task is to show that the derivatives of the in inverse
trigonometric functions are what we expect them to be. Given that we
have now obtained the derivatives of all of the trigonometric
functions it appears that we could proceed just as we did in
Section~\ref{sec:diff-inverse-tancot} and
Section~\ref{sec:other-inverse-trig}.

But that would require that we explicitly assume that each of the
inverse trigonometric functions is differentiable, similar to the way
we found the derivative of a quotient. This is a valid approach of
course, but proceeding in that manner would mask some issues that will
be of interest to us later. So we will approach the derivatives of
inverse functions abstractly by (rigorously) finding a formula for the
derivative of the inverse of a generic, invertible function. After
that we'll only need to apply the formula to each of the inverse
trigonmetric formulas.

\begin{Digression}[Inverse and Derivative Notation]
  As we saw in Digression~\ref{digression:inverse-function-notation} there are some
  difficulties with the notation we use to indicate inverse
  functions. These problems only get worse when we mix the standard
  derivative notations with the inverse function notation.
  Lagrange's prime notation is especially problematic.
  
  For example if $f(x)$ is an invertible function the
  derivative of $\inverse{f}(x)$ could be denoted either  as:\\
  \centerline{$\dfdx{(\inverse{f})}{x}$ \hskip3mm or\hskip3mm 
    ${\inverse{f}}^\prime(x)$.}
  
  But both of these are somewhat awkward. Mathematicians also
  sometimes use the \term{operator} notation:
  $$
  \DD(f(x)) =  f^\prime(x)=\dfdx{f}{x}
  $$
  and in this situation it minimizes the awkwardness a bit.
  
  As we've seen there can also be some vagueness involving the
  distinction between functions and variables. For example  suppose we
  want to sketch a graph of this relation between $x$ and $y$:
  $$
  y-x^3=0.
  $$
  The simplest thing to do is to choose a value for either $x$ or $y$
  and then figure out what the corresponding $y$ or $x$ is. This is
  simpler to do that if we rearrange the relation so that we
  have one variable strictly in terms of (``as a function of'') the
  other. For this particular relation it is easiest to choose a value
  for $x$ and compute the corresponding $y$ value so we would normally
  rearrange it as
  <me>
    y(x)=x^3.\label{eq:Inv1}
  </me>
  Equation~\ref{eq:Inv1} defines $y$ as a function of $x$.
  
  But we only solved for $y$ because we could see it was a little
  easier to do. Otherwise our choice was completely arbitrary. We could also
  have solved for $x$ giving,
  <me>
    x(y)=\sqrt[3]{y}.\label{eq:Inv2}
  </me>
  In this case we have $x$ as a function of $y$.
  
  The two functions, $y(x)$, (``cube'') and $x(y)$ (``cube
  root''), clearly contain the same information as the original
  relation  $y-x^3=0$. But they are different, related,
  functions. They are in fact mutually inverse.

  For example suppose we choose $x=2$ and use Equation~(\ref{eq:Inv1})
  to find $y=8$. If we then take $y=8$ and use
  Equation~(\ref{eq:Inv2}) we find that $x=2$. That is, $x(y)$ has
  ``undone'' $y(x)$ for the single
  pair $(2,8)$. Drill~\#\ref{drill:CubeRoot} asks you to show that it is
  true for every pair $(x, y(x))$. This ``undoing'' makes $y(x)$ and
  $x(y)$ a pair of mutually inverse functions.
          
  But in function notation the variable (frequently $x$ or $t$) is a
  placeholder. For example, each of $y(x)=x^3,$ $y(t)=t^3$,
  $f(\alpha)=\alpha^3$, or even $f(\halmos)=\halmos^3$ defines
  exactly the same function: The function which cubes its
  input. It doesn't matter what we call the variable. It just holds a
  place in the formula that tells us what the input  is
  and what to do with it. Since it doesn't matter what we call
  the variable we usually call it $x$ unless there is some compelling
  reason to use something else.
  
  To avoid confusing variable names with function names we usually
  denote $y(x)$ as $f(x)$. It's inverse, $x(y)$ should probably be
  denoted as $\inverse{f}(y)$.  But sadly, recognizing that the
  variable is just a placeholder in function notation we use the same
  variable name in both the function and it's inverse. So we denote
  the inverse of $f(x)$ as $\inverse{f}(x)$, even though it would
  probably make it easier for beginners to use $\inverse{f}(y)$, as a
  reminder that both functions come from the same original
  relation.

    \begin{ProficiencyDrill}
    \label{drill:CubeRoot}
Prove that $f(x)=x^3$ and     $\inverse{f}(x)=\sqrt[3]{x}$ are
mutually inverse by showing that they satisfy the conditions stated in
Definition~\#\ref{def:FunctionInverse}
  \end{ProficiencyDrill}

          
  The notation for inverse functions is not great. It can be very
  confusing, especially for beginners. Be careful with it.
\end{Digression}

Our next task is to show that if $f(x)$ is invertible and
differentiable, then $\inverse{f}$ is also differentiable\aside{$\inverse{f}$ is
  obviously invertible}. We do this by showing that the limit
<me>
  \label{eq:InvFuncDerivLimit}
  \DD\left(\inverse{f}(x)\right)=\limit{h}{0}{\frac{\inverse{f}(x+h)-\inverse{f}(x)}{h}}
</me>
exists.

In general this is true but there is one exception that has to be
addressed. When $f$ is differentiable at $a$ and $f^\prime(a) = 0$
then the limit in Equation~(\ref{eq:InvFuncDerivLimit} does not
exist. Hence $\inverse f$ is not differentiable at $f(a)$.
More formally, we have the following lemma.


\begin{mylemma}
  \label{lemma:InvDerivAtZero}
  If  $f$ is an invertible function, $f(a)=b$, $f$ is differentiable
  at $x=a$, and $f^\prime(a)=0$, then $\inverse f$ is not
  differentiable at  $x=b$. That is $\DD\left(\inverse f(b)\right)$
  does not exist.
\end{mylemma}
The following proof of this lemma is very challenging to read and
understand for several reasons.

First, it is quite abstract. We don't have a particular function to
think about so we can't simply write down formulas for the function
and its inverse. Instead we have only the generic function, $f$ and
its inverse $\inverse{f}$, and we'll need to remember
what these symbols represent.

Second, we need to think about the functions $f$ and
$\inverse{f}$ as well as their derivatives.

Third, instead of using the differential notation, $\dfdx{f}{x}$ that we've grown very
comfortable with 
we'll be using the less familiar Lagrange prime notation  and the
operator notation we just introduced.

Finally, the nature of the problem forces us to mix these last two
notations, using one here and the other there. This can make for
difficult reading.

Read slowly. Remember that each symbol has meaning. Take time to
understand that meaning and what each formula as a whole is telling
you.


We include this proof in its full abstraction for two reasons:
\begin{enumerate}
\item To be as precise and as rigorous and we can.
\item We want to give you practice with higher level abstract
  reasoning in this (fairly) simple case.
\end{enumerate}

The strategy behind the following proof follows the same general
scheme as the Sherlock Holmes Maxim that we referred to in
Problem~\#\ref{problem:MaximalTriangle}. We will eliminate the
impossible so that ``whatever remains, however improbable, must be the
truth.''

There are two possibilities: Either the derivative of $\inverse{f}(b)$
exists or it does not exist. There are two steps:
\begin{enumerate}[label={  (\arabic*)}]
\item Assume that the derivative of $\inverse{f}$ does exist at $x=b$
  and calculate what $\DD\left(\inverse{f}(b)\right)$ must be.
\item Show that our computed value is impossible. Then \foreign{\'a 
    la} Holme's Maxim  the
  only possibility left will be that the derivative of
  $\inverse{f}$ does not exist at $x=b$.
\end{enumerate}
\begin{myproofof}{Lemma~\#\ref{lemma:InvDerivAtZero}}
  Assume that $\inverse{f}$ is differentiable\aside{We don't really
    believe this assumption. Be sure you are very clear on this
    point. We make this assumption so that we can use it to derive an
    absurd result; a contradiction. If there are no errors in our
    reasoning then the only possible conclusion will be that this
    assumption is false: $\inverse{f}$ is not differentiable at
    $x=b$.} at $x=b$. Because $f$ and $\inverse{f}$ are mutually
  inverse we know that
  $$ f\left(\inverse{f}(x)\right)=x.$$
  Therefore
  $$ \DD\left(f\left(\inverse{f}(x)\right)\right)=\DD\left(x\right).$$
  On the right we have
  $$
  \DD(x)=1.
  $$
   On the left apply the Chain Rule:
  <me>
    \label{eq:InvDeriv2}
    f^\prime\left(\inverse{f}(x)\right)\cdot\DD\left(\inverse{f}(x)\right)=1.
  </me>
  But when $x=b$ we find that 
  $f^\prime\left(\inverse{f}(b)\right)=f^\prime\left(a\right)=0$,
  so that
  $$
  0=\underbrace{f^\prime\left(\inverse{f}(b)\right)}_{=0}\cdot\DD\left(\inverse{f}(b)\right)=1
  $$
  or
  $$  0=1$$
  which is ridiculous or in Holmes' word, impossible.
  
  Therefore our assumption cannot true so $\inverse{f}$ is not
  differentiable at $x=b$.
\end{myproofof}

While valid and correct, this proof is not very enlightening.  A well
chosen sketch would be much more convincing, if less rigorous.


\begin{embeddedproblem}
  Choose a function whose derivative is equal to zero at some point
  and sketch the graph of your function and its inverse on the same
  set of axes. Be sure to include the point where the derivative is
  zero.
  
  Use your graph to explain why the derivative of the inverse of your
  function does not exist.
\end{embeddedproblem}

We now understand what conditions are necessary for an arbitrary
function, $f(x)$, to have a differentiable inverse.

 Also, from Equation~(\ref{eq:InvDeriv2}) we know what
the derivative of the inverse will be if it exists:
$$
\DD\left(\inverse{f}(x)\right)=\frac{1}{f^\prime\left(\inverse{f}(x)\right)}.
$$


\begin{ProficiencyDrill}
  Let $y=\inverse{f}(x)$ and explain how the formula above is
  equivalent to
  <me>
    \dfdx{y}{x}=\frac{1}{\dfdx{x}{y}}\label{eq:InvDeriv}
  </me>
  \end{ProficiencyDrill}


The only thing left is to show that under the conditions on $f$ in
Lemma~\ref{lemma:InvDerivAtZero} the
derivative (that is, the limit which defines the derivative) of the
inverse does in fact exist.


\begin{mytheorem}[The Derivative of Inverse Functions]
  \label{thm:DerivInvFunc}
  Suppose that
  \begin{enumerate}
    \begin{multicols}{2}
    \item $f$ is differentiable at $x=a$,
    \item $f(a)=b$,
    \item $f^\prime(a)\neq0$,
    \item $\inverse{f}$ is continuous at\aside{In fact, this follows
        from the continuity of $f$ at $x=a$. We do not have all of the
        tools necessary to prove this so we must include it in the
        assumptions of our theorem.} $x=b$.
    \end{multicols}
  \end{enumerate}
  Then the inverse of $f$ is differentiable at $x=b$ and 
  $$
  \DD\left(\inverse{f}(b)\right)=\frac{1}{f^\prime\left(\inverse{f}(b)\right)}.
  $$
  \end{mytheorem}




Reading and understanding the notation in
Theorem~\ref{thm:DerivInvFunc} presents the same difficulties we saw
in the proof of Lemma~\ref{lemma:InvDerivAtZero}. Read it carefully.
Be patient with yourself and do not rush.


\begin{myproof}
  We want to show that the limit
  $$
  \DD\left(\inverse{f}(b)\right)=\limit{h}{0}{\frac{\inverse{f}(b+h)-\inverse{f}(b)}{h}}=\frac{1}{f^\prime\left(\inverse{f}(b)\right)}.
  $$
  Since $f(a)=b$ we know that
  $\textcolor{red}{\inverse{f}(b)=a}$ so that
  \begin{align*}
    \limit{h}{0}{\frac{\inverse{f}(b+h)-\textcolor{red}{\inverse{f}(b)}}{h}}
    \amp = \limit{h}{0}{\frac{\inverse{f}(b+h)-\textcolor{red}{a}}{h}}
    \intertext{ Observe that if $b+h$ is in the domain of $\inverse{f}$ then it is in the
    range of $f$. Thus there is some number in the domain   of
    $f$ (for convenience we'll call it $a+k$) such that
    $\textcolor{blue}{b+h}=\textcolor{blue}{f(a+k)}$. Thus}
    \limit{h}{0}{\frac{\inverse{f}(\textcolor{blue}{b+h})-\inverse{f}(b)}{h}}
    \amp = \limit{h}{0}
    {\frac{\inverse{f}(\textcolor{blue}{f(a+k)})-a}{h}}.\\
    \intertext{Again since $f$ and $\inverse{f}$ are mutually inverse
    they ``undo'' each other, so  $\inverse{f}(f(a+k))=a+k$ so that }
    \limit{h}{0}{\frac{\inverse{f}(b+h)-\inverse{f}(b)}{h}}       \amp =
    \limit{h}{0}{\frac{k}{h}}.\\
    \intertext{ Solving ${b+h=f(a+k)}$ for $h$ gives
    $\textcolor{blue}{h}=\textcolor{blue}{f(a+k)-b}$ so  }
    \limit{h}{0}{\frac{k}{h}}\amp = \limit{h}{0}{\frac{k}{\textcolor{blue}{f(a+k)-b}}}\\
    \intertext{and since $b=f(a)$ we have}
    \amp = \limit{h}{0}{\frac{k}{f(a+k)-f(a)}}\\
    \amp = \limit{h}{0} {\frac{1} {\frac{f(a+k)-f(a)}{k}}}.\\
    \amp =  \frac{1} {\limit{h}{0}{\frac{f(a+k)-f(a)}{k}}}.\\
  \end{align*}
  
  The expression $\limit{h}{0}{\frac{f(a+k)-f(a)}{k}}$ would be
  $f^\prime(a)$ if only we had $k\rightarrow0$ instead of
  $h\rightarrow0$.  What we need to show now is that if
  $h\rightarrow0$ then $k\rightarrow0$. Then we could write
  <me>
    \label{eq:InvProof}
    \DD\left(\inverse{f}(b)\right) = \frac{1}
    {\limit{k}{0}{\frac{f(a+k)-f(a)}{k}}}=\frac{1}{f^\prime(a)}
</me>
  and our proof would be complete
  
  Written a little more carefully, what we need to show is that
  $\tlimit{h}{0}{k}=0$. Recall that $a=\inverse{f}(b)$, and that
  $a+k=\inverse{f}(b+h)$ so we need to show  that
  $$
\tlimit{h}{0}{k}=\limit{h}{0}{\left[(a+k)-a\right]}=  \tlimit{h}{0}{\left[\inverse{f}(b+h)-\inverse{f}(b)\right]} = 0$$ or
        But we assumed that $\inverse{f}$ is continuous at $x=b$ which means
  that 
  $$
  \tlimit{h}{0}{\left[\inverse{f}(b+h)-\inverse{f}(b)\right]}=0,
  $$
   and the proof is complete.
  
  
  
          
          
   One last point: On the left side of formula~(\ref{eq:InvProof}) the
   variable is $b$ and on the right it is $a$. While this is not
   strictly wrong it is a more useful theorem if we state it in terms
   of $b$ alone.
  
  Since $f(a)=b$ we see that $\inverse{f}(b)=a$ so
  $$
  \DD\left(\inverse{f}(b)\right) =\frac{1}{f^\prime(\inverse{f}(b))}
  $$
  and the proof is complete.
\end{myproof}

Using Theorem~\ref{thm:DerivInvFunc} we can now show that the
derivatives of the inverse trigonometric functions and the natural
logarithm are exactly what we expect them to be. The difference is
that now there is no uncertainty or vagueness in our foundations. No
modern Bishop Berkeley can step in and sew doubt.


\begin{myexample}[The Derivative of the Inverse Sine]
  Suppose $f(x)=\sin(x).$ Then $\inverse{f}(x)=\inverse{\sin}(x)$ so
  \begin{align*}
    \DD\left(\inverse{f}(x)\right) =     \DD\left(\inverse\sin(x)\right) 
    \amp = \frac{1}{\textcolor{red}{f^\prime}(\textcolor{blue}{\inverse{f}}(x))}\\
    \amp = \frac{1}{\textcolor{red}{\cos}(\textcolor{blue}{\inverse\sin}(x))}\\
    \DD\left(\inverse{f}(x)\right) \amp =  \frac{1}{\sqrt{1-x^2}}.
  \end{align*}
\end{myexample}

\begin{embeddedproblem}
  Use Theorem~\ref{thm:DerivInvFunc} to show that each of the following
  differentiation rules is correct:
  \begin{enumerate}[label={  (\alph*)}]
    \begin{multicols}{3}
    \item
      $\DD\left(\inverse{\cos}(x)\right) = \frac{-1}{\sqrt{1-x^2}}$
    \item $\DD\left(\inverse{\tan}(x)\right) = \frac{1}{1+x^2}$
    \item $\DD\left(\inverse{\cot}(x)\right) = \frac{-1}{1+x^2}$
    \item
      $\DD\left(\inverse{\sec}(x)\right) =
      \frac{1}{\abs{x}\sqrt{x^2-1}}$
    \item
      $\DD\left(\inverse{\csc}(x)\right) =
      \frac{-1}{\abs{x}\sqrt{x^2-1}}$
    \item $\DD\left(\inverse{\ln}(x)\right) = e^x$
    \end{multicols}
  \end{enumerate}
\end{embeddedproblem}

Wait a minute! Did we forget one? What about the natural exponential
function?  Don't we also have to show that $\DD\left(e^x\right)=e^x$?

\begin{ProficiencyDrill}
  Look back at Definition~\ref{def:natural-exponential-ivp} and
  explain why it is not necessary to use limits to show that
  $\DD\left(e^x\right)=e^x$.
\end{ProficiencyDrill}


</chapter>
